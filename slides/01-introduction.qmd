---
title: "Introduction to Data Pipelines"
subtitle: "CFA Data Pipelines Workshop"
format:
  revealjs:
    theme: simple
    slide-number: true
    chalkboard: true
    preview-links: auto
    logo: ../ForeSITE-logo.png
knitr:
  opts_chunk:
    echo: true
---

## Welcome!

Welcome to the CFA Data Pipelines Workshop

- Introduction to workshop objectives
- Overview of data pipelines
- Setting up your environment

## Course Structure

- **Session 1**: Introduction (Now!)
- **Session 2**: Data Acquisition from APIs
- **Session 3**: Database Queries
- **Session 4**: Data Cleaning

## Resources

- Course website: [GitHub](https://epiforesite.github.io/CFA-Data-Pipelines-Workshop/)
- Course repository: [GitHub](https://github.com/EpiForeSITE/CFA-Data-Pipelines-Workshop)
- Documentation links will be provided
- Example code in repository

## Disclaimer

The views and opinions expressed in this presentation are those of the presenter and do not necessarily reflect the official policy or position of the University of Utah, the Center for Forecasting and Outbreak Analytics (CFA), or any other affiliated organizations.


## AI Disclosure

This presentation was created with the assistance of artificial intelligence tools, including:

- GitHub Copilot for code generation and suggestions
- Large language models for content refinement and structure

All content has been reviewed and validated by the presenter.


## Newish R Conventions

* The pipe operators
* Lambda functions


## The Pipe Operator

The pipe operator (`%>%` from `magrittr` or `|>` native in R ≥4.1) chains operations together for cleaner, more readable code.

:::: {.columns}

::: {.column width="50%"}
**Without pipe:**
```r
# Nested functions (hard to read)
result <- round(
  mean(
    filter(data, value > 0)$value
  ), 
  2
)

# Multiple intermediate variables
filtered <- filter(data, value > 0)
values <- filtered$value
avg <- mean(values)
result <- round(avg, 2)
```
:::

::: {.column width="50%"}
**With pipe:**
```r
# Clean, left-to-right flow
result <- data |>
  filter(value > 0) |>
  pull(value) |>
  mean() |>
  round(2)

# Read as: "take data, then
# filter, then extract values,
# then calculate mean, then round"
```
:::

::::

## Lambda Functions (Anonymous Functions)

Short, unnamed functions created on-the-fly.

:::: {.columns}

::: {.column width="50%"}
**R:**
```r
# Traditional function
square <- function(x) x^2
lapply(1:5, square)

# Lambda/anonymous function
lapply(1:5, function(x) x^2)

# New R ≥4.1 shorthand
lapply(1:5, \(x) x^2)
```
:::

::: {.column width="50%"}
**Python:**
```python
# Traditional function
def square(x):
    return x**2
list(map(square, range(1, 6)))

# Lambda function
list(map(lambda x: x**2, range(1, 6)))

# List comprehension (often clearer)
[x**2 for x in range(1, 6)]
```
:::

::::

## What is a Data Pipeline?

A **data pipeline** is a series of data processing steps where:

- Data is ingested from one or more sources
- Data is transformed or processed
- Data is loaded into a destination for analysis or storage


## Why Data Pipelines?

:::: {.columns}

::: {.column width="50%"}
**Benefits**

- Automation
- Reproducibility
- Scalability
- Error handling
:::

::: {.column width="50%"}
**Use Cases**

- ETL processes
- Real-time analytics
- Data warehousing
- ML model training
:::

::::

## Pipeline Design Patterns

### 1. Extract-Transform-Load (ETL)

- Extract data from sources
- Transform in staging area
- Load into destination

### 2. Extract-Load-Transform (ELT)

- Extract data from sources
- Load into destination
- Transform in destination

## Common Data Sources

- **APIs**: REST, GraphQL, SOAP
- **Databases**: SQL, NoSQL
- **Files**: CSV, JSON, XML, Parquet
- **Streaming**: Kafka, message queues, logs
- **Web scraping**: HTML parsing

## Pipeline Components

```{mermaid}
flowchart LR
    A[Data Source] --> B[Extraction]
    B --> C[Validation]
    C --> D[Transformation]
    D --> E[Loading]
    E --> F[Destination]
```

## Best Practices

1. **Idempotency**: Pipeline can run multiple times safely
2. **Error Handling**: Graceful failures and retries
3. **Logging**: Track pipeline execution
4. **Monitoring**: Alert on failures
5. **Testing**: Validate data quality
6. **Documentation**: Clear pipeline logic

## Idempotency (Definition)

> An operation (task, step, or entire pipeline run) is **idempotent** if executing it multiple times with the same inputs results in the same final state and does not create duplicate side effects (e.g., repeated inserts, duplicate files, double-charged API calls).

## Idempotency Key strategies:

- Use upserts/merges instead of plain inserts (match on a natural or surrogate key).
- Write outputs atomically (temp file then rename) to avoid partial artifacts.
- Make transformations pure: avoid relying on hidden global state or timestamps unless required.
- Guard external side effects (e.g., check if data already fetched; hash content before reprocessing).
- Design idempotent DAG tasks: each task declares its inputs and outputs; reruns only recompute what is invalidated.

Benefits: safer reruns after failure, reproducibility, easier recovery, and confidence in scheduled (e.g., nightly) executions.

## Error Handling Strategies

:::: {.columns}

::: {.column width="50%"}
**Detection**

- Input validation
- Schema checks
- Data quality tests
- Exception catching
:::

::: {.column width="50%"}
**Recovery**

- Retry with backoff
- Dead letter queues (i.e. Human intervention needed queue)
- Partial success handling
- Alerting and notifications
:::

::::

## Logging and Monitoring

:::: {.columns}

::: {.column width="50%"}
**Logging**

- Structured logs (JSON)
- Log levels (DEBUG, INFO, WARNING, ERROR)
- Contextual information (timestamps, user IDs)
- Centralized log storage
:::

::: {.column width="50%"}
**Monitoring**

- Pipeline execution metrics
- Data quality metrics
- Performance dashboards
- Alerting thresholds
- SLA (Service Level Agreement) tracking
:::

::::

## Documentation Best Practices

:::: {.columns}

::: {.column width="50%"}
**What to Document**

- Pipeline purpose and business logic
- Data sources and schemas
- Transformation rules
- Dependencies and prerequisites
- Expected runtime and resources
- Contact information
:::

::: {.column width="50%"}
**How to Document**

- README files in repositories
- Inline code comments
- Docstrings for functions
- Data dictionaries
- Architecture diagrams
- Runbooks for operators (Advanced)
:::

::::

## Tools and Frameworks

:::: {.columns}

::: {.column width="50%"}
**Python**

- [Apache Airflow](https://airflow.apache.org/)
- [Luigi](https://luigi.readthedocs.io/)
- [Prefect](https://www.prefect.io/)
- [Dagster](https://dagster.io/)
:::

::: {.column width="50%"}
**R**

- [targets](https://docs.ropensci.org/targets/)
- [drake](https://books.ropensci.org/drake/) (legacy, superceeded by targets.)
- custom workflows
:::

::::

## Development Environment Setup

### Python Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install common packages
pip install requests pandas sqlalchemy
```

### R Setup

```r
# Install common packages
install.packages(c("httr2", "dplyr", "DBI", "dbplyr"))
```

## Your First Pipeline

Let's build a simple pipeline:

1. Fetch data from an API
2. Clean and validate
3. Store in a database

We'll start with the basics and build complexity throughout the course.

## Example Pipeline: Weather Data

### Fetch weather data → Clean → Store

We will be fetching daily precipitation for Brighton Ski Resort for the last year.

**R Example:**

```{r}
## Needed Packages #####
library(httr2)      #< interacting with API (modern version)
library(dplyr)      #< data cleaning and processing
library(DBI)        #< generic database interface
library(RSQLite)    #< specific database we will use
library(ggplot2)    #< for visualization
```

## R Example Pipeline (continued)

**Step 1: Fetch data from Open-Meteo API (Brighton Ski Resort)**
```{r}
end_date <- Sys.Date()
start_date <- end_date - 365

request("https://archive-api.open-meteo.com/v1/archive") |>
  req_url_query(
    latitude = 40.5981,
    longitude = -111.5831,
    start_date = format(start_date, "%Y-%m-%d"),
    end_date = format(end_date, "%Y-%m-%d"),
    daily = "precipitation_sum,snowfall_sum",
    timezone = "America/Denver"
  ) |>
  req_perform() |>
  resp_body_json() -> data
```

## R Example Pipeline (continued)
**Step 2: Clean and validate**

```{r}
df <- data.frame(
  date = as.Date(unlist(data$daily$time)),
  precipitation_mm = unlist(data$daily$precipitation_sum),
  snowfall_cm = unlist(data$daily$snowfall_sum)
)

df <- df %>%
  # Remove nulls and validate ranges
  filter(!is.na(precipitation_mm), !is.na(snowfall_cm)) %>%
  filter(precipitation_mm >= 0, snowfall_cm >= 0) %>%
  # Add derived variables
  mutate(
    snow_water_equiv_mm = snowfall_cm * 10,
    rain_mm = pmax(0, precipitation_mm - snow_water_equiv_mm)
  )
```

## R Example Pipeline (continued)
**Step 3: Store in SQLite database**
```{r}
conn <- dbConnect(RSQLite::SQLite(), "weather_data.db")
dbWriteTable(conn, "brighton_weather", df, overwrite = TRUE)
dbDisconnect(conn)

cat(sprintf("Stored %d records\n", nrow(df)))
cat(sprintf("Total snowfall: %.1f cm\n", sum(df$snowfall_cm)))
```

## R Example Pipeline (finished)

**Step 4: Do something with the data**
```{r}
#| label: brighton-plot
#| echo: true
#| eval: false
# Create stacked area plot
ggplot(df, aes(x = date)) +
  geom_area(aes(y = rain_mm, fill = "Rain"), alpha = 0.7) +
  geom_area(aes(y = snow_water_equiv_mm, fill = "Snow"), alpha = 0.7) +
  scale_fill_manual(values = c("Rain" = "steelblue", "Snow" = "lightblue")) +
  labs(
    title = "Brighton Ski Resort: Snow vs Rain Precipitation",
    x = "Date",
    y = "Precipitation (mm water equivalent)",
    fill = "Type"
  ) +
  theme_minimal()
```

## R Example Pipeline (plot)

```{r}
#| ref.label: brighton-plot
#| echo: false
#| eval: true
```

## Python Example Pipeline: Weather Data

```{r}
#| echo: false
reticulate::py_require(c('requests', 'pandas', 'matplotlib'))
```
```{python}
## Needed Packages #####
import requests     # interacting with API
import pandas as pd # data cleaning and processing
import sqlite3      # database interface
from datetime import datetime, timedelta
```

## Python Example Pipeline (continued)

**Step 1: Fetch data from Open-Meteo API (Brighton Ski Resort)**

```{python}
end_date = datetime.now()
start_date = end_date - timedelta(days=365)

url = "https://archive-api.open-meteo.com/v1/archive"
params = {
    'latitude': 40.5981,
    'longitude': -111.5831,
    'start_date': start_date.strftime('%Y-%m-%d'),
    'end_date': end_date.strftime('%Y-%m-%d'),
    'daily': 'precipitation_sum,snowfall_sum',
    'timezone': 'America/Denver'
}

response = requests.get(url, params=params)
data = response.json()
```

## Python Example Pipeline (continued)

**Step 2: Clean and validate**

```{python}
df = pd.DataFrame({
    'date': pd.to_datetime(data['daily']['time']),
    'precipitation_mm': data['daily']['precipitation_sum'],
    'snowfall_cm': data['daily']['snowfall_sum']
})

# Remove nulls and validate ranges
df = df.dropna()
df = df[(df['precipitation_mm'] >= 0) & (df['snowfall_cm'] >= 0)]

# Add derived variables
df['snow_water_equiv_mm'] = df['snowfall_cm'] * 10
df['rain_mm'] = (df['precipitation_mm'] - df['snow_water_equiv_mm']).clip(lower=0)
```

## Python Example Pipeline (continued)

**Step 3: Store in SQLite database**

```{python}
conn = sqlite3.connect('weather_data_py.db')
df.to_sql('brighton_weather', conn, if_exists='replace', index=False)
conn.close()

print(f"Stored {len(df)} records")
print(f"Total snowfall: {df['snowfall_cm'].sum():.1f} cm")
```

## Python Example Pipeline (finished)

**Step 4: Do something with the data**

```{python}
#| label: brighton-plot-py
#| echo: true
#| eval: false
import matplotlib.pyplot as plt

# Create stacked area plot
fig, ax = plt.subplots(figsize=(10, 6))
ax.fill_between(df['date'], 0, df['rain_mm'], 
                label='Rain', alpha=0.7, color='steelblue')
ax.fill_between(df['date'], df['rain_mm'], 
                df['rain_mm'] + df['snow_water_equiv_mm'],
                label='Snow', alpha=0.7, color='lightblue')
ax.set_xlabel('Date')
ax.set_ylabel('Precipitation (mm water equivalent)')
ax.set_title('Brighton Ski Resort: Snow vs Rain Precipitation')
ax.legend()
plt.tight_layout()
plt.show()
```

## Python Example Pipeline (plot)

```{python}
#| ref.label: brighton-plot-py
#| echo: false
#| eval: true
```



## Questions?

Next session: Data Acquisition from APIs
