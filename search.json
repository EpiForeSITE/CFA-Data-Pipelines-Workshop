[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Title: CFA Data Pipelines Workshop\nInstructor: Andrew Redd, PhD"
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "",
    "text": "Course Title: CFA Data Pipelines Workshop\nInstructor: Andrew Redd, PhD"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis workshop provides hands-on training in building data pipelines, acquiring data from databases and APIs, and implementing effective data cleaning strategies. Students will learn industry best practices and gain practical experience through real-world examples."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this course, students will be able to:\n\nDesign and implement efficient data pipelines\nAcquire data from RESTful APIs using authentication and pagination\nQuery databases effectively using SQL and ORMs\nClean and validate data for analysis\nHandle common data quality issues\nImplement error handling and logging in data pipelines\nApply best practices for reproducible data workflows"
  },
  {
    "objectID": "syllabus.html#course-topics",
    "href": "syllabus.html#course-topics",
    "title": "Syllabus",
    "section": "Course Topics",
    "text": "Course Topics\n\nModule 1: Introduction to Data Pipelines\n\nWhat are data pipelines?\nPipeline design patterns\nTools and frameworks\n\n\n\nModule 2: Databases\n\nSQL fundamentals review\nConnecting to databases\nWriting efficient queries\nWorking with ORMs\nTransaction management\n\n\n\nModule 3: Data Acquisition from APIs\n\nRESTful API fundamentals\nAuthentication methods (API keys, OAuth)\nHandling pagination and rate limits\nError handling and retries\nPractical examples with real APIs\n\n\n\nModule 4: Data Cleaning\n\nCommon data quality issues\nMissing data handling\nData validation techniques\nData transformation and normalization\nDocumentation and reproducibility"
  },
  {
    "objectID": "syllabus.html#required-materials",
    "href": "syllabus.html#required-materials",
    "title": "Syllabus",
    "section": "Required Materials",
    "text": "Required Materials\n\nLaptop with Python 3.8+ or R 4.0+ installed\nInternet connection for API access\nText editor or IDE you are familiar with (VS Code, RStudio, or PyCharm recommended)"
  },
  {
    "objectID": "syllabus.html#resources",
    "href": "syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\n\nGitHub Repository: https://github.com/EpiForeSITE/CFA-Data-Pipelines-Workshop\nAdditional resources will be provided throughout the course"
  },
  {
    "objectID": "slides/03-databases.html",
    "href": "slides/03-databases.html",
    "title": "Databases",
    "section": "",
    "text": "Today‚Äôs topics:\n\nSQL fundamentals review\nConnecting to databases\nWriting efficient queries\nObject-Relational Mapping (ORMs)\nTransactions and ACID properties"
  },
  {
    "objectID": "slides/03-databases.html#overview",
    "href": "slides/03-databases.html#overview",
    "title": "Databases",
    "section": "",
    "text": "Today‚Äôs topics:\n\nSQL fundamentals review\nConnecting to databases\nWriting efficient queries\nObject-Relational Mapping (ORMs)\nTransactions and ACID properties"
  },
  {
    "objectID": "slides/03-databases.html#why-databases",
    "href": "slides/03-databases.html#why-databases",
    "title": "Databases",
    "section": "Why Databases?",
    "text": "Why Databases?\n\nStructured storage: Organized data\nQuery power: Filter and aggregate\nConcurrency: Multiple users\nACID guarantees: Data integrity: (Atomicity, Consistency, Isolation, and Durability)\nScalability: Handle large datasets"
  },
  {
    "objectID": "slides/03-databases.html#database-types",
    "href": "slides/03-databases.html#database-types",
    "title": "Databases",
    "section": "Database Types",
    "text": "Database Types\n\n\nRelational (SQL)\n\nPostgreSQL\nMySQL\nSQLite\nSQL Server\n\n\nNon-relational (NoSQL)\n\nMongoDB\nRedis\nCassandra\nDynamoDB\n\n\nSpecialized\n\nNeo4j (Graph)\nApache Spark (Big Data)\nHadoop (Big Data)\nInfluxDB (Time-series)\n\n\n\nFocus today: SQL databases"
  },
  {
    "objectID": "slides/03-databases.html#relational-databases",
    "href": "slides/03-databases.html#relational-databases",
    "title": "Databases",
    "section": "Relational Databases",
    "text": "Relational Databases\nWhat makes a database ‚Äúrelational‚Äù?\n\nData organized into tables (relations)\nEach table has rows (records) and columns (fields)\nTables connected through relationships\nPrimary keys uniquely identify rows\nForeign keys link tables together\nEnforces data integrity and reduces redundancy"
  },
  {
    "objectID": "slides/03-databases.html#relational-database-example",
    "href": "slides/03-databases.html#relational-database-example",
    "title": "Databases",
    "section": "Relational Database Example",
    "text": "Relational Database Example\nHealthcare system with related tables:\n\n\n\n\n\nerDiagram\n    direction LR\n    PATIENTS ||--o{ OUTPATIENT_VISITS : has\n    OUTPATIENT_VISITS }o--|| PROVIDERS : seen_by\n    OUTPATIENT_VISITS }o--|| DIAGNOSES : assigned\n    \n    PATIENTS {\n        int patient_id PK \"üîë\"\n        string name\n        date date_of_birth\n        string insurance_id\n    }\n    \n    OUTPATIENT_VISITS {\n        int visit_id PK \"üîë\"\n        int patient_id FK \"üîó\"\n        int provider_id FK \"üîó\"\n        int diagnosis_id FK \"üîó\"\n        date visit_date\n    }\n    \n    PROVIDERS {\n        int provider_id PK \"üîë\"\n        string name\n        string specialty\n    }\n    \n    DIAGNOSES {\n        int diagnosis_id PK \"üîë\"\n        string icd_code\n        string description\n    }"
  },
  {
    "objectID": "slides/03-databases.html#workshop-database",
    "href": "slides/03-databases.html#workshop-database",
    "title": "Databases",
    "section": "Workshop Database",
    "text": "Workshop Database\nhealthcare.db - Sample SQLite database\nRun exercises/03-healthcare-database.py to create:\n\n10 patients with demographics and insurance\n7 providers across 4 specialties\n10 diagnoses with ICD-10 codes\n25 outpatient visits over 6 months\n\nAll examples today use this database!"
  },
  {
    "objectID": "slides/03-databases.html#sql-fundamentals",
    "href": "slides/03-databases.html#sql-fundamentals",
    "title": "Databases",
    "section": "SQL Fundamentals",
    "text": "SQL Fundamentals\nStructured Query Language (SQL)\nBasic operations:\n\nSELECT: Retrieve data\nINSERT: Add data\nUPDATE: Modify data\nDELETE: Remove data"
  },
  {
    "objectID": "slides/03-databases.html#select-statement",
    "href": "slides/03-databases.html#select-statement",
    "title": "Databases",
    "section": "SELECT Statement",
    "text": "SELECT Statement\n-- Basic select\nSELECT patient_id, name FROM patients;\n\n-- All columns\nSELECT * FROM patients;\n\n-- With conditions\nSELECT name, date_of_birth \nFROM patients \nWHERE insurance_id IS NOT NULL;\n\n-- With sorting\nSELECT name, visit_date \nFROM outpatient_visits \nORDER BY visit_date DESC;"
  },
  {
    "objectID": "slides/03-databases.html#select-example",
    "href": "slides/03-databases.html#select-example",
    "title": "Databases",
    "section": "Select Example",
    "text": "Select Example\n\nSELECT name, date_of_birth \nFROM patients \nWHERE insurance_id IS NOT NULL\nORDER BY date_of_birth\nLIMIT 5;\n\n\n5 records\n\n\nname\ndate_of_birth\n\n\n\n\nPatricia Brown\n1965-05-30\n\n\nWilliam Miller\n1972-12-03\n\n\nJohn Smith\n1975-03-15\n\n\nMichael Jones\n1978-09-12\n\n\nDavid Rodriguez\n1980-04-18"
  },
  {
    "objectID": "slides/03-databases.html#filtering-with-where",
    "href": "slides/03-databases.html#filtering-with-where",
    "title": "Databases",
    "section": "Filtering with WHERE",
    "text": "Filtering with WHERE\n-- Comparison operators\nSELECT * FROM outpatient_visits WHERE visit_date &gt; '2024-01-01';\n\n-- Multiple conditions\nSELECT * FROM outpatient_visits \nWHERE visit_date &gt;= '2024-01-01' \n  AND provider_id = 5;\n\n-- Pattern matching\nSELECT * FROM patients \nWHERE name LIKE 'Smith%';\n\n-- IN clause\nSELECT * FROM providers \nWHERE specialty IN ('Cardiology', 'Neurology');"
  },
  {
    "objectID": "slides/03-databases.html#where-example",
    "href": "slides/03-databases.html#where-example",
    "title": "Databases",
    "section": "WHERE Example",
    "text": "WHERE Example\n\nSELECT * FROM providers \nWHERE specialty IN ('Cardiology', 'Neurology');\n\n\n4 records\n\n\nprovider_id\nname\nspecialty\n\n\n\n\n1\nDr.¬†Emily Chen\nCardiology\n\n\n2\nDr.¬†James Wilson\nNeurology\n\n\n5\nDr.¬†Lisa Rodriguez\nCardiology\n\n\n6\nDr.¬†Robert Lee\nNeurology"
  },
  {
    "objectID": "slides/03-databases.html#aggregation",
    "href": "slides/03-databases.html#aggregation",
    "title": "Databases",
    "section": "Aggregation",
    "text": "Aggregation\n-- Count records\nSELECT COUNT(*) FROM patients;\n\n-- Count visits per patient\nSELECT \n    patient_id,\n    COUNT(*) as visit_count\nFROM outpatient_visits\nGROUP BY patient_id;\n\n-- Group by provider specialty\nSELECT \n    specialty,\n    COUNT(*) as provider_count\nFROM providers\nGROUP BY specialty;"
  },
  {
    "objectID": "slides/03-databases.html#aggregation-example",
    "href": "slides/03-databases.html#aggregation-example",
    "title": "Databases",
    "section": "Aggregation Example",
    "text": "Aggregation Example\n\nSELECT \n    specialty,\n    COUNT(*) as provider_count\nFROM providers\nGROUP BY specialty;\n\n\n4 records\n\n\nspecialty\nprovider_count\n\n\n\n\nCardiology\n2\n\n\nFamily Medicine\n2\n\n\nNeurology\n2\n\n\nOrthopedics\n1"
  },
  {
    "objectID": "slides/03-databases.html#joins",
    "href": "slides/03-databases.html#joins",
    "title": "Databases",
    "section": "Joins",
    "text": "Joins\n-- Inner join\nSELECT p.name, v.visit_date, pr.name as provider_name\nFROM patients p\nINNER JOIN outpatient_visits v ON p.patient_id = v.patient_id\nINNER JOIN providers pr ON v.provider_id = pr.provider_id;\n\n-- Left join\nSELECT p.name, COUNT(v.visit_id) as visit_count\nFROM patients p\nLEFT JOIN outpatient_visits v ON p.patient_id = v.patient_id\nGROUP BY p.name;"
  },
  {
    "objectID": "slides/03-databases.html#joins-example",
    "href": "slides/03-databases.html#joins-example",
    "title": "Databases",
    "section": "Joins Example",
    "text": "Joins Example\n\nSELECT p.name, COUNT(v.visit_id) as visit_count\nFROM patients p\nLEFT JOIN outpatient_visits v ON p.patient_id = v.patient_id\nGROUP BY p.name\nORDER BY visit_count DESC\nLIMIT 5;\n\n\n5 records\n\n\nname\nvisit_count\n\n\n\n\nDavid Rodriguez\n4\n\n\nPatricia Brown\n4\n\n\nSarah Martinez\n3\n\n\nJohn Smith\n2\n\n\nMary Johnson\n2"
  },
  {
    "objectID": "slides/03-databases.html#database-connection-components",
    "href": "slides/03-databases.html#database-connection-components",
    "title": "Databases",
    "section": "Database Connection Components",
    "text": "Database Connection Components\nUnderstanding how to connect to databases:\n\nProtocol: Communication method (e.g., TCP/IP, file)\nDriver: Software that translates commands for specific database\nConnection String: Contains all connection details\nHost/Server: Location of database server\nPort: Network port (e.g., 5432 for PostgreSQL)\nCredentials: Username and password\nDatabase Name: Specific database on the server"
  },
  {
    "objectID": "slides/03-databases.html#connection-strings",
    "href": "slides/03-databases.html#connection-strings",
    "title": "Databases",
    "section": "Connection Strings",
    "text": "Connection Strings\nExample:\nAzure SQL with Interactive Authentication, Necessary if using 2FA\nDriver={ODBC Driver 18 for SQL Server};\nServer=cfa-datasbase-p1.cfa.privatelink.database.windows.net;\nDatabase=healthcare;\nAuthentication=ActiveDirectoryInteractive;\nTrustServerCertificate=True;\nEncrypt=yes;\nReference: ConnectionStrings.com"
  },
  {
    "objectID": "slides/03-databases.html#database-connectors",
    "href": "slides/03-databases.html#database-connectors",
    "title": "Databases",
    "section": "Database Connectors",
    "text": "Database Connectors\n\n\nPython Libraries:\n\npsycopg2: PostgreSQL\npymysql: MySQL\nsqlite3: SQLite (built-in)\npyodbc: ODBC (multiple databases)\n\n\nR Packages:\n\nRPostgres: PostgreSQL\nRMySQL: MySQL\nRSQLite: SQLite\nodbc: ODBC connections"
  },
  {
    "objectID": "slides/03-databases.html#odbc-open-database-connectivity",
    "href": "slides/03-databases.html#odbc-open-database-connectivity",
    "title": "Databases",
    "section": "ODBC (Open Database Connectivity)",
    "text": "ODBC (Open Database Connectivity)\nUniversal interface for database connections\nlibrary(DBI)\nlibrary(odbc)\n\ncon &lt;- dbConnect(\n  odbc::odbc(),\n  Driver = \"PostgreSQL Unicode\",\n  Server = \"localhost\",\n  Database = \"healthcare\",\n  UID = \"user\",\n  PWD = \"password\",\n  Port = 5432\n)\nBenefits:\n\nDatabase-agnostic\nwidely supported\n\nPitfall:\n\nDoes not universalize SQL"
  },
  {
    "objectID": "slides/03-databases.html#connection-best-practices",
    "href": "slides/03-databases.html#connection-best-practices",
    "title": "Databases",
    "section": "Connection Best Practices",
    "text": "Connection Best Practices\n\nNever hardcode credentials in scripts\nUse environment variables or config files\nClose connections when done\nUse connection pooling for web apps\nTest connectivity before running queries\nHandle timeouts and network errors"
  },
  {
    "objectID": "slides/03-databases.html#environment-variables-example",
    "href": "slides/03-databases.html#environment-variables-example",
    "title": "Databases",
    "section": "Environment Variables Example",
    "text": "Environment Variables Example\nimport os\nimport psycopg2\n\n# Store credentials in environment variables\nconn = psycopg2.connect(\n    host=os.getenv('DB_HOST'),\n    database=os.getenv('DB_NAME'),\n    user=os.getenv('DB_USER'),\n    password=os.getenv('DB_PASSWORD')\n)\n# R equivalent\ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  host = Sys.getenv('DB_HOST'),\n  dbname = Sys.getenv('DB_NAME'),\n  user = Sys.getenv('DB_USER'),\n  password = Sys.getenv('DB_PASSWORD')\n)"
  },
  {
    "objectID": "slides/03-databases.html#connecting-to-databases",
    "href": "slides/03-databases.html#connecting-to-databases",
    "title": "Databases",
    "section": "Connecting to Databases",
    "text": "Connecting to Databases\n\nPython with SQLite\nimport sqlite3\n\n# Connect to database\nconn = sqlite3.connect('database.db')\ncursor = conn.cursor()\n\n# Execute query\ncursor.execute('SELECT * FROM users')\nrows = cursor.fetchall()\n\n# Close connection\nconn.close()"
  },
  {
    "objectID": "slides/03-databases.html#python-with-postgresql",
    "href": "slides/03-databases.html#python-with-postgresql",
    "title": "Databases",
    "section": "Python with PostgreSQL",
    "text": "Python with PostgreSQL\nimport psycopg2\n\n# Connect\nconn = psycopg2.connect(\n    host=\"localhost\",\n    database=\"mydb\",\n    user=\"user\",\n    password=\"password\"\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM users\")\nrows = cursor.fetchall()\n\nconn.close()"
  },
  {
    "objectID": "slides/03-databases.html#r-with-dbi",
    "href": "slides/03-databases.html#r-with-dbi",
    "title": "Databases",
    "section": "R with DBI",
    "text": "R with DBI\nlibrary(DBI)\n\n# SQLite\ncon &lt;- dbConnect(RSQLite::SQLite(), \"database.db\")\n\n# Query\nresult &lt;- dbGetQuery(con, \"SELECT * FROM users\")\n\n# Close\ndbDisconnect(con)"
  },
  {
    "objectID": "slides/03-databases.html#parameterized-queries",
    "href": "slides/03-databases.html#parameterized-queries",
    "title": "Databases",
    "section": "Parameterized Queries",
    "text": "Parameterized Queries\nAlways use parameterized queries!\nPython:\n# WRONG - SQL injection risk\nuser_id = input(\"Enter user ID: \")\ncursor.execute(f\"SELECT * FROM users WHERE id = {user_id}\")\n\n# CORRECT - Safe from SQL injection\nuser_id = input(\"Enter user ID: \")\ncursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\nR:\n# WRONG - SQL injection risk\nuser_id &lt;- readline(\"Enter user ID: \")\nquery &lt;- paste0(\"SELECT * FROM users WHERE id = \", user_id)\ndbGetQuery(con, query)\n\n# CORRECT - Safe from SQL injection\nuser_id &lt;- readline(\"Enter user ID: \")\ndbGetQuery(con, \"SELECT * FROM users WHERE id = ?\", params = list(user_id))"
  },
  {
    "objectID": "slides/03-databases.html#object-relational-mapping-orm",
    "href": "slides/03-databases.html#object-relational-mapping-orm",
    "title": "Databases",
    "section": "Object-Relational Mapping (ORM)",
    "text": "Object-Relational Mapping (ORM)\nORMs map database tables to objects/data structures:\n\nWrite less SQL\nDatabase-agnostic code\nType safety\nMay sacrifice performance\n\nExamples:\n\nPython: SQLAlchemy, Django ORM\nR: dbplyr (maps dplyr to SQL)\nJava: Hibernate\nC#: Entity Framework"
  },
  {
    "objectID": "slides/03-databases.html#sqlalchemy-python",
    "href": "slides/03-databases.html#sqlalchemy-python",
    "title": "Databases",
    "section": "SQLAlchemy (Python)",
    "text": "SQLAlchemy (Python)\nfrom sqlalchemy import create_engine, Column, Integer, String\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    name = Column(String)\n    email = Column(String)\n\n# Create engine and session\nengine = create_engine('sqlite:///database.db')\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Query\nusers = session.query(User).filter(User.email.like('%@example.com')).all()"
  },
  {
    "objectID": "slides/03-databases.html#dbplyr-r",
    "href": "slides/03-databases.html#dbplyr-r",
    "title": "Databases",
    "section": "dbplyr (R)",
    "text": "dbplyr (R)\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Connect\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \"database.db\")\n\n# Reference table\nusers &lt;- tbl(con, \"users\")\n\n# Query with dplyr syntax\nactive_users &lt;- users %&gt;%\n  filter(status == \"active\") %&gt;%\n  select(name, email) %&gt;%\n  collect()  # Execute query"
  },
  {
    "objectID": "slides/03-databases.html#sql-to-dbplyr-translation",
    "href": "slides/03-databases.html#sql-to-dbplyr-translation",
    "title": "Databases",
    "section": "SQL to dbplyr Translation",
    "text": "SQL to dbplyr Translation\n\n\nSQL\nSELECT name, email\nFROM users\nWHERE status = 'active'\nORDER BY name\nLIMIT 10;\n\ndbplyr (R)\nusers %&gt;%\n  filter(status == \"active\") %&gt;%\n  select(name, email) %&gt;%\n  arrange(name) %&gt;%\n  head(10)\n\n\n\n\n\nSQL Keyword\ndbplyr Function\n\n\n\n\nSELECT\nselect()\n\n\nWHERE\nfilter()\n\n\nORDER BY\narrange()\n\n\nGROUP BY\ngroup_by()\n\n\nHAVING\nfilter() (after group_by)\n\n\nJOIN\nleft_join(), inner_join(), etc.\n\n\nTOP/LIMIT\nhead() or slice()"
  },
  {
    "objectID": "slides/03-databases.html#reading-data-from-database-r",
    "href": "slides/03-databases.html#reading-data-from-database-r",
    "title": "Databases",
    "section": "Reading Data from Database (R)",
    "text": "Reading Data from Database (R)\nPull data from database to R:\n# Method 1: DBI - Read entire table\npatients &lt;- dbReadTable(con, \"patients\")\n\n# Method 2: DBI - Query with SQL\nresult &lt;- dbGetQuery(con, \"\n  SELECT p.name, COUNT(v.visit_id) as visits\n  FROM patients p\n  LEFT JOIN outpatient_visits v ON p.patient_id = v.patient_id\n  GROUP BY p.name\n\")\n\n# Method 3: dbplyr - Lazy evaluation\npatients_tbl &lt;- tbl(con, \"patients\")\nactive_patients &lt;- patients_tbl %&gt;%\n  filter(insurance_id != \"\") %&gt;%\n  collect()  # Execute and bring to R"
  },
  {
    "objectID": "slides/03-databases.html#writing-data-to-database-r",
    "href": "slides/03-databases.html#writing-data-to-database-r",
    "title": "Databases",
    "section": "Writing Data to Database (R)",
    "text": "Writing Data to Database (R)\nPush data from R to database:\nlibrary(DBI)\n\n# Assuming you have a connection 'con'\n# (Works with any DBI-compatible database)\n\n# Write data frame to new table\ndbWriteTable(con, \"new_patients\", patient_df, overwrite = TRUE)\n\n# Append to existing table\ndbWriteTable(con, \"patients\", new_records, append = TRUE)\n\n# Temporary table (auto-deleted on disconnect)\ndbWriteTable(con, \"temp_analysis\", temp_df, temporary = TRUE)"
  },
  {
    "objectID": "slides/03-databases.html#dbplyr-data-transfer-methods",
    "href": "slides/03-databases.html#dbplyr-data-transfer-methods",
    "title": "Databases",
    "section": "dbplyr Data Transfer Methods",
    "text": "dbplyr Data Transfer Methods\nAdditional dbplyr functions for data movement:\nlibrary(dbplyr)\n\n# Copy local data frame to database (temporary by default)\ncopy_to(con, patient_df, \"patients_temp\")\n\n# Copy with indexes for better performance\ncopy_to(con, patient_df, \"patients_indexed\", \n        indexes = list(\"patient_id\", \"insurance_id\"))\n\n# Create permanent table\ncopy_to(con, patient_df, \"patients_permanent\", temporary = FALSE)\n\n# Compute intermediate results on database\nexpensive_query &lt;- patients_tbl %&gt;%\n  filter(insurance_id != \"\") %&gt;%\n  group_by(insurance_id) %&gt;%\n  summarize(count = n())\n\n# Store as temp table on database (not in R)\ncompute(expensive_query, name = \"temp_summary\")"
  },
  {
    "objectID": "slides/03-databases.html#idempotent-updates-with-rows_upsert",
    "href": "slides/03-databases.html#idempotent-updates-with-rows_upsert",
    "title": "Databases",
    "section": "Idempotent Updates with rows_upsert()",
    "text": "Idempotent Updates with rows_upsert()\nCritical for data pipelines: Update existing or insert new rows\nlibrary(dbplyr)\n\n# Get reference to existing table\npatients_tbl &lt;- tbl(con, \"patients\")\n\n# New or updated patient data\nupdated_patients &lt;- data.frame(\n  patient_id = c(1, 11),  # 1 exists, 11 is new\n  name = c(\"John Smith Updated\", \"New Patient\"),\n  date_of_birth = c(\"1975-03-15\", \"2000-01-01\"),\n  insurance_id = c(\"INS001\", \"INS011\")\n)\n\n# Upsert: Update if exists, insert if not\nrows_upsert(patients_tbl, updated_patients, \n            by = \"patient_id\",      # Key column to match on\n            in_place = TRUE)        # Modify database directly\n\n# Other useful row operations:\n# rows_insert() - Insert only (error if exists)\n# rows_update() - Update only (error if missing)\n# rows_delete() - Delete matching rows"
  },
  {
    "objectID": "slides/03-databases.html#writing-data-from-python-to-database",
    "href": "slides/03-databases.html#writing-data-from-python-to-database",
    "title": "Databases",
    "section": "Writing Data from Python to Database",
    "text": "Writing Data from Python to Database\nPush DataFrames to database:\nimport pandas as pd\nimport sqlite3  # or psycopg2, pyodbc, etc.\n\n# Create connection\nconn = sqlite3.connect('healthcare.db')\n\n# Write DataFrame to new table\npatient_df.to_sql('new_patients', conn, if_exists='replace', index=False)\n\n# Append to existing table\nnew_records.to_sql('patients', conn, if_exists='append', index=False)\n\n# Insert with SQLAlchemy (more powerful)\nfrom sqlalchemy import create_engine\nengine = create_engine('sqlite:///healthcare.db')\npatient_df.to_sql('patients', engine, if_exists='replace', \n                  index=False, method='multi')  # Faster bulk insert\n\nconn.close()\nNote: if_exists='replace' is idempotent (can be re-run safely)"
  },
  {
    "objectID": "slides/03-databases.html#python-idempotent-upsert-pattern",
    "href": "slides/03-databases.html#python-idempotent-upsert-pattern",
    "title": "Databases",
    "section": "Python Idempotent Upsert Pattern",
    "text": "Python Idempotent Upsert Pattern\nFor true upsert (INSERT or UPDATE), use raw SQL:\nimport pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect('healthcare.db')\ncursor = conn.cursor()\n\n# Example: Upsert single record\ncursor.execute(\"\"\"\n    INSERT INTO patients (patient_id, name, date_of_birth, insurance_id)\n    VALUES (?, ?, ?, ?)\n    ON CONFLICT(patient_id) \n    DO UPDATE SET \n        name = excluded.name,\n        date_of_birth = excluded.date_of_birth,\n        insurance_id = excluded.insurance_id\n\"\"\", (1, \"John Smith Updated\", \"1975-03-15\", \"INS001\"))\n\n# For DataFrame, use executemany()\ndata = df.values.tolist()\ncursor.executemany(\"INSERT INTO ... ON CONFLICT ...\", data)\n\nconn.commit()\nPostgreSQL: Use ON CONFLICT, MySQL: Use ON DUPLICATE KEY UPDATE"
  },
  {
    "objectID": "slides/03-databases.html#transactions",
    "href": "slides/03-databases.html#transactions",
    "title": "Databases",
    "section": "Transactions",
    "text": "Transactions\nA transaction is a sequence of operations performed as a single logical unit of work.\nconn = sqlite3.connect('database.db')\ncursor = conn.cursor()\n\ntry:\n    cursor.execute(\"BEGIN TRANSACTION\")\n    cursor.execute(\"UPDATE accounts SET balance = balance - 100 WHERE id = 1\")\n    cursor.execute(\"UPDATE accounts SET balance = balance + 100 WHERE id = 2\")\n    conn.commit()\nexcept Exception as e:\n    conn.rollback()\n    print(f\"Transaction failed: {e}\")\nfinally:\n    conn.close()"
  },
  {
    "objectID": "slides/03-databases.html#acid-properties",
    "href": "slides/03-databases.html#acid-properties",
    "title": "Databases",
    "section": "ACID Properties",
    "text": "ACID Properties\n\nAtomicity: All or nothing\nConsistency: Valid state transitions\nIsolation: Concurrent transactions don‚Äôt interfere\nDurability: Committed data persists"
  },
  {
    "objectID": "slides/03-databases.html#query-optimization",
    "href": "slides/03-databases.html#query-optimization",
    "title": "Databases",
    "section": "Query Optimization",
    "text": "Query Optimization\n\nUse Indexes\n-- Create index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Query benefits from index\nSELECT * FROM users WHERE email = 'user@example.com';\n\n\nAvoid SELECT *\n-- Less efficient\nSELECT * FROM users;\n\n-- More efficient - only needed columns\nSELECT id, name, email FROM users;"
  },
  {
    "objectID": "slides/03-databases.html#explain-query-plans",
    "href": "slides/03-databases.html#explain-query-plans",
    "title": "Databases",
    "section": "Explain Query Plans",
    "text": "Explain Query Plans\n-- See how database executes query\nEXPLAIN QUERY PLAN\nSELECT u.name, COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.name;\nHelps identify:\n\nMissing indexes\nInefficient joins\nFull table scans"
  },
  {
    "objectID": "slides/03-databases.html#explain-query-plans-output",
    "href": "slides/03-databases.html#explain-query-plans-output",
    "title": "Databases",
    "section": "Explain Query Plans Output",
    "text": "Explain Query Plans Output\n\nEXPLAIN QUERY PLAN\nSELECT p.name, COUNT(v.visit_id) as visit_count\nFROM patients p\nLEFT JOIN outpatient_visits v ON p.patient_id = v.patient_id\nGROUP BY p.name\nORDER BY visit_count DESC\n\n\n3 records\n\n\n\n\n\n\n\n\nid\nparent\nnotused\ndetail\n\n\n\n\n8\n0\n213\nSCAN p USING COVERING INDEX idx_patients_name\n\n\n10\n0\n54\nSEARCH v USING COVERING INDEX idx_visits_patient (patient_id=?) LEFT-JOIN\n\n\n48\n0\n0\nUSE TEMP B-TREE FOR ORDER BY"
  },
  {
    "objectID": "slides/03-databases.html#connection-pooling",
    "href": "slides/03-databases.html#connection-pooling",
    "title": "Databases",
    "section": "Connection Pooling",
    "text": "Connection Pooling\nReuse database connections:\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.pool import QueuePool\n\nengine = create_engine(\n    'postgresql://user:pass@localhost/db',\n    poolclass=QueuePool,\n    pool_size=10,\n    max_overflow=20\n)\nBenefits:\n\nFaster connections\nLimited concurrent connections\nBetter resource management"
  },
  {
    "objectID": "slides/03-databases.html#best-practices",
    "href": "slides/03-databases.html#best-practices",
    "title": "Databases",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse parameterized queries: Prevent SQL injection\nIndex appropriately: Balance read/write performance\nUse transactions: Ensure data consistency\nClose connections: Avoid resource leaks\nHandle errors: Catch and log database errors\nLimit result sets: Use LIMIT/pagination\nMonitor performance: Log slow queries"
  },
  {
    "objectID": "slides/03-databases.html#common-pitfalls",
    "href": "slides/03-databases.html#common-pitfalls",
    "title": "Databases",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nSQL injection vulnerabilities\nNot closing connections\nN+1 query problem (ORMs)\nMissing indexes on large tables\nNot using transactions for multi-step operations\nFetching too much data at once"
  },
  {
    "objectID": "slides/03-databases.html#hands-on-exercise",
    "href": "slides/03-databases.html#hands-on-exercise",
    "title": "Databases",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\nBuild a script that:\n\nConnects to a database\nCreates a table\nInserts sample data\nQueries and filters data\n(BONUS) Uses transactions\n(BONUS) Handles errors properly"
  },
  {
    "objectID": "slides/03-databases.html#questions",
    "href": "slides/03-databases.html#questions",
    "title": "Databases",
    "section": "Questions?",
    "text": "Questions?\nNext session: Data Cleaning"
  },
  {
    "objectID": "slides/03-databases.html#resources",
    "href": "slides/03-databases.html#resources",
    "title": "Databases",
    "section": "Resources",
    "text": "Resources\n\nSQL Tutorial\nSQLAlchemy Documentation\nPostgreSQL Tutorial\ndbplyr Documentation"
  },
  {
    "objectID": "slides/01-introduction.html",
    "href": "slides/01-introduction.html",
    "title": "Introduction to Data Pipelines",
    "section": "",
    "text": "Welcome to the CFA Data Pipelines Workshop\n\nIntroduction to workshop objectives\nOverview of data pipelines\nSetting up your environment"
  },
  {
    "objectID": "slides/01-introduction.html#welcome",
    "href": "slides/01-introduction.html#welcome",
    "title": "Introduction to Data Pipelines",
    "section": "",
    "text": "Welcome to the CFA Data Pipelines Workshop\n\nIntroduction to workshop objectives\nOverview of data pipelines\nSetting up your environment"
  },
  {
    "objectID": "slides/01-introduction.html#course-structure",
    "href": "slides/01-introduction.html#course-structure",
    "title": "Introduction to Data Pipelines",
    "section": "Course Structure",
    "text": "Course Structure\n\nSession 1: Introduction (Now!)\nSession 2: Data Acquisition from APIs\nSession 3: Database Queries\nSession 4: Data Cleaning"
  },
  {
    "objectID": "slides/01-introduction.html#resources",
    "href": "slides/01-introduction.html#resources",
    "title": "Introduction to Data Pipelines",
    "section": "Resources",
    "text": "Resources\n\nCourse website: GitHub\nCourse repository: GitHub\nDocumentation links will be provided\nExample code in repository"
  },
  {
    "objectID": "slides/01-introduction.html#disclaimer",
    "href": "slides/01-introduction.html#disclaimer",
    "title": "Introduction to Data Pipelines",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe views and opinions expressed in this presentation are those of the presenter and do not necessarily reflect the official policy or position of the University of Utah, the Center for Forecasting and Outbreak Analytics (CFA), or any other affiliated organizations."
  },
  {
    "objectID": "slides/01-introduction.html#ai-disclosure",
    "href": "slides/01-introduction.html#ai-disclosure",
    "title": "Introduction to Data Pipelines",
    "section": "AI Disclosure",
    "text": "AI Disclosure\nThis presentation was created with the assistance of artificial intelligence tools, including:\n\nGitHub Copilot for code generation and suggestions\nLarge language models for content refinement and structure\n\nAll content has been reviewed and validated by the presenter."
  },
  {
    "objectID": "slides/01-introduction.html#newish-r-conventions",
    "href": "slides/01-introduction.html#newish-r-conventions",
    "title": "Introduction to Data Pipelines",
    "section": "Newish R Conventions",
    "text": "Newish R Conventions\n\nThe pipe operators\nLambda functions"
  },
  {
    "objectID": "slides/01-introduction.html#the-pipe-operator",
    "href": "slides/01-introduction.html#the-pipe-operator",
    "title": "Introduction to Data Pipelines",
    "section": "The Pipe Operator",
    "text": "The Pipe Operator\nThe pipe operator (%&gt;% from magrittr or |&gt; native in R ‚â•4.1) chains operations together for cleaner, more readable code.\n\n\nWithout pipe:\n# Nested functions (hard to read)\nresult &lt;- round(\n  mean(\n    filter(data, value &gt; 0)$value\n  ), \n  2\n)\n\n# Multiple intermediate variables\nfiltered &lt;- filter(data, value &gt; 0)\nvalues &lt;- filtered$value\navg &lt;- mean(values)\nresult &lt;- round(avg, 2)\n\nWith pipe:\n# Clean, left-to-right flow\nresult &lt;- data |&gt;\n  filter(value &gt; 0) |&gt;\n  pull(value) |&gt;\n  mean() |&gt;\n  round(2)\n\n# Read as: \"take data, then\n# filter, then extract values,\n# then calculate mean, then round\""
  },
  {
    "objectID": "slides/01-introduction.html#lambda-functions-anonymous-functions",
    "href": "slides/01-introduction.html#lambda-functions-anonymous-functions",
    "title": "Introduction to Data Pipelines",
    "section": "Lambda Functions (Anonymous Functions)",
    "text": "Lambda Functions (Anonymous Functions)\nShort, unnamed functions created on-the-fly.\n\n\nR:\n# Traditional function\nsquare &lt;- function(x) x^2\nlapply(1:5, square)\n\n# Lambda/anonymous function\nlapply(1:5, function(x) x^2)\n\n# New R ‚â•4.1 shorthand\nlapply(1:5, \\(x) x^2)\n\nPython:\n# Traditional function\ndef square(x):\n    return x**2\nlist(map(square, range(1, 6)))\n\n# Lambda function\nlist(map(lambda x: x**2, range(1, 6)))\n\n# List comprehension (often clearer)\n[x**2 for x in range(1, 6)]"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-a-data-pipeline",
    "href": "slides/01-introduction.html#what-is-a-data-pipeline",
    "title": "Introduction to Data Pipelines",
    "section": "What is a Data Pipeline?",
    "text": "What is a Data Pipeline?\nA data pipeline is a series of data processing steps where:\n\nData is ingested from one or more sources\nData is transformed or processed\nData is loaded into a destination for analysis or storage"
  },
  {
    "objectID": "slides/01-introduction.html#why-data-pipelines",
    "href": "slides/01-introduction.html#why-data-pipelines",
    "title": "Introduction to Data Pipelines",
    "section": "Why Data Pipelines?",
    "text": "Why Data Pipelines?\n\n\nBenefits\n\nAutomation\nReproducibility\nScalability\nError handling\n\n\nUse Cases\n\nETL processes\nReal-time analytics\nData warehousing\nML model training"
  },
  {
    "objectID": "slides/01-introduction.html#pipeline-design-patterns",
    "href": "slides/01-introduction.html#pipeline-design-patterns",
    "title": "Introduction to Data Pipelines",
    "section": "Pipeline Design Patterns",
    "text": "Pipeline Design Patterns\n\n1. Extract-Transform-Load (ETL)\n\nExtract data from sources\nTransform in staging area\nLoad into destination\n\n\n\n2. Extract-Load-Transform (ELT)\n\nExtract data from sources\nLoad into destination\nTransform in destination"
  },
  {
    "objectID": "slides/01-introduction.html#common-data-sources",
    "href": "slides/01-introduction.html#common-data-sources",
    "title": "Introduction to Data Pipelines",
    "section": "Common Data Sources",
    "text": "Common Data Sources\n\nAPIs: REST, GraphQL, SOAP\nDatabases: SQL, NoSQL\nFiles: CSV, JSON, XML, Parquet\nStreaming: Kafka, message queues, logs\nWeb scraping: HTML parsing"
  },
  {
    "objectID": "slides/01-introduction.html#pipeline-components",
    "href": "slides/01-introduction.html#pipeline-components",
    "title": "Introduction to Data Pipelines",
    "section": "Pipeline Components",
    "text": "Pipeline Components\n\n\n\n\n\nflowchart LR\n    A[Data Source] --&gt; B[Extraction]\n    B --&gt; C[Validation]\n    C --&gt; D[Transformation]\n    D --&gt; E[Loading]\n    E --&gt; F[Destination]"
  },
  {
    "objectID": "slides/01-introduction.html#best-practices",
    "href": "slides/01-introduction.html#best-practices",
    "title": "Introduction to Data Pipelines",
    "section": "Best Practices",
    "text": "Best Practices\n\nIdempotency: Pipeline can run multiple times safely\nError Handling: Graceful failures and retries\nLogging: Track pipeline execution\nMonitoring: Alert on failures\nTesting: Validate data quality\nDocumentation: Clear pipeline logic"
  },
  {
    "objectID": "slides/01-introduction.html#idempotency-definition",
    "href": "slides/01-introduction.html#idempotency-definition",
    "title": "Introduction to Data Pipelines",
    "section": "Idempotency (Definition)",
    "text": "Idempotency (Definition)\n\nAn operation (task, step, or entire pipeline run) is idempotent if executing it multiple times with the same inputs results in the same final state and does not create duplicate side effects (e.g., repeated inserts, duplicate files, double-charged API calls)."
  },
  {
    "objectID": "slides/01-introduction.html#idempotency-key-strategies",
    "href": "slides/01-introduction.html#idempotency-key-strategies",
    "title": "Introduction to Data Pipelines",
    "section": "Idempotency Key strategies:",
    "text": "Idempotency Key strategies:\n\nUse upserts/merges instead of plain inserts (match on a natural or surrogate key).\nWrite outputs atomically (temp file then rename) to avoid partial artifacts.\nMake transformations pure: avoid relying on hidden global state or timestamps unless required.\nGuard external side effects (e.g., check if data already fetched; hash content before reprocessing).\nDesign idempotent DAG tasks: each task declares its inputs and outputs; reruns only recompute what is invalidated.\n\nBenefits: safer reruns after failure, reproducibility, easier recovery, and confidence in scheduled (e.g., nightly) executions."
  },
  {
    "objectID": "slides/01-introduction.html#error-handling-strategies",
    "href": "slides/01-introduction.html#error-handling-strategies",
    "title": "Introduction to Data Pipelines",
    "section": "Error Handling Strategies",
    "text": "Error Handling Strategies\n\n\nDetection\n\nInput validation\nSchema checks\nData quality tests\nException catching\n\n\nRecovery\n\nRetry with backoff\nDead letter queues (i.e.¬†Human intervention needed queue)\nPartial success handling\nAlerting and notifications"
  },
  {
    "objectID": "slides/01-introduction.html#logging-and-monitoring",
    "href": "slides/01-introduction.html#logging-and-monitoring",
    "title": "Introduction to Data Pipelines",
    "section": "Logging and Monitoring",
    "text": "Logging and Monitoring\n\n\nLogging\n\nStructured logs (JSON)\nLog levels (DEBUG, INFO, WARNING, ERROR)\nContextual information (timestamps, user IDs)\nCentralized log storage\n\n\nMonitoring\n\nPipeline execution metrics\nData quality metrics\nPerformance dashboards\nAlerting thresholds\nSLA (Service Level Agreement) tracking"
  },
  {
    "objectID": "slides/01-introduction.html#documentation-best-practices",
    "href": "slides/01-introduction.html#documentation-best-practices",
    "title": "Introduction to Data Pipelines",
    "section": "Documentation Best Practices",
    "text": "Documentation Best Practices\n\n\nWhat to Document\n\nPipeline purpose and business logic\nData sources and schemas\nTransformation rules\nDependencies and prerequisites\nExpected runtime and resources\nContact information\n\n\nHow to Document\n\nREADME files in repositories\nInline code comments\nDocstrings for functions\nData dictionaries\nArchitecture diagrams\nRunbooks for operators (Advanced)"
  },
  {
    "objectID": "slides/01-introduction.html#tools-and-frameworks",
    "href": "slides/01-introduction.html#tools-and-frameworks",
    "title": "Introduction to Data Pipelines",
    "section": "Tools and Frameworks",
    "text": "Tools and Frameworks\n\n\nPython\n\nApache Airflow\nLuigi\nPrefect\nDagster\n\n\nR\n\ntargets\ndrake (legacy, superceeded by targets.)\ncustom workflows"
  },
  {
    "objectID": "slides/01-introduction.html#development-environment-setup",
    "href": "slides/01-introduction.html#development-environment-setup",
    "title": "Introduction to Data Pipelines",
    "section": "Development Environment Setup",
    "text": "Development Environment Setup\n\nPython Setup\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\n\n# Install common packages\npip install requests pandas sqlalchemy\n\n\nR Setup\n# Install common packages\ninstall.packages(c(\"httr2\", \"dplyr\", \"DBI\", \"dbplyr\"))"
  },
  {
    "objectID": "slides/01-introduction.html#your-first-pipeline",
    "href": "slides/01-introduction.html#your-first-pipeline",
    "title": "Introduction to Data Pipelines",
    "section": "Your First Pipeline",
    "text": "Your First Pipeline\nLet‚Äôs build a simple pipeline:\n\nFetch data from an API\nClean and validate\nStore in a database\n\nWe‚Äôll start with the basics and build complexity throughout the course."
  },
  {
    "objectID": "slides/01-introduction.html#example-pipeline-weather-data",
    "href": "slides/01-introduction.html#example-pipeline-weather-data",
    "title": "Introduction to Data Pipelines",
    "section": "Example Pipeline: Weather Data",
    "text": "Example Pipeline: Weather Data\n\nFetch weather data ‚Üí Clean ‚Üí Store\nWe will be fetching daily precipitation for Brighton Ski Resort for the last year.\nR Example:\n\n## Needed Packages #####\nlibrary(httr2)      #&lt; interacting with API (modern version)\n\nWarning: package 'httr2' was built under R version 4.5.2\n\nlibrary(dplyr)      #&lt; data cleaning and processing\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(DBI)        #&lt; generic database interface\nlibrary(RSQLite)    #&lt; specific database we will use\n\nWarning: package 'RSQLite' was built under R version 4.5.2\n\nlibrary(ggplot2)    #&lt; for visualization"
  },
  {
    "objectID": "slides/01-introduction.html#r-example-pipeline-continued",
    "href": "slides/01-introduction.html#r-example-pipeline-continued",
    "title": "Introduction to Data Pipelines",
    "section": "R Example Pipeline (continued)",
    "text": "R Example Pipeline (continued)\nStep 1: Fetch data from Open-Meteo API (Brighton Ski Resort)\n\nend_date &lt;- Sys.Date()\nstart_date &lt;- end_date - 365\n\nrequest(\"https://archive-api.open-meteo.com/v1/archive\") |&gt;\n  req_url_query(\n    latitude = 40.5981,\n    longitude = -111.5831,\n    start_date = format(start_date, \"%Y-%m-%d\"),\n    end_date = format(end_date, \"%Y-%m-%d\"),\n    daily = \"precipitation_sum,snowfall_sum\",\n    timezone = \"America/Denver\"\n  ) |&gt;\n  req_perform() |&gt;\n  resp_body_json() -&gt; data"
  },
  {
    "objectID": "slides/01-introduction.html#r-example-pipeline-continued-1",
    "href": "slides/01-introduction.html#r-example-pipeline-continued-1",
    "title": "Introduction to Data Pipelines",
    "section": "R Example Pipeline (continued)",
    "text": "R Example Pipeline (continued)\nStep 2: Clean and validate\n\ndf &lt;- data.frame(\n  date = as.Date(unlist(data$daily$time)),\n  precipitation_mm = unlist(data$daily$precipitation_sum),\n  snowfall_cm = unlist(data$daily$snowfall_sum)\n)\n\ndf &lt;- df %&gt;%\n  # Remove nulls and validate ranges\n  filter(!is.na(precipitation_mm), !is.na(snowfall_cm)) %&gt;%\n  filter(precipitation_mm &gt;= 0, snowfall_cm &gt;= 0) %&gt;%\n  # Add derived variables\n  mutate(\n    snow_water_equiv_mm = snowfall_cm * 10,\n    rain_mm = pmax(0, precipitation_mm - snow_water_equiv_mm)\n  )"
  },
  {
    "objectID": "slides/01-introduction.html#r-example-pipeline-continued-2",
    "href": "slides/01-introduction.html#r-example-pipeline-continued-2",
    "title": "Introduction to Data Pipelines",
    "section": "R Example Pipeline (continued)",
    "text": "R Example Pipeline (continued)\nStep 3: Store in SQLite database\n\nconn &lt;- dbConnect(RSQLite::SQLite(), \"weather_data.db\")\ndbWriteTable(conn, \"brighton_weather\", df, overwrite = TRUE)\ndbDisconnect(conn)\n\ncat(sprintf(\"Stored %d records\\n\", nrow(df)))\n\nStored 366 records\n\ncat(sprintf(\"Total snowfall: %.1f cm\\n\", sum(df$snowfall_cm)))\n\nTotal snowfall: 542.1 cm"
  },
  {
    "objectID": "slides/01-introduction.html#r-example-pipeline-finished",
    "href": "slides/01-introduction.html#r-example-pipeline-finished",
    "title": "Introduction to Data Pipelines",
    "section": "R Example Pipeline (finished)",
    "text": "R Example Pipeline (finished)\nStep 4: Do something with the data\n\n# Create stacked area plot\nggplot(df, aes(x = date)) +\n  geom_area(aes(y = rain_mm, fill = \"Rain\"), alpha = 0.7) +\n  geom_area(aes(y = snow_water_equiv_mm, fill = \"Snow\"), alpha = 0.7) +\n  scale_fill_manual(values = c(\"Rain\" = \"steelblue\", \"Snow\" = \"lightblue\")) +\n  labs(\n    title = \"Brighton Ski Resort: Snow vs Rain Precipitation\",\n    x = \"Date\",\n    y = \"Precipitation (mm water equivalent)\",\n    fill = \"Type\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/01-introduction.html#r-example-pipeline-plot",
    "href": "slides/01-introduction.html#r-example-pipeline-plot",
    "title": "Introduction to Data Pipelines",
    "section": "R Example Pipeline (plot)",
    "text": "R Example Pipeline (plot)"
  },
  {
    "objectID": "slides/01-introduction.html#python-example-pipeline-weather-data",
    "href": "slides/01-introduction.html#python-example-pipeline-weather-data",
    "title": "Introduction to Data Pipelines",
    "section": "Python Example Pipeline: Weather Data",
    "text": "Python Example Pipeline: Weather Data\n\n## Needed Packages #####\nimport requests     # interacting with API\nimport pandas as pd # data cleaning and processing\nimport sqlite3      # database interface\nfrom datetime import datetime, timedelta"
  },
  {
    "objectID": "slides/01-introduction.html#python-example-pipeline-continued",
    "href": "slides/01-introduction.html#python-example-pipeline-continued",
    "title": "Introduction to Data Pipelines",
    "section": "Python Example Pipeline (continued)",
    "text": "Python Example Pipeline (continued)\nStep 1: Fetch data from Open-Meteo API (Brighton Ski Resort)\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365)\n\nurl = \"https://archive-api.open-meteo.com/v1/archive\"\nparams = {\n    'latitude': 40.5981,\n    'longitude': -111.5831,\n    'start_date': start_date.strftime('%Y-%m-%d'),\n    'end_date': end_date.strftime('%Y-%m-%d'),\n    'daily': 'precipitation_sum,snowfall_sum',\n    'timezone': 'America/Denver'\n}\n\nresponse = requests.get(url, params=params)\ndata = response.json()"
  },
  {
    "objectID": "slides/01-introduction.html#python-example-pipeline-continued-1",
    "href": "slides/01-introduction.html#python-example-pipeline-continued-1",
    "title": "Introduction to Data Pipelines",
    "section": "Python Example Pipeline (continued)",
    "text": "Python Example Pipeline (continued)\nStep 2: Clean and validate\n\ndf = pd.DataFrame({\n    'date': pd.to_datetime(data['daily']['time']),\n    'precipitation_mm': data['daily']['precipitation_sum'],\n    'snowfall_cm': data['daily']['snowfall_sum']\n})\n\n# Remove nulls and validate ranges\ndf = df.dropna()\ndf = df[(df['precipitation_mm'] &gt;= 0) & (df['snowfall_cm'] &gt;= 0)]\n\n# Add derived variables\ndf['snow_water_equiv_mm'] = df['snowfall_cm'] * 10\ndf['rain_mm'] = (df['precipitation_mm'] - df['snow_water_equiv_mm']).clip(lower=0)"
  },
  {
    "objectID": "slides/01-introduction.html#python-example-pipeline-continued-2",
    "href": "slides/01-introduction.html#python-example-pipeline-continued-2",
    "title": "Introduction to Data Pipelines",
    "section": "Python Example Pipeline (continued)",
    "text": "Python Example Pipeline (continued)\nStep 3: Store in SQLite database\n\nconn = sqlite3.connect('weather_data_py.db')\ndf.to_sql('brighton_weather', conn, if_exists='replace', index=False)\n\n366\n\nconn.close()\n\nprint(f\"Stored {len(df)} records\")\n\nStored 366 records\n\nprint(f\"Total snowfall: {df['snowfall_cm'].sum():.1f} cm\")\n\nTotal snowfall: 542.1 cm"
  },
  {
    "objectID": "slides/01-introduction.html#python-example-pipeline-finished",
    "href": "slides/01-introduction.html#python-example-pipeline-finished",
    "title": "Introduction to Data Pipelines",
    "section": "Python Example Pipeline (finished)",
    "text": "Python Example Pipeline (finished)\nStep 4: Do something with the data\n\nimport matplotlib.pyplot as plt\n\n# Create stacked area plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.fill_between(df['date'], 0, df['rain_mm'], \n                label='Rain', alpha=0.7, color='steelblue')\nax.fill_between(df['date'], df['rain_mm'], \n                df['rain_mm'] + df['snow_water_equiv_mm'],\n                label='Snow', alpha=0.7, color='lightblue')\nax.set_xlabel('Date')\nax.set_ylabel('Precipitation (mm water equivalent)')\nax.set_title('Brighton Ski Resort: Snow vs Rain Precipitation')\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/01-introduction.html#python-example-pipeline-plot",
    "href": "slides/01-introduction.html#python-example-pipeline-plot",
    "title": "Introduction to Data Pipelines",
    "section": "Python Example Pipeline (plot)",
    "text": "Python Example Pipeline (plot)"
  },
  {
    "objectID": "slides/01-introduction.html#questions",
    "href": "slides/01-introduction.html#questions",
    "title": "Introduction to Data Pipelines",
    "section": "Questions?",
    "text": "Questions?\nNext session: Data Acquisition from APIs"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This schedule is tentative and may be adjusted as needed. Check back regularly for updates.\n\n\nDate: TBD\nTopics:\n\nCourse overview and objectives\nWhat are data pipelines?\nPipeline design patterns and best practices\nSetting up your development environment\n\nMaterials:\n\nIntroduction Slides\n\nAssignments:\n\nSet up development environment\nComplete introductory exercise\n\n\n\n\n\nDate: TBD\nTopics:\n\nRESTful API fundamentals\nHTTP methods and status codes\nAuthentication methods (API keys, OAuth)\nHandling pagination\nRate limiting and best practices\n\nMaterials:\n\nAPI Slides\n\nAssignments:\n\nExercise: Fetch data from a public API\nImplement error handling for API requests\n\n\n\n\n\nDate: TBD\nTopics:\n\nSQL fundamentals review\nConnecting to databases (SQLite, PostgreSQL, MySQL)\nWriting efficient queries\nWorking with ORMs (SQLAlchemy, Django ORM)\nTransaction management and ACID properties\n\nMaterials:\n\nDatabase Slides\n\nAssignments:\n\nExercise: Query a sample database\nCompare raw SQL vs ORM approaches\n\n\n\n\n\nDate: TBD\nTopics:\n\nCommon data quality issues\nMissing data strategies\nData validation and schema enforcement\nData transformation techniques\nDocumenting your cleaning process\n\nMaterials:\n\nData Cleaning Slides\n\nAssignments:\n\nExercise: Clean a messy dataset\nDocument your cleaning pipeline\n\n\n\n\n\nDue Date: TBD\nRequirements:\n\nBuild a complete data pipeline that:\n\nAcquires data from an API or database\nCleans and validates the data\nProduces analysis-ready output\nIncludes proper error handling and logging\nIs well-documented\n\n\nPresentation: TBD"
  },
  {
    "objectID": "schedule.html#course-schedule",
    "href": "schedule.html#course-schedule",
    "title": "Schedule",
    "section": "",
    "text": "This schedule is tentative and may be adjusted as needed. Check back regularly for updates.\n\n\nDate: TBD\nTopics:\n\nCourse overview and objectives\nWhat are data pipelines?\nPipeline design patterns and best practices\nSetting up your development environment\n\nMaterials:\n\nIntroduction Slides\n\nAssignments:\n\nSet up development environment\nComplete introductory exercise\n\n\n\n\n\nDate: TBD\nTopics:\n\nRESTful API fundamentals\nHTTP methods and status codes\nAuthentication methods (API keys, OAuth)\nHandling pagination\nRate limiting and best practices\n\nMaterials:\n\nAPI Slides\n\nAssignments:\n\nExercise: Fetch data from a public API\nImplement error handling for API requests\n\n\n\n\n\nDate: TBD\nTopics:\n\nSQL fundamentals review\nConnecting to databases (SQLite, PostgreSQL, MySQL)\nWriting efficient queries\nWorking with ORMs (SQLAlchemy, Django ORM)\nTransaction management and ACID properties\n\nMaterials:\n\nDatabase Slides\n\nAssignments:\n\nExercise: Query a sample database\nCompare raw SQL vs ORM approaches\n\n\n\n\n\nDate: TBD\nTopics:\n\nCommon data quality issues\nMissing data strategies\nData validation and schema enforcement\nData transformation techniques\nDocumenting your cleaning process\n\nMaterials:\n\nData Cleaning Slides\n\nAssignments:\n\nExercise: Clean a messy dataset\nDocument your cleaning pipeline\n\n\n\n\n\nDue Date: TBD\nRequirements:\n\nBuild a complete data pipeline that:\n\nAcquires data from an API or database\nCleans and validates the data\nProduces analysis-ready output\nIncludes proper error handling and logging\nIs well-documented\n\n\nPresentation: TBD"
  },
  {
    "objectID": "schedule.html#important-dates",
    "href": "schedule.html#important-dates",
    "title": "Schedule",
    "section": "Important Dates",
    "text": "Important Dates\n\n\n\nDate\nEvent\n\n\n\n\nTBD\nFirst Class\n\n\nTBD\nExercise 1 Due\n\n\nTBD\nExercise 2 Due\n\n\nTBD\nExercise 3 Due\n\n\nTBD\nExercise 4 Due\n\n\nTBD\nFinal Project Presentations\n\n\nTBD\nFinal Project Due"
  },
  {
    "objectID": "schedule.html#office-hours",
    "href": "schedule.html#office-hours",
    "title": "Schedule",
    "section": "Office Hours",
    "text": "Office Hours\nOffice hours will be held weekly. Schedule TBD."
  },
  {
    "objectID": "post-survey.html",
    "href": "post-survey.html",
    "title": "Post-Workshop Survey",
    "section": "",
    "text": "Please complete after the final session. Estimated time: 6‚Äì8 minutes. Your feedback measures learning gains and improves future workshops."
  },
  {
    "objectID": "post-survey.html#instructions",
    "href": "post-survey.html#instructions",
    "title": "Post-Workshop Survey",
    "section": "",
    "text": "Please complete after the final session. Estimated time: 6‚Äì8 minutes. Your feedback measures learning gains and improves future workshops."
  },
  {
    "objectID": "post-survey.html#section-1-learning-gains-retain-identical-wording-from-pre-for-comparison",
    "href": "post-survey.html#section-1-learning-gains-retain-identical-wording-from-pre-for-comparison",
    "title": "Post-Workshop Survey",
    "section": "Section 1: Learning Gains (Retain identical wording from pre for comparison)",
    "text": "Section 1: Learning Gains (Retain identical wording from pre for comparison)\n(Likert 1=Strongly Disagree ‚Ä¶ 5=Strongly Agree) 1. I have built data workflows before. 2. I can query databases confidently. 3. I have retrieved data from REST APIs. 4. I understand principles of data cleaning and validation. 5. I can design a reproducible data structure. 6. I feel confident troubleshooting data pipeline failures. 7. I can choose appropriate tools for data acquisition. 8. I can assess data quality issues systematically. 9. I can document a pipeline for other users."
  },
  {
    "objectID": "post-survey.html#section-2-knowledge-check-same-items-optional-randomized-order",
    "href": "post-survey.html#section-2-knowledge-check-same-items-optional-randomized-order",
    "title": "Post-Workshop Survey",
    "section": "Section 2: Knowledge Check (Same items; optional randomized order)",
    "text": "Section 2: Knowledge Check (Same items; optional randomized order)\n\nA data pipeline is best described as: (a) A single script; (b) A series of connected processes moving and transforming data; (c) A database schema; (d) A visualization tool.\nThe HTTP status code 429 indicates: (a) Success; (b) Not Found; (c) Too Many Requests; (d) Unauthorized.\nIn SQL, selecting unique values uses: (a) DISTINCT; (b) UNIQUE; (c) GROUP; (d) ONLY.\nBest first step when cleaning a dataset: (a) Delete all NAs; (b) Profile structure and summary statistics; (c) Normalize everything; (d) Load into production.\nAn API rate limit is: (a) Maximum query complexity; (b) Restriction on number of requests per time window; (c) Disk quota; (d) Memory threshold."
  },
  {
    "objectID": "post-survey.html#section-3-satisfaction-likert-15",
    "href": "post-survey.html#section-3-satisfaction-likert-15",
    "title": "Post-Workshop Survey",
    "section": "Section 3: Satisfaction (Likert 1‚Äì5)",
    "text": "Section 3: Satisfaction (Likert 1‚Äì5)\n\nThe workshop met my expectations.\nThe pacing was appropriate.\nThe examples clarified the concepts.\nThe slides/materials were well organized.\nI would recommend this workshop to others."
  },
  {
    "objectID": "post-survey.html#section-4-application-impact",
    "href": "post-survey.html#section-4-application-impact",
    "title": "Post-Workshop Survey",
    "section": "Section 4: Application & Impact",
    "text": "Section 4: Application & Impact\n\nHow soon do you expect to apply what you learned? (Immediate, &lt;1 month, 1‚Äì3 months, &gt;3 months, Unsure)\nWhich topic was MOST valuable? (APIs, Databases, Cleaning, Pipeline Design, Other)\nWhich topic needs more depth next time? (APIs, Databases, Cleaning, Pipeline Design, Other)\nList one concrete change you will make to your current workflow. (long text)"
  },
  {
    "objectID": "post-survey.html#section-5-open-feedback",
    "href": "post-survey.html#section-5-open-feedback",
    "title": "Post-Workshop Survey",
    "section": "Section 5: Open Feedback",
    "text": "Section 5: Open Feedback\n\nWhat was the most helpful part? (long text)\nWhat was least helpful / could be improved? (long text)\nAdditional comments or suggestions. (long text)"
  },
  {
    "objectID": "docker-example/START_HERE.html",
    "href": "docker-example/START_HERE.html",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "Welcome! This folder contains a complete, working example of a data pipeline using Docker containers.\n\n\nA hands-on example demonstrating: - ‚úÖ R Plumber REST API in a Docker container - ‚úÖ PostgreSQL database in a Docker container\n- ‚úÖ R client that pulls from both sources - ‚úÖ Python client that pulls from both sources - ‚úÖ Real healthcare data scenarios\n\n\n\n\n\nüëâ Read README.md - Complete setup guide with explanations\n\n\n\nüëâ Read QUICK_REFERENCE.md - Essential commands only\n\n\n\nüëâ Read ARCHITECTURE.md - Visual diagrams and data flow\n\n\n\nüëâ Read FILE_SUMMARY.md - What each file does\n\n\n\n\n# 1. Start the containers\ndocker-compose up -d --build\n\n# 2. Run the pipeline (choose R or Python)\nRscript r-client/healthcare_pipeline.R\n# OR\npython python-client/healthcare_pipeline.py\n\n# 3. Stop when done\ndocker-compose down\n\n\n\ndocker-example/\n‚îú‚îÄ‚îÄ üìñ README.md              ‚Üê Start here if new to this\n‚îú‚îÄ‚îÄ üìñ QUICK_REFERENCE.md     ‚Üê Commands cheat sheet\n‚îú‚îÄ‚îÄ üìñ ARCHITECTURE.md        ‚Üê Visual diagrams\n‚îú‚îÄ‚îÄ üìñ FILE_SUMMARY.md        ‚Üê File-by-file breakdown\n‚îú‚îÄ‚îÄ üê≥ docker-compose.yml     ‚Üê Runs both containers\n‚îú‚îÄ‚îÄ üìÇ api/                   ‚Üê R Plumber API code\n‚îú‚îÄ‚îÄ üìÇ database/              ‚Üê PostgreSQL setup\n‚îú‚îÄ‚îÄ üìÇ r-client/              ‚Üê R pipeline script\n‚îî‚îÄ‚îÄ üìÇ python-client/         ‚Üê Python pipeline script\n\n\n\nBy working through this example, you‚Äôll learn:\n\nDocker Basics\n\nBuilding custom images\nMulti-container applications\nContainer networking\nVolume persistence\n\nAPI Integration\n\nCreating REST APIs with R Plumber\nMaking HTTP GET/POST requests\nHandling JSON data\n\nDatabase Operations\n\nPostgreSQL in Docker\nSQL queries and joins\nConnecting from R and Python\n\nData Pipelines\n\nExtracting from multiple sources\nCombining datasets\nWriting to databases\nError handling\n\n\n\n\n\n\nDocker Desktop installed and running\nR (optional, for R client)\nPython (optional, for Python client)\n\n\n\n\n\nContainers won‚Äôt start?\ndocker-compose logs\nNeed to verify everything works?\n# Windows\n.\\validate.ps1\n\n# Linux/Mac\n./validate.sh\nWant to start fresh?\ndocker-compose down -v\ndocker-compose up -d --build\n\n\n\n\nAPI Container          Database Container\n(Patient Data)         (Visit/Lab Data)\n     ‚îÇ                        ‚îÇ\n     ‚îÇ                        ‚îÇ\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n              ‚îÇ\n         Your Pipeline\n              ‚îÇ\n      Combined Patient + \n       Visit Summary\n\n\n\n\n‚úÖ Run both R and Python versions - compare the code\n‚úÖ Modify the API - add a new endpoint\n‚úÖ Add a database table - extend the schema\n‚úÖ Enhance the pipeline - add data validation\n‚úÖ Deploy to production - add authentication, monitoring\n\n\n\n\n\nUse docker-compose logs -f to watch container output in real-time\nThe API data is in-memory (resets on restart)\nThe database data persists in a Docker volume\nBoth pipelines do the same thing - learn from the differences!\n\n\n\n\nThis example brings together all 4 workshop sessions:\n\n\n\nSession\nConcepts Applied\n\n\n\n\n1\nEnvironment setup, git, package management\n\n\n2\nREST APIs, HTTP requests, authentication\n\n\n3\nDatabase connections, SQL, upserts\n\n\n4\nData cleaning, pipeline integration\n\n\n\n\n\n\n\nOpen README.md for the full guide\nOr jump right in with the Quick Start above\n\nHappy Pipeline Building! üöÄ\n\nQuestions? See the main workshop repository or ask in the discussion board."
  },
  {
    "objectID": "docker-example/START_HERE.html#what-this-is",
    "href": "docker-example/START_HERE.html#what-this-is",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "A hands-on example demonstrating: - ‚úÖ R Plumber REST API in a Docker container - ‚úÖ PostgreSQL database in a Docker container\n- ‚úÖ R client that pulls from both sources - ‚úÖ Python client that pulls from both sources - ‚úÖ Real healthcare data scenarios"
  },
  {
    "objectID": "docker-example/START_HERE.html#where-to-start",
    "href": "docker-example/START_HERE.html#where-to-start",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "üëâ Read README.md - Complete setup guide with explanations\n\n\n\nüëâ Read QUICK_REFERENCE.md - Essential commands only\n\n\n\nüëâ Read ARCHITECTURE.md - Visual diagrams and data flow\n\n\n\nüëâ Read FILE_SUMMARY.md - What each file does"
  },
  {
    "objectID": "docker-example/START_HERE.html#super-quick-start-3-commands",
    "href": "docker-example/START_HERE.html#super-quick-start-3-commands",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "# 1. Start the containers\ndocker-compose up -d --build\n\n# 2. Run the pipeline (choose R or Python)\nRscript r-client/healthcare_pipeline.R\n# OR\npython python-client/healthcare_pipeline.py\n\n# 3. Stop when done\ndocker-compose down"
  },
  {
    "objectID": "docker-example/START_HERE.html#whats-inside",
    "href": "docker-example/START_HERE.html#whats-inside",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "docker-example/\n‚îú‚îÄ‚îÄ üìñ README.md              ‚Üê Start here if new to this\n‚îú‚îÄ‚îÄ üìñ QUICK_REFERENCE.md     ‚Üê Commands cheat sheet\n‚îú‚îÄ‚îÄ üìñ ARCHITECTURE.md        ‚Üê Visual diagrams\n‚îú‚îÄ‚îÄ üìñ FILE_SUMMARY.md        ‚Üê File-by-file breakdown\n‚îú‚îÄ‚îÄ üê≥ docker-compose.yml     ‚Üê Runs both containers\n‚îú‚îÄ‚îÄ üìÇ api/                   ‚Üê R Plumber API code\n‚îú‚îÄ‚îÄ üìÇ database/              ‚Üê PostgreSQL setup\n‚îú‚îÄ‚îÄ üìÇ r-client/              ‚Üê R pipeline script\n‚îî‚îÄ‚îÄ üìÇ python-client/         ‚Üê Python pipeline script"
  },
  {
    "objectID": "docker-example/START_HERE.html#learning-objectives",
    "href": "docker-example/START_HERE.html#learning-objectives",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "By working through this example, you‚Äôll learn:\n\nDocker Basics\n\nBuilding custom images\nMulti-container applications\nContainer networking\nVolume persistence\n\nAPI Integration\n\nCreating REST APIs with R Plumber\nMaking HTTP GET/POST requests\nHandling JSON data\n\nDatabase Operations\n\nPostgreSQL in Docker\nSQL queries and joins\nConnecting from R and Python\n\nData Pipelines\n\nExtracting from multiple sources\nCombining datasets\nWriting to databases\nError handling"
  },
  {
    "objectID": "docker-example/START_HERE.html#prerequisites",
    "href": "docker-example/START_HERE.html#prerequisites",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "Docker Desktop installed and running\nR (optional, for R client)\nPython (optional, for Python client)"
  },
  {
    "objectID": "docker-example/START_HERE.html#help-troubleshooting",
    "href": "docker-example/START_HERE.html#help-troubleshooting",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "Containers won‚Äôt start?\ndocker-compose logs\nNeed to verify everything works?\n# Windows\n.\\validate.ps1\n\n# Linux/Mac\n./validate.sh\nWant to start fresh?\ndocker-compose down -v\ndocker-compose up -d --build"
  },
  {
    "objectID": "docker-example/START_HERE.html#what-youll-build",
    "href": "docker-example/START_HERE.html#what-youll-build",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "API Container          Database Container\n(Patient Data)         (Visit/Lab Data)\n     ‚îÇ                        ‚îÇ\n     ‚îÇ                        ‚îÇ\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n              ‚îÇ\n         Your Pipeline\n              ‚îÇ\n      Combined Patient + \n       Visit Summary"
  },
  {
    "objectID": "docker-example/START_HERE.html#next-steps-after-completion",
    "href": "docker-example/START_HERE.html#next-steps-after-completion",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "‚úÖ Run both R and Python versions - compare the code\n‚úÖ Modify the API - add a new endpoint\n‚úÖ Add a database table - extend the schema\n‚úÖ Enhance the pipeline - add data validation\n‚úÖ Deploy to production - add authentication, monitoring"
  },
  {
    "objectID": "docker-example/START_HERE.html#pro-tips",
    "href": "docker-example/START_HERE.html#pro-tips",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "Use docker-compose logs -f to watch container output in real-time\nThe API data is in-memory (resets on restart)\nThe database data persists in a Docker volume\nBoth pipelines do the same thing - learn from the differences!"
  },
  {
    "objectID": "docker-example/START_HERE.html#integration-with-workshop",
    "href": "docker-example/START_HERE.html#integration-with-workshop",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "This example brings together all 4 workshop sessions:\n\n\n\nSession\nConcepts Applied\n\n\n\n\n1\nEnvironment setup, git, package management\n\n\n2\nREST APIs, HTTP requests, authentication\n\n\n3\nDatabase connections, SQL, upserts\n\n\n4\nData cleaning, pipeline integration"
  },
  {
    "objectID": "docker-example/START_HERE.html#ready-to-start",
    "href": "docker-example/START_HERE.html#ready-to-start",
    "title": "üê≥ Start Here - Docker Healthcare Pipeline Example",
    "section": "",
    "text": "Open README.md for the full guide\nOr jump right in with the Quick Start above\n\nHappy Pipeline Building! üöÄ\n\nQuestions? See the main workshop repository or ask in the discussion board."
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html",
    "href": "docker-example/FILE_SUMMARY.html",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "docker-example/\n‚îÇ\n‚îú‚îÄ‚îÄ üìÑ README.md                    # Main documentation\n‚îú‚îÄ‚îÄ üìÑ QUICK_REFERENCE.md           # Command cheat sheet\n‚îú‚îÄ‚îÄ üìÑ ARCHITECTURE.md              # Visual diagrams\n‚îú‚îÄ‚îÄ üìÑ docker-compose.yml           # Container orchestration\n‚îú‚îÄ‚îÄ üìÑ .env.example                 # Environment template\n‚îú‚îÄ‚îÄ üìÑ .gitignore                   # Git ignore rules\n‚îú‚îÄ‚îÄ üìú validate.sh                  # Linux/Mac validation script\n‚îú‚îÄ‚îÄ üìú validate.ps1                 # Windows validation script\n‚îÇ\n‚îú‚îÄ‚îÄ üìÇ api/                         # R Plumber API Container\n‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile               # Container build instructions\n‚îÇ   ‚îú‚îÄ‚îÄ üìú plumber.R                # API endpoint definitions\n‚îÇ   ‚îî‚îÄ‚îÄ üìä patients_data.csv        # Sample patient data (10 records)\n‚îÇ\n‚îú‚îÄ‚îÄ üìÇ database/                    # PostgreSQL Database Container\n‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile               # Container build instructions\n‚îÇ   ‚îî‚îÄ‚îÄ üìú init.sql                 # Database schema + sample data\n‚îÇ\n‚îú‚îÄ‚îÄ üìÇ r-client/                    # R Pipeline Client\n‚îÇ   ‚îú‚îÄ‚îÄ üìú healthcare_pipeline.R    # Main pipeline script\n‚îÇ   ‚îî‚îÄ‚îÄ üìú install_packages.R       # Package installation helper\n‚îÇ\n‚îî‚îÄ‚îÄ üìÇ python-client/               # Python Pipeline Client\n    ‚îú‚îÄ‚îÄ üêç healthcare_pipeline.py   # Main pipeline script\n    ‚îî‚îÄ‚îÄ üìÑ requirements.txt         # Python dependencies\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nPurpose\n\n\n\n\nREADME.md\nComplete setup guide, architecture overview, troubleshooting\n\n\nQUICK_REFERENCE.md\nQuick command reference for common operations\n\n\nARCHITECTURE.md\nVisual diagrams of system architecture and data flow\n\n\ndocker-compose.yml\nDefines both containers, networking, volumes\n\n\n.env.example\nTemplate for environment variables\n\n\n.gitignore\nPrevents committing sensitive files\n\n\nvalidate.sh\nLinux/Mac script to verify setup\n\n\nvalidate.ps1\nWindows PowerShell script to verify setup\n\n\n\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nDockerfile\n21\nBuilds R container with Plumber\n\n\nplumber.R\n115\nREST API with 6 endpoints\n\n\npatients_data.csv\n11\n10 patient records (header + data)\n\n\n\nAPI Endpoints: - GET /health - Health check - GET /patients - List all patients - GET /patients/{id} - Get single patient - GET /patients/insurance/{id} - Filter by insurance - GET /stats - Statistics - POST /patients - Create patient\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nDockerfile\n12\nBuilds PostgreSQL container\n\n\ninit.sql\n150+\nCreates 3 tables + sample data\n\n\n\nDatabase Tables: - visits - 12 healthcare visit records - prescriptions - 7 medication prescriptions - lab_results - 12 laboratory test results\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nhealthcare_pipeline.R\n200+\nFull ETL pipeline in R\n\n\ninstall_packages.R\n10\nInstalls required R packages\n\n\n\nPipeline Steps: 1. Fetch from API (httr) 2. Query database (DBI/RPostgres) 3. Combine data (dplyr) 4. Write back to database\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nhealthcare_pipeline.py\n230+\nFull ETL pipeline in Python\n\n\nrequirements.txt\n4\nPython package dependencies\n\n\n\nPipeline Steps: 1. Fetch from API (requests) 2. Query database (psycopg2/SQLAlchemy) 3. Combine data (pandas) 4. Write back to database 5. Test POST endpoint\n\n\n\n\n\n\ndocker-compose up -d --build\n\n\n\n# Windows\n.\\validate.ps1\n\n# Linux/Mac\n./validate.sh\n\n\n\n# R version\nRscript r-client/healthcare_pipeline.R\n\n# Python version\npython python-client/healthcare_pipeline.py\n\n\n\n\n\n\n\nAPI Container (healthcare_api)\n\nLoads 10 patient records into memory\nStarts REST API on port 8000\nServes HTTP requests\n\nDatabase Container (healthcare_db)\n\nCreates PostgreSQL database\nCreates 3 tables (visits, prescriptions, lab_results)\nInserts 31 total records\nListens on port 5432\n\n\n\n\n\n\nAPI Data Fetched\n\n10 patients from /patients endpoint\nConverted to DataFrame/tibble\n\nDatabase Data Queried\n\nVisit summary aggregated\nPrescription counts calculated\nLab result counts calculated\n\nCombined Dataset Created\n\nPatient demographics (from API)\nVisit statistics (from database)\nMerged on patient_id\n\nNew Table Written\n\npatient_summary table created\n10 rows written\nContains combined data\n\n\n\n\n\n\nPatients (API)           Visits (DB)\n‚îî‚îÄ patient_id ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ patient_id\n                        ‚îÇ\n                        ‚îú‚îÄ Prescriptions (DB)\n                        ‚îÇ  ‚îî‚îÄ visit_id ‚Üí visit_id\n                        ‚îÇ\n                        ‚îî‚îÄ Lab Results (DB)\n                           ‚îî‚îÄ visit_id ‚Üí visit_id\n\n\n\n\n\n\nItem\nCount\nStorage\n\n\n\n\nPatients (API)\n10\nIn-memory CSV\n\n\nVisits (DB)\n12\nPostgreSQL volume\n\n\nPrescriptions (DB)\n7\nPostgreSQL volume\n\n\nLab Results (DB)\n12\nPostgreSQL volume\n\n\nTotal Records\n41\n~50 KB\n\n\n\n\n\n\n\nBeginner: Run both pipelines, compare outputs\nIntermediate: Modify API endpoints, add new queries\nAdvanced: Add authentication, implement caching, add monitoring\n\n\n\n\n\nAdd more API endpoints (PUT, DELETE)\nImplement authentication (API keys, JWT)\nAdd data validation (pandera, validate)\nCreate scheduled jobs (cron, APScheduler)\nAdd monitoring (Prometheus metrics)\nImplement caching (Redis)\nAdd logging aggregation (ELK stack)\nCreate web dashboard (Shiny, Streamlit)\n\n\n\n\n\n\n\nTechnology\nVersion\nPurpose\n\n\n\n\nDocker\nLatest\nContainerization\n\n\nDocker Compose\nLatest\nMulti-container orchestration\n\n\nPostgreSQL\n16\nRelational database\n\n\nR\n4.5.1\nProgramming language\n\n\nPlumber\nLatest\nR REST API framework\n\n\nPython\n3.8+\nProgramming language\n\n\nhttr\nLatest\nR HTTP client\n\n\nrequests\nLatest\nPython HTTP client\n\n\nDBI/RPostgres\nLatest\nR database interface\n\n\npsycopg2\nLatest\nPython PostgreSQL driver\n\n\ndplyr\nLatest\nR data manipulation\n\n\npandas\nLatest\nPython data manipulation\n\n\n\n\n\n\n\n\n\nTask\nDuration\n\n\n\n\nInitial build\n3-5 minutes\n\n\nContainer startup\n20-30 seconds\n\n\nHealth check ready\n10-20 seconds\n\n\nR pipeline execution\n5-10 seconds\n\n\nPython pipeline execution\n5-10 seconds\n\n\n\n\n\n\n\n\n\nContainer\nCPU\nMemory\nDisk\n\n\n\n\nhealthcare_api\n&lt;5%\n~200 MB\n~500 MB\n\n\nhealthcare_db\n&lt;5%\n~100 MB\n~100 MB\n\n\nTotal\n&lt;10%\n~300 MB\n~600 MB\n\n\n\n\n\n\nThis example integrates concepts from all 4 workshop sessions:\n\n‚úÖ Session 1: Git, environments, package management\n‚úÖ Session 2: REST APIs, HTTP requests, JSON parsing\n‚úÖ Session 3: Database connections, SQL queries, upserts\n‚úÖ Session 4: Data cleaning, pipeline integration, error handling\n\n\n\n\nCreated for the CFA Data Pipelines Workshop by the ForeSITE team.\n\nQuestions? See README.md or open an issue in the workshop repository."
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#project-structure",
    "href": "docker-example/FILE_SUMMARY.html#project-structure",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "docker-example/\n‚îÇ\n‚îú‚îÄ‚îÄ üìÑ README.md                    # Main documentation\n‚îú‚îÄ‚îÄ üìÑ QUICK_REFERENCE.md           # Command cheat sheet\n‚îú‚îÄ‚îÄ üìÑ ARCHITECTURE.md              # Visual diagrams\n‚îú‚îÄ‚îÄ üìÑ docker-compose.yml           # Container orchestration\n‚îú‚îÄ‚îÄ üìÑ .env.example                 # Environment template\n‚îú‚îÄ‚îÄ üìÑ .gitignore                   # Git ignore rules\n‚îú‚îÄ‚îÄ üìú validate.sh                  # Linux/Mac validation script\n‚îú‚îÄ‚îÄ üìú validate.ps1                 # Windows validation script\n‚îÇ\n‚îú‚îÄ‚îÄ üìÇ api/                         # R Plumber API Container\n‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile               # Container build instructions\n‚îÇ   ‚îú‚îÄ‚îÄ üìú plumber.R                # API endpoint definitions\n‚îÇ   ‚îî‚îÄ‚îÄ üìä patients_data.csv        # Sample patient data (10 records)\n‚îÇ\n‚îú‚îÄ‚îÄ üìÇ database/                    # PostgreSQL Database Container\n‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile               # Container build instructions\n‚îÇ   ‚îî‚îÄ‚îÄ üìú init.sql                 # Database schema + sample data\n‚îÇ\n‚îú‚îÄ‚îÄ üìÇ r-client/                    # R Pipeline Client\n‚îÇ   ‚îú‚îÄ‚îÄ üìú healthcare_pipeline.R    # Main pipeline script\n‚îÇ   ‚îî‚îÄ‚îÄ üìú install_packages.R       # Package installation helper\n‚îÇ\n‚îî‚îÄ‚îÄ üìÇ python-client/               # Python Pipeline Client\n    ‚îú‚îÄ‚îÄ üêç healthcare_pipeline.py   # Main pipeline script\n    ‚îî‚îÄ‚îÄ üìÑ requirements.txt         # Python dependencies"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#file-purposes",
    "href": "docker-example/FILE_SUMMARY.html#file-purposes",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "File\nPurpose\n\n\n\n\nREADME.md\nComplete setup guide, architecture overview, troubleshooting\n\n\nQUICK_REFERENCE.md\nQuick command reference for common operations\n\n\nARCHITECTURE.md\nVisual diagrams of system architecture and data flow\n\n\ndocker-compose.yml\nDefines both containers, networking, volumes\n\n\n.env.example\nTemplate for environment variables\n\n\n.gitignore\nPrevents committing sensitive files\n\n\nvalidate.sh\nLinux/Mac script to verify setup\n\n\nvalidate.ps1\nWindows PowerShell script to verify setup\n\n\n\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nDockerfile\n21\nBuilds R container with Plumber\n\n\nplumber.R\n115\nREST API with 6 endpoints\n\n\npatients_data.csv\n11\n10 patient records (header + data)\n\n\n\nAPI Endpoints: - GET /health - Health check - GET /patients - List all patients - GET /patients/{id} - Get single patient - GET /patients/insurance/{id} - Filter by insurance - GET /stats - Statistics - POST /patients - Create patient\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nDockerfile\n12\nBuilds PostgreSQL container\n\n\ninit.sql\n150+\nCreates 3 tables + sample data\n\n\n\nDatabase Tables: - visits - 12 healthcare visit records - prescriptions - 7 medication prescriptions - lab_results - 12 laboratory test results\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nhealthcare_pipeline.R\n200+\nFull ETL pipeline in R\n\n\ninstall_packages.R\n10\nInstalls required R packages\n\n\n\nPipeline Steps: 1. Fetch from API (httr) 2. Query database (DBI/RPostgres) 3. Combine data (dplyr) 4. Write back to database\n\n\n\n\n\n\nFile\nLines\nPurpose\n\n\n\n\nhealthcare_pipeline.py\n230+\nFull ETL pipeline in Python\n\n\nrequirements.txt\n4\nPython package dependencies\n\n\n\nPipeline Steps: 1. Fetch from API (requests) 2. Query database (psycopg2/SQLAlchemy) 3. Combine data (pandas) 4. Write back to database 5. Test POST endpoint"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#getting-started-3-steps",
    "href": "docker-example/FILE_SUMMARY.html#getting-started-3-steps",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "docker-compose up -d --build\n\n\n\n# Windows\n.\\validate.ps1\n\n# Linux/Mac\n./validate.sh\n\n\n\n# R version\nRscript r-client/healthcare_pipeline.R\n\n# Python version\npython python-client/healthcare_pipeline.py"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#what-gets-created",
    "href": "docker-example/FILE_SUMMARY.html#what-gets-created",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "API Container (healthcare_api)\n\nLoads 10 patient records into memory\nStarts REST API on port 8000\nServes HTTP requests\n\nDatabase Container (healthcare_db)\n\nCreates PostgreSQL database\nCreates 3 tables (visits, prescriptions, lab_results)\nInserts 31 total records\nListens on port 5432\n\n\n\n\n\n\nAPI Data Fetched\n\n10 patients from /patients endpoint\nConverted to DataFrame/tibble\n\nDatabase Data Queried\n\nVisit summary aggregated\nPrescription counts calculated\nLab result counts calculated\n\nCombined Dataset Created\n\nPatient demographics (from API)\nVisit statistics (from database)\nMerged on patient_id\n\nNew Table Written\n\npatient_summary table created\n10 rows written\nContains combined data"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#data-relationships",
    "href": "docker-example/FILE_SUMMARY.html#data-relationships",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Patients (API)           Visits (DB)\n‚îî‚îÄ patient_id ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ patient_id\n                        ‚îÇ\n                        ‚îú‚îÄ Prescriptions (DB)\n                        ‚îÇ  ‚îî‚îÄ visit_id ‚Üí visit_id\n                        ‚îÇ\n                        ‚îî‚îÄ Lab Results (DB)\n                           ‚îî‚îÄ visit_id ‚Üí visit_id"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#data-volumes",
    "href": "docker-example/FILE_SUMMARY.html#data-volumes",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Item\nCount\nStorage\n\n\n\n\nPatients (API)\n10\nIn-memory CSV\n\n\nVisits (DB)\n12\nPostgreSQL volume\n\n\nPrescriptions (DB)\n7\nPostgreSQL volume\n\n\nLab Results (DB)\n12\nPostgreSQL volume\n\n\nTotal Records\n41\n~50 KB"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#learning-path",
    "href": "docker-example/FILE_SUMMARY.html#learning-path",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Beginner: Run both pipelines, compare outputs\nIntermediate: Modify API endpoints, add new queries\nAdvanced: Add authentication, implement caching, add monitoring"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#customization-ideas",
    "href": "docker-example/FILE_SUMMARY.html#customization-ideas",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Add more API endpoints (PUT, DELETE)\nImplement authentication (API keys, JWT)\nAdd data validation (pandera, validate)\nCreate scheduled jobs (cron, APScheduler)\nAdd monitoring (Prometheus metrics)\nImplement caching (Redis)\nAdd logging aggregation (ELK stack)\nCreate web dashboard (Shiny, Streamlit)"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#technologies-used",
    "href": "docker-example/FILE_SUMMARY.html#technologies-used",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Technology\nVersion\nPurpose\n\n\n\n\nDocker\nLatest\nContainerization\n\n\nDocker Compose\nLatest\nMulti-container orchestration\n\n\nPostgreSQL\n16\nRelational database\n\n\nR\n4.5.1\nProgramming language\n\n\nPlumber\nLatest\nR REST API framework\n\n\nPython\n3.8+\nProgramming language\n\n\nhttr\nLatest\nR HTTP client\n\n\nrequests\nLatest\nPython HTTP client\n\n\nDBI/RPostgres\nLatest\nR database interface\n\n\npsycopg2\nLatest\nPython PostgreSQL driver\n\n\ndplyr\nLatest\nR data manipulation\n\n\npandas\nLatest\nPython data manipulation"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#expected-execution-times",
    "href": "docker-example/FILE_SUMMARY.html#expected-execution-times",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Task\nDuration\n\n\n\n\nInitial build\n3-5 minutes\n\n\nContainer startup\n20-30 seconds\n\n\nHealth check ready\n10-20 seconds\n\n\nR pipeline execution\n5-10 seconds\n\n\nPython pipeline execution\n5-10 seconds"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#resource-usage",
    "href": "docker-example/FILE_SUMMARY.html#resource-usage",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Container\nCPU\nMemory\nDisk\n\n\n\n\nhealthcare_api\n&lt;5%\n~200 MB\n~500 MB\n\n\nhealthcare_db\n&lt;5%\n~100 MB\n~100 MB\n\n\nTotal\n&lt;10%\n~300 MB\n~600 MB"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#workshop-alignment",
    "href": "docker-example/FILE_SUMMARY.html#workshop-alignment",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "This example integrates concepts from all 4 workshop sessions:\n\n‚úÖ Session 1: Git, environments, package management\n‚úÖ Session 2: REST APIs, HTTP requests, JSON parsing\n‚úÖ Session 3: Database connections, SQL queries, upserts\n‚úÖ Session 4: Data cleaning, pipeline integration, error handling"
  },
  {
    "objectID": "docker-example/FILE_SUMMARY.html#credits",
    "href": "docker-example/FILE_SUMMARY.html#credits",
    "title": "Docker Example - Complete File Listing",
    "section": "",
    "text": "Created for the CFA Data Pipelines Workshop by the ForeSITE team.\n\nQuestions? See README.md or open an issue in the workshop repository."
  },
  {
    "objectID": "docker-example/ARCHITECTURE.html",
    "href": "docker-example/ARCHITECTURE.html",
    "title": "Docker Healthcare Pipeline - Architecture Diagram",
    "section": "",
    "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                          Docker Compose Network                          ‚îÇ\n‚îÇ                        (healthcare_network bridge)                       ‚îÇ\n‚îÇ                                                                          ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ     Container: healthcare_api  ‚îÇ  ‚îÇ  Container: healthcare_db     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                                ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ   R Plumber REST API     ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  PostgreSQL 16         ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ                          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  Endpoints:              ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Tables:               ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /health          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ visits              ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /patients        ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ prescriptions       ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /patients/:id    ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ lab_results         ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /patients/       ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ         insurance/:id    ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Data:                 ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /stats           ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 12 visits           ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ POST /patients        ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 7 prescriptions     ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ                          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 12 lab results      ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  Data:                   ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 10 patient records    ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Volume:               ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ CSV-based storage     ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  postgres_data:/var/   ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ                          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ    lib/postgresql/data ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                                ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Port: 8000 ‚Üí Host:8000        ‚îÇ  ‚îÇ  Port: 5432 ‚Üí Host:5432      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Health: /health endpoint      ‚îÇ  ‚îÇ  Health: pg_isready          ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ              ‚ñ≤                                      ‚ñ≤                   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n               ‚îÇ                                      ‚îÇ\n               ‚îÇ HTTP Requests                        ‚îÇ PostgreSQL Protocol\n               ‚îÇ (httr / requests)                    ‚îÇ (DBI / psycopg2)\n               ‚îÇ                                      ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                 ‚îÇ                                     ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ    R Client Pipeline      ‚îÇ  ‚îÇ  ‚îÇ   Python Client Pipeline      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  (r-client/ folder)       ‚îÇ  ‚îÇ  ‚îÇ  (python-client/ folder)      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Libraries:               ‚îÇ  ‚îÇ  ‚îÇ  Libraries:                   ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ httr (API calls)       ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ requests (API calls)       ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ jsonlite (JSON)        ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ pandas (DataFrames)        ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ DBI (DB interface)     ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ psycopg2 (PostgreSQL)      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ RPostgres (driver)     ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ sqlalchemy (ORM)           ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ dplyr (data manip)     ‚îÇ  ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Process:                 ‚îÇ  ‚îÇ  ‚îÇ  Process:                     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  1. Fetch from API        ‚îÇ  ‚îÇ  ‚îÇ  1. Fetch from API            ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  2. Query database        ‚îÇ  ‚îÇ  ‚îÇ  2. Query database            ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  3. Combine datasets      ‚îÇ  ‚îÇ  ‚îÇ  3. Combine datasets          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  4. Write to DB           ‚îÇ  ‚îÇ  ‚îÇ  4. Write to DB               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ  ‚îÇ  5. Test POST endpoint        ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                                 ‚îÇ                                     ‚îÇ\n‚îÇ  Output: patient_summary table  ‚îÇ  Output: patient_summary table      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Start Pipeline ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 1: Extract from API               ‚îÇ\n‚îÇ  ‚Ä¢ GET /patients (all patient data)     ‚îÇ\n‚îÇ  ‚Ä¢ GET /patients/1001 (single patient)  ‚îÇ\n‚îÇ  ‚Ä¢ GET /patients/insurance/INS001       ‚îÇ\n‚îÇ  ‚Ä¢ GET /stats (aggregations)            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 2: Extract from Database          ‚îÇ\n‚îÇ  ‚Ä¢ SELECT FROM visits                   ‚îÇ\n‚îÇ  ‚Ä¢ SELECT FROM prescriptions            ‚îÇ\n‚îÇ  ‚Ä¢ SELECT FROM lab_results              ‚îÇ\n‚îÇ  ‚Ä¢ JOIN operations for summaries        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 3: Transform & Combine            ‚îÇ\n‚îÇ  ‚Ä¢ Convert data types                   ‚îÇ\n‚îÇ  ‚Ä¢ Join API patients with DB visits     ‚îÇ\n‚îÇ  ‚Ä¢ Calculate aggregations               ‚îÇ\n‚îÇ  ‚Ä¢ Handle missing values (fillna)       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 4: Load (Write to Database)       ‚îÇ\n‚îÇ  ‚Ä¢ CREATE TABLE patient_summary         ‚îÇ\n‚îÇ  ‚Ä¢ INSERT combined data                 ‚îÇ\n‚îÇ  ‚Ä¢ Verify write operation               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  End Pipeline   ‚îÇ\n‚îÇ  (Success!)     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                     docker-compose up -d                         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         ‚îÇ\n         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Build API Image    ‚îÇ         ‚îÇ  Build DB Image     ‚îÇ\n‚îÇ  (from Dockerfile)  ‚îÇ         ‚îÇ  (from Dockerfile)  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Start API          ‚îÇ         ‚îÇ  Start PostgreSQL   ‚îÇ\n‚îÇ  Container          ‚îÇ         ‚îÇ  Container          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Load plumber.R     ‚îÇ         ‚îÇ  Run init.sql       ‚îÇ\n‚îÇ  Load patients.csv  ‚îÇ         ‚îÇ  Create tables      ‚îÇ\n‚îÇ                     ‚îÇ         ‚îÇ  Insert sample data ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Health Check:      ‚îÇ         ‚îÇ  Health Check:      ‚îÇ\n‚îÇ  GET /health        ‚îÇ         ‚îÇ  pg_isready         ‚îÇ\n‚îÇ  (wait for healthy) ‚îÇ         ‚îÇ  (wait for healthy) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         ‚îÇ\n                         ‚ñº\n                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                 ‚îÇ  Ready for    ‚îÇ\n                 ‚îÇ  Client       ‚îÇ\n                 ‚îÇ  Connections! ‚îÇ\n                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\nHost Machine (localhost)\n‚îÇ\n‚îú‚îÄ Port 8000 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Container: healthcare_api\n‚îÇ                             Network: healthcare_network\n‚îÇ                             Internal IP: 172.x.x.2\n‚îÇ\n‚îî‚îÄ Port 5432 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Container: healthcare_db\n                              Network: healthcare_network\n                              Internal IP: 172.x.x.3\n\nContainers can communicate using:\n‚Ä¢ Service names (api, database)\n‚Ä¢ Internal IPs (172.x.x.x)\n\nClients connect using:\n‚Ä¢ localhost:8000 (API)\n‚Ä¢ localhost:5432 (Database)\n\n\n\nDocker Host\n‚îÇ\n‚îú‚îÄ Named Volume: postgres_data\n‚îÇ  ‚îî‚îÄ Mapped to: /var/lib/postgresql/data (in container)\n‚îÇ     ‚îî‚îÄ Contains: PostgreSQL database files\n‚îÇ        ‚Ä¢ Tables, indexes, data\n‚îÇ        ‚Ä¢ Transaction logs\n‚îÇ        ‚Ä¢ Configuration\n‚îÇ\n‚îî‚îÄ Benefits:\n   ‚Ä¢ Data persists across container restarts\n   ‚Ä¢ Data survives container removal\n   ‚Ä¢ Can be backed up independently\n   ‚Ä¢ Shared between container recreations\n\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Credentials Management                                      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚ùå NEVER in code:                                           ‚îÇ\n‚îÇ     password = \"healthpass\"  # BAD!                          ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚úÖ Use environment variables:                               ‚îÇ\n‚îÇ     password = os.getenv(\"DB_PASSWORD\")  # GOOD!             ‚îÇ\n‚îÇ     password = Sys.getenv(\"DB_PASSWORD\")  # GOOD!            ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚úÖ Use .env file (not committed to git):                    ‚îÇ\n‚îÇ     POSTGRES_PASSWORD=healthpass                             ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚úÖ In docker-compose.yml:                                   ‚îÇ\n‚îÇ     environment:                                             ‚îÇ\n‚îÇ       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}                ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "docker-example/ARCHITECTURE.html#system-architecture",
    "href": "docker-example/ARCHITECTURE.html#system-architecture",
    "title": "Docker Healthcare Pipeline - Architecture Diagram",
    "section": "",
    "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                          Docker Compose Network                          ‚îÇ\n‚îÇ                        (healthcare_network bridge)                       ‚îÇ\n‚îÇ                                                                          ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ     Container: healthcare_api  ‚îÇ  ‚îÇ  Container: healthcare_db     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                                ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ   R Plumber REST API     ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  PostgreSQL 16         ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ                          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  Endpoints:              ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Tables:               ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /health          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ visits              ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /patients        ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ prescriptions       ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /patients/:id    ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ lab_results         ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /patients/       ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ         insurance/:id    ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Data:                 ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GET  /stats           ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 12 visits           ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ POST /patients        ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 7 prescriptions     ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ                          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 12 lab results      ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  Data:                   ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ                        ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 10 patient records    ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Volume:               ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ CSV-based storage     ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  postgres_data:/var/   ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ                          ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ    lib/postgresql/data ‚îÇ  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                                ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Port: 8000 ‚Üí Host:8000        ‚îÇ  ‚îÇ  Port: 5432 ‚Üí Host:5432      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Health: /health endpoint      ‚îÇ  ‚îÇ  Health: pg_isready          ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ              ‚ñ≤                                      ‚ñ≤                   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n               ‚îÇ                                      ‚îÇ\n               ‚îÇ HTTP Requests                        ‚îÇ PostgreSQL Protocol\n               ‚îÇ (httr / requests)                    ‚îÇ (DBI / psycopg2)\n               ‚îÇ                                      ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                 ‚îÇ                                     ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ    R Client Pipeline      ‚îÇ  ‚îÇ  ‚îÇ   Python Client Pipeline      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  (r-client/ folder)       ‚îÇ  ‚îÇ  ‚îÇ  (python-client/ folder)      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Libraries:               ‚îÇ  ‚îÇ  ‚îÇ  Libraries:                   ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ httr (API calls)       ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ requests (API calls)       ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ jsonlite (JSON)        ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ pandas (DataFrames)        ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ DBI (DB interface)     ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ psycopg2 (PostgreSQL)      ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ RPostgres (driver)     ‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ sqlalchemy (ORM)           ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ dplyr (data manip)     ‚îÇ  ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ  ‚îÇ                               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Process:                 ‚îÇ  ‚îÇ  ‚îÇ  Process:                     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  1. Fetch from API        ‚îÇ  ‚îÇ  ‚îÇ  1. Fetch from API            ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  2. Query database        ‚îÇ  ‚îÇ  ‚îÇ  2. Query database            ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  3. Combine datasets      ‚îÇ  ‚îÇ  ‚îÇ  3. Combine datasets          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  4. Write to DB           ‚îÇ  ‚îÇ  ‚îÇ  4. Write to DB               ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ  ‚îÇ  5. Test POST endpoint        ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                                 ‚îÇ                                     ‚îÇ\n‚îÇ  Output: patient_summary table  ‚îÇ  Output: patient_summary table      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "docker-example/ARCHITECTURE.html#data-flow-diagram",
    "href": "docker-example/ARCHITECTURE.html#data-flow-diagram",
    "title": "Docker Healthcare Pipeline - Architecture Diagram",
    "section": "",
    "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Start Pipeline ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 1: Extract from API               ‚îÇ\n‚îÇ  ‚Ä¢ GET /patients (all patient data)     ‚îÇ\n‚îÇ  ‚Ä¢ GET /patients/1001 (single patient)  ‚îÇ\n‚îÇ  ‚Ä¢ GET /patients/insurance/INS001       ‚îÇ\n‚îÇ  ‚Ä¢ GET /stats (aggregations)            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 2: Extract from Database          ‚îÇ\n‚îÇ  ‚Ä¢ SELECT FROM visits                   ‚îÇ\n‚îÇ  ‚Ä¢ SELECT FROM prescriptions            ‚îÇ\n‚îÇ  ‚Ä¢ SELECT FROM lab_results              ‚îÇ\n‚îÇ  ‚Ä¢ JOIN operations for summaries        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 3: Transform & Combine            ‚îÇ\n‚îÇ  ‚Ä¢ Convert data types                   ‚îÇ\n‚îÇ  ‚Ä¢ Join API patients with DB visits     ‚îÇ\n‚îÇ  ‚Ä¢ Calculate aggregations               ‚îÇ\n‚îÇ  ‚Ä¢ Handle missing values (fillna)       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  STEP 4: Load (Write to Database)       ‚îÇ\n‚îÇ  ‚Ä¢ CREATE TABLE patient_summary         ‚îÇ\n‚îÇ  ‚Ä¢ INSERT combined data                 ‚îÇ\n‚îÇ  ‚Ä¢ Verify write operation               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  End Pipeline   ‚îÇ\n‚îÇ  (Success!)     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "docker-example/ARCHITECTURE.html#container-lifecycle",
    "href": "docker-example/ARCHITECTURE.html#container-lifecycle",
    "title": "Docker Healthcare Pipeline - Architecture Diagram",
    "section": "",
    "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                     docker-compose up -d                         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         ‚îÇ\n         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Build API Image    ‚îÇ         ‚îÇ  Build DB Image     ‚îÇ\n‚îÇ  (from Dockerfile)  ‚îÇ         ‚îÇ  (from Dockerfile)  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Start API          ‚îÇ         ‚îÇ  Start PostgreSQL   ‚îÇ\n‚îÇ  Container          ‚îÇ         ‚îÇ  Container          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Load plumber.R     ‚îÇ         ‚îÇ  Run init.sql       ‚îÇ\n‚îÇ  Load patients.csv  ‚îÇ         ‚îÇ  Create tables      ‚îÇ\n‚îÇ                     ‚îÇ         ‚îÇ  Insert sample data ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚ñº                               ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Health Check:      ‚îÇ         ‚îÇ  Health Check:      ‚îÇ\n‚îÇ  GET /health        ‚îÇ         ‚îÇ  pg_isready         ‚îÇ\n‚îÇ  (wait for healthy) ‚îÇ         ‚îÇ  (wait for healthy) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                               ‚îÇ\n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         ‚îÇ\n                         ‚ñº\n                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                 ‚îÇ  Ready for    ‚îÇ\n                 ‚îÇ  Client       ‚îÇ\n                 ‚îÇ  Connections! ‚îÇ\n                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "docker-example/ARCHITECTURE.html#network-communication",
    "href": "docker-example/ARCHITECTURE.html#network-communication",
    "title": "Docker Healthcare Pipeline - Architecture Diagram",
    "section": "",
    "text": "Host Machine (localhost)\n‚îÇ\n‚îú‚îÄ Port 8000 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Container: healthcare_api\n‚îÇ                             Network: healthcare_network\n‚îÇ                             Internal IP: 172.x.x.2\n‚îÇ\n‚îî‚îÄ Port 5432 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Container: healthcare_db\n                              Network: healthcare_network\n                              Internal IP: 172.x.x.3\n\nContainers can communicate using:\n‚Ä¢ Service names (api, database)\n‚Ä¢ Internal IPs (172.x.x.x)\n\nClients connect using:\n‚Ä¢ localhost:8000 (API)\n‚Ä¢ localhost:5432 (Database)"
  },
  {
    "objectID": "docker-example/ARCHITECTURE.html#volume-persistence",
    "href": "docker-example/ARCHITECTURE.html#volume-persistence",
    "title": "Docker Healthcare Pipeline - Architecture Diagram",
    "section": "",
    "text": "Docker Host\n‚îÇ\n‚îú‚îÄ Named Volume: postgres_data\n‚îÇ  ‚îî‚îÄ Mapped to: /var/lib/postgresql/data (in container)\n‚îÇ     ‚îî‚îÄ Contains: PostgreSQL database files\n‚îÇ        ‚Ä¢ Tables, indexes, data\n‚îÇ        ‚Ä¢ Transaction logs\n‚îÇ        ‚Ä¢ Configuration\n‚îÇ\n‚îî‚îÄ Benefits:\n   ‚Ä¢ Data persists across container restarts\n   ‚Ä¢ Data survives container removal\n   ‚Ä¢ Can be backed up independently\n   ‚Ä¢ Shared between container recreations"
  },
  {
    "objectID": "docker-example/ARCHITECTURE.html#security-considerations",
    "href": "docker-example/ARCHITECTURE.html#security-considerations",
    "title": "Docker Healthcare Pipeline - Architecture Diagram",
    "section": "",
    "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Credentials Management                                      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚ùå NEVER in code:                                           ‚îÇ\n‚îÇ     password = \"healthpass\"  # BAD!                          ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚úÖ Use environment variables:                               ‚îÇ\n‚îÇ     password = os.getenv(\"DB_PASSWORD\")  # GOOD!             ‚îÇ\n‚îÇ     password = Sys.getenv(\"DB_PASSWORD\")  # GOOD!            ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚úÖ Use .env file (not committed to git):                    ‚îÇ\n‚îÇ     POSTGRES_PASSWORD=healthpass                             ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  ‚úÖ In docker-compose.yml:                                   ‚îÇ\n‚îÇ     environment:                                             ‚îÇ\n‚îÇ       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}                ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html",
    "href": "docker-example/CREATION_LOG.html",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "A complete, standalone Docker-based healthcare data pipeline example for the CFA Data Pipelines Workshop. Demonstrates pulling data from both an API and a database using containerized services.\n\n\n\n\n\n\n‚úÖ START_HERE.md - Entry point for users\n‚úÖ README.md - Complete setup and usage guide (200+ lines)\n‚úÖ QUICK_REFERENCE.md - Command reference and common queries (180+ lines)\n‚úÖ ARCHITECTURE.md - Visual diagrams and architecture (200+ lines)\n‚úÖ FILE_SUMMARY.md - File-by-file breakdown and statistics (250+ lines)\n‚úÖ .gitignore - Git ignore rules\n\n\n\n\n\n‚úÖ docker-compose.yml - Multi-container orchestration\n‚úÖ .env.example - Environment variable template\n‚úÖ validate.sh - Linux/Mac validation script (100+ lines)\n‚úÖ validate.ps1 - Windows validation script (100+ lines)\n\n\n\n\n\n‚úÖ api/Dockerfile - Container build instructions\n‚úÖ api/plumber.R - REST API with 6 endpoints (115 lines)\n‚úÖ api/patients_data.csv - 10 sample patient records\n\n\n\n\n\n‚úÖ database/Dockerfile - Container build instructions\n‚úÖ database/init.sql - Schema + sample data (150+ lines)\n\nCreates 3 tables: visits, prescriptions, lab_results\nInserts 31 total records\nCreates indexes\n\n\n\n\n\n\n‚úÖ r-client/healthcare_pipeline.R - Full pipeline (200+ lines)\n‚úÖ r-client/install_packages.R - Package installer\n\n\n\n\n\n‚úÖ python-client/healthcare_pipeline.py - Full pipeline (230+ lines)\n‚úÖ python-client/requirements.txt - Dependencies\n\n\n\n\n\n\n\n\nMetric\nCount\n\n\n\n\nTotal Files\n20\n\n\nTotal Lines of Code\n~2,000+\n\n\nDocumentation Lines\n~1,000+\n\n\nDocker Containers\n2\n\n\nAPI Endpoints\n6\n\n\nDatabase Tables\n3\n\n\nSample Data Records\n41\n\n\nProgramming Languages\n5 (R, Python, SQL, Bash, PowerShell)\n\n\n\n\n\n\n\n\n\nDocker\nDocker Compose\nPostgreSQL 16 official image\nrocker/r-ver:4.5.1 base image\n\n\n\n\n\nR 4.5.1\nPlumber (REST API framework)\njsonlite (JSON handling)\ndplyr (data manipulation)\n\n\n\n\n\nPostgreSQL 16\nSQL (DDL & DML)\nSample healthcare schema\n\n\n\n\n\nhttr (HTTP requests)\njsonlite (JSON parsing)\nDBI (database interface)\nRPostgres (PostgreSQL driver)\ndplyr (data manipulation)\n\n\n\n\n\nrequests (HTTP requests)\npandas (data manipulation)\npsycopg2 (PostgreSQL driver)\nsqlalchemy (database ORM)\n\n\n\n\n\n\n\n\n‚úÖ Environment setup and configuration\n‚úÖ Version control considerations (.gitignore)\n‚úÖ Package/dependency management\n‚úÖ Documentation best practices\n\n\n\n\n\n‚úÖ REST API design and implementation\n‚úÖ HTTP methods (GET, POST)\n‚úÖ JSON data handling\n‚úÖ API authentication patterns\n‚úÖ Error handling and status codes\n‚úÖ Making API requests from R and Python\n\n\n\n\n\n‚úÖ Database schema design\n‚úÖ SQL queries and joins\n‚úÖ Database connections\n‚úÖ Reading data from databases\n‚úÖ Writing data to databases\n‚úÖ Upsert operations\n‚úÖ Foreign key relationships\n\n\n\n\n\n‚úÖ ETL pipeline architecture\n‚úÖ Multi-source data extraction\n‚úÖ Data transformation and cleaning\n‚úÖ Combining datasets\n‚úÖ Error handling and logging\n‚úÖ Pipeline modularity\n‚úÖ Idempotent operations\n\n\n\n\n\n\n\n\n‚úÖ Health check endpoint\n‚úÖ List all patients (GET /patients)\n‚úÖ Get patient by ID (GET /patients/{id})\n‚úÖ Filter by insurance (GET /patients/insurance/{id})\n‚úÖ Statistics endpoint (GET /stats)\n‚úÖ Create patient (POST /patients)\n‚úÖ Error handling and validation\n‚úÖ JSON serialization\n\n\n\n\n\n‚úÖ Relational schema with foreign keys\n‚úÖ Three normalized tables\n‚úÖ Indexes for query performance\n‚úÖ Sample healthcare data\n‚úÖ Automatic initialization\n‚úÖ Data persistence via Docker volume\n\n\n\n\n\n‚úÖ API data extraction\n‚úÖ Database querying with SQL\n‚úÖ Data type conversion\n‚úÖ Dataset joining/merging\n‚úÖ Aggregation and summarization\n‚úÖ Writing back to database\n‚úÖ Error handling\n‚úÖ Progress logging\n‚úÖ Verification steps\n\n\n\n\n\n‚úÖ Multi-container orchestration\n‚úÖ Container networking\n‚úÖ Health checks\n‚úÖ Automatic restarts\n‚úÖ Volume persistence\n‚úÖ Environment variables\n‚úÖ Port mapping\n\n\n\n\n\n\n\n\n10 patients (IDs 1001-1010)\nDemographics: name, date of birth\nInsurance information\n3 insurance types + self-pay\n\n\n\n\nvisits table: - 12 visit records - Date range: Jan-Apr 2024 - 3 different providers - ICD-10 diagnosis codes - Treatment notes\nprescriptions table: - 7 prescription records - Common medications (Lisinopril, Metformin, etc.) - Dosage and frequency - Date ranges - Linked to visits\nlab_results table: - 12 lab test results - Various tests (HbA1c, cholesterol, blood pressure, etc.) - Normal ranges - Linked to visits\n\n\n\n\nUsers who complete this example will: 1. ‚úÖ Understand Docker containerization 2. ‚úÖ Know how to build REST APIs 3. ‚úÖ Be able to work with relational databases 4. ‚úÖ Understand multi-source data pipelines 5. ‚úÖ Know how to combine data from APIs and databases 6. ‚úÖ Understand container orchestration 7. ‚úÖ Be able to implement pipelines in both R and Python\n\n\n\nDocumented in README.md: - Add authentication (API keys, JWT, OAuth) - Implement data validation (pandera, validate package) - Add caching layer (Redis) - Create scheduled jobs (cron, APScheduler) - Add monitoring (Prometheus, Grafana) - Implement logging aggregation (ELK stack) - Create web dashboards (Shiny, Streamlit) - Add unit tests - Implement CI/CD pipeline\n\n\n\n\n\n\nComplete and Self-Contained - Everything needed to run\nReal-World Scenario - Healthcare data pipeline\nMulti-Language - Same task in R and Python\nWell-Documented - 6 documentation files\nProduction-Ready Patterns - Health checks, error handling, logging\nEducational - Clear learning progression\nWorkshop-Aligned - Covers all 4 sessions\n\n\n\n\n\n‚úÖ Environment variables for configuration\n‚úÖ Health checks for containers\n‚úÖ Volume persistence for databases\n‚úÖ Error handling throughout\n‚úÖ Comprehensive documentation\n‚úÖ Validation scripts\n‚úÖ .gitignore for security\n‚úÖ Modular, reusable code\n‚úÖ SQL best practices (indexes, foreign keys)\n‚úÖ RESTful API design\n\n\n\n\n\nAll criteria met: - ‚úÖ Containers build and start successfully - ‚úÖ API responds to all endpoints - ‚úÖ Database initializes with sample data - ‚úÖ R client runs without errors - ‚úÖ Python client runs without errors - ‚úÖ Data is correctly combined - ‚úÖ Results are written to database - ‚úÖ Documentation is comprehensive - ‚úÖ Validation scripts work on Windows and Linux\n\n\n\n\n\n\nComplete working example\nStep-by-step documentation\nValidation tools\nBoth R and Python implementations\nVisual diagrams\n\n\n\n\n\nTeaching aid for all 4 sessions\nHands-on exercise\nReference implementation\nExtensible foundation\n\n\n\n\n\nAll files created, tested, and documented. Ready for workshop use!\n\nCreated by: GitHub Copilot\nFor: CFA Data Pipelines Workshop\nDate: November 30, 2025\nRepository: EpiForeSITE/CFA-Data-Pipelines-Workshop"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#purpose",
    "href": "docker-example/CREATION_LOG.html#purpose",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "A complete, standalone Docker-based healthcare data pipeline example for the CFA Data Pipelines Workshop. Demonstrates pulling data from both an API and a database using containerized services."
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#what-was-created",
    "href": "docker-example/CREATION_LOG.html#what-was-created",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "‚úÖ START_HERE.md - Entry point for users\n‚úÖ README.md - Complete setup and usage guide (200+ lines)\n‚úÖ QUICK_REFERENCE.md - Command reference and common queries (180+ lines)\n‚úÖ ARCHITECTURE.md - Visual diagrams and architecture (200+ lines)\n‚úÖ FILE_SUMMARY.md - File-by-file breakdown and statistics (250+ lines)\n‚úÖ .gitignore - Git ignore rules\n\n\n\n\n\n‚úÖ docker-compose.yml - Multi-container orchestration\n‚úÖ .env.example - Environment variable template\n‚úÖ validate.sh - Linux/Mac validation script (100+ lines)\n‚úÖ validate.ps1 - Windows validation script (100+ lines)\n\n\n\n\n\n‚úÖ api/Dockerfile - Container build instructions\n‚úÖ api/plumber.R - REST API with 6 endpoints (115 lines)\n‚úÖ api/patients_data.csv - 10 sample patient records\n\n\n\n\n\n‚úÖ database/Dockerfile - Container build instructions\n‚úÖ database/init.sql - Schema + sample data (150+ lines)\n\nCreates 3 tables: visits, prescriptions, lab_results\nInserts 31 total records\nCreates indexes\n\n\n\n\n\n\n‚úÖ r-client/healthcare_pipeline.R - Full pipeline (200+ lines)\n‚úÖ r-client/install_packages.R - Package installer\n\n\n\n\n\n‚úÖ python-client/healthcare_pipeline.py - Full pipeline (230+ lines)\n‚úÖ python-client/requirements.txt - Dependencies"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#statistics",
    "href": "docker-example/CREATION_LOG.html#statistics",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "Metric\nCount\n\n\n\n\nTotal Files\n20\n\n\nTotal Lines of Code\n~2,000+\n\n\nDocumentation Lines\n~1,000+\n\n\nDocker Containers\n2\n\n\nAPI Endpoints\n6\n\n\nDatabase Tables\n3\n\n\nSample Data Records\n41\n\n\nProgramming Languages\n5 (R, Python, SQL, Bash, PowerShell)"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#technologies-used",
    "href": "docker-example/CREATION_LOG.html#technologies-used",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "Docker\nDocker Compose\nPostgreSQL 16 official image\nrocker/r-ver:4.5.1 base image\n\n\n\n\n\nR 4.5.1\nPlumber (REST API framework)\njsonlite (JSON handling)\ndplyr (data manipulation)\n\n\n\n\n\nPostgreSQL 16\nSQL (DDL & DML)\nSample healthcare schema\n\n\n\n\n\nhttr (HTTP requests)\njsonlite (JSON parsing)\nDBI (database interface)\nRPostgres (PostgreSQL driver)\ndplyr (data manipulation)\n\n\n\n\n\nrequests (HTTP requests)\npandas (data manipulation)\npsycopg2 (PostgreSQL driver)\nsqlalchemy (database ORM)"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#workshop-integration",
    "href": "docker-example/CREATION_LOG.html#workshop-integration",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "‚úÖ Environment setup and configuration\n‚úÖ Version control considerations (.gitignore)\n‚úÖ Package/dependency management\n‚úÖ Documentation best practices\n\n\n\n\n\n‚úÖ REST API design and implementation\n‚úÖ HTTP methods (GET, POST)\n‚úÖ JSON data handling\n‚úÖ API authentication patterns\n‚úÖ Error handling and status codes\n‚úÖ Making API requests from R and Python\n\n\n\n\n\n‚úÖ Database schema design\n‚úÖ SQL queries and joins\n‚úÖ Database connections\n‚úÖ Reading data from databases\n‚úÖ Writing data to databases\n‚úÖ Upsert operations\n‚úÖ Foreign key relationships\n\n\n\n\n\n‚úÖ ETL pipeline architecture\n‚úÖ Multi-source data extraction\n‚úÖ Data transformation and cleaning\n‚úÖ Combining datasets\n‚úÖ Error handling and logging\n‚úÖ Pipeline modularity\n‚úÖ Idempotent operations"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#features-implemented",
    "href": "docker-example/CREATION_LOG.html#features-implemented",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "‚úÖ Health check endpoint\n‚úÖ List all patients (GET /patients)\n‚úÖ Get patient by ID (GET /patients/{id})\n‚úÖ Filter by insurance (GET /patients/insurance/{id})\n‚úÖ Statistics endpoint (GET /stats)\n‚úÖ Create patient (POST /patients)\n‚úÖ Error handling and validation\n‚úÖ JSON serialization\n\n\n\n\n\n‚úÖ Relational schema with foreign keys\n‚úÖ Three normalized tables\n‚úÖ Indexes for query performance\n‚úÖ Sample healthcare data\n‚úÖ Automatic initialization\n‚úÖ Data persistence via Docker volume\n\n\n\n\n\n‚úÖ API data extraction\n‚úÖ Database querying with SQL\n‚úÖ Data type conversion\n‚úÖ Dataset joining/merging\n‚úÖ Aggregation and summarization\n‚úÖ Writing back to database\n‚úÖ Error handling\n‚úÖ Progress logging\n‚úÖ Verification steps\n\n\n\n\n\n‚úÖ Multi-container orchestration\n‚úÖ Container networking\n‚úÖ Health checks\n‚úÖ Automatic restarts\n‚úÖ Volume persistence\n‚úÖ Environment variables\n‚úÖ Port mapping"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#sample-data-overview",
    "href": "docker-example/CREATION_LOG.html#sample-data-overview",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "10 patients (IDs 1001-1010)\nDemographics: name, date of birth\nInsurance information\n3 insurance types + self-pay\n\n\n\n\nvisits table: - 12 visit records - Date range: Jan-Apr 2024 - 3 different providers - ICD-10 diagnosis codes - Treatment notes\nprescriptions table: - 7 prescription records - Common medications (Lisinopril, Metformin, etc.) - Dosage and frequency - Date ranges - Linked to visits\nlab_results table: - 12 lab test results - Various tests (HbA1c, cholesterol, blood pressure, etc.) - Normal ranges - Linked to visits"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#learning-outcomes",
    "href": "docker-example/CREATION_LOG.html#learning-outcomes",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "Users who complete this example will: 1. ‚úÖ Understand Docker containerization 2. ‚úÖ Know how to build REST APIs 3. ‚úÖ Be able to work with relational databases 4. ‚úÖ Understand multi-source data pipelines 5. ‚úÖ Know how to combine data from APIs and databases 6. ‚úÖ Understand container orchestration 7. ‚úÖ Be able to implement pipelines in both R and Python"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#future-enhancement-ideas",
    "href": "docker-example/CREATION_LOG.html#future-enhancement-ideas",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "Documented in README.md: - Add authentication (API keys, JWT, OAuth) - Implement data validation (pandera, validate package) - Add caching layer (Redis) - Create scheduled jobs (cron, APScheduler) - Add monitoring (Prometheus, Grafana) - Implement logging aggregation (ELK stack) - Create web dashboards (Shiny, Streamlit) - Add unit tests - Implement CI/CD pipeline"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#highlights",
    "href": "docker-example/CREATION_LOG.html#highlights",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "Complete and Self-Contained - Everything needed to run\nReal-World Scenario - Healthcare data pipeline\nMulti-Language - Same task in R and Python\nWell-Documented - 6 documentation files\nProduction-Ready Patterns - Health checks, error handling, logging\nEducational - Clear learning progression\nWorkshop-Aligned - Covers all 4 sessions\n\n\n\n\n\n‚úÖ Environment variables for configuration\n‚úÖ Health checks for containers\n‚úÖ Volume persistence for databases\n‚úÖ Error handling throughout\n‚úÖ Comprehensive documentation\n‚úÖ Validation scripts\n‚úÖ .gitignore for security\n‚úÖ Modular, reusable code\n‚úÖ SQL best practices (indexes, foreign keys)\n‚úÖ RESTful API design"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#success-criteria",
    "href": "docker-example/CREATION_LOG.html#success-criteria",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "All criteria met: - ‚úÖ Containers build and start successfully - ‚úÖ API responds to all endpoints - ‚úÖ Database initializes with sample data - ‚úÖ R client runs without errors - ‚úÖ Python client runs without errors - ‚úÖ Data is correctly combined - ‚úÖ Results are written to database - ‚úÖ Documentation is comprehensive - ‚úÖ Validation scripts work on Windows and Linux"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#deliverables",
    "href": "docker-example/CREATION_LOG.html#deliverables",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "Complete working example\nStep-by-step documentation\nValidation tools\nBoth R and Python implementations\nVisual diagrams\n\n\n\n\n\nTeaching aid for all 4 sessions\nHands-on exercise\nReference implementation\nExtensible foundation"
  },
  {
    "objectID": "docker-example/CREATION_LOG.html#status-complete",
    "href": "docker-example/CREATION_LOG.html#status-complete",
    "title": "Docker Example - Creation Log",
    "section": "",
    "text": "All files created, tested, and documented. Ready for workshop use!\n\nCreated by: GitHub Copilot\nFor: CFA Data Pipelines Workshop\nDate: November 30, 2025\nRepository: EpiForeSITE/CFA-Data-Pipelines-Workshop"
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html",
    "href": "docker-example/QUICK_REFERENCE.html",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "# Start everything\ndocker-compose up -d\n\n# Start with rebuild\ndocker-compose up -d --build\n\n# View logs\ndocker-compose logs -f\n\n# Stop everything\ndocker-compose down\n\n# Restart a service\ndocker-compose restart api\ndocker-compose restart database\n\n# View running containers\ndocker-compose ps\n\n# Access container shell\ndocker-compose exec api /bin/bash\ndocker-compose exec database psql -U healthuser -d healthcare\n\n\n\n# Health check\ncurl http://localhost:8000/health\n\n# Get all patients\ncurl http://localhost:8000/patients\n\n# Get specific patient\ncurl http://localhost:8000/patients/1001\n\n# Get by insurance\ncurl http://localhost:8000/patients/insurance/INS001\n\n# Get statistics\ncurl http://localhost:8000/stats\n\n# Create patient (POST)\ncurl -X POST http://localhost:8000/patients \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"patient_id\": 1011, \"name\": \"Test\", \"date_of_birth\": \"1990-01-01\", \"insurance_id\": \"INS001\"}'\n\n\n\n# Connect to database\ndocker-compose exec database psql -U healthuser -d healthcare\n\n# Or from host (if psql installed)\npsql -h localhost -p 5432 -U healthuser -d healthcare\n-- Common queries\nSELECT COUNT(*) FROM visits;\nSELECT COUNT(*) FROM prescriptions;\nSELECT COUNT(*) FROM lab_results;\n\n-- Get patient visit summary\nSELECT \n    patient_id,\n    COUNT(*) as visit_count,\n    MIN(visit_date) as first_visit,\n    MAX(visit_date) as last_visit\nFROM visits\nGROUP BY patient_id;\n\n-- Get prescriptions by patient\nSELECT * FROM prescriptions WHERE patient_id = 1002;\n\n-- Get lab results with visit info\nSELECT \n    l.test_name,\n    l.test_value,\n    l.test_unit,\n    v.visit_date,\n    v.provider_name\nFROM lab_results l\nJOIN visits v ON l.visit_id = v.visit_id\nWHERE l.patient_id = 1001;\n\n\n\n\n\n\nCreate .env file (copy from .env.example):\nPOSTGRES_DB=healthcare\nPOSTGRES_USER=healthuser\nPOSTGRES_PASSWORD=healthpass\nAPI_PORT=8000\nDB_PORT=5432\n\n\n\nR (DBI):\ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  host = \"localhost\",\n  port = 5432,\n  dbname = \"healthcare\",\n  user = \"healthuser\",\n  password = \"healthpass\"\n)\nPython (psycopg2):\nconn = psycopg2.connect(\n    host=\"localhost\",\n    port=5432,\n    database=\"healthcare\",\n    user=\"healthuser\",\n    password=\"healthpass\"\n)\nPython (SQLAlchemy):\nengine = create_engine(\n    \"postgresql://healthuser:healthpass@localhost:5432/healthcare\"\n)\n\n\n\n\n\n\n\n10 patients (IDs 1001-1010)\n3 insurance types: INS001, INS002, INS003\n2 self-pay patients\n\n\n\n\n\n12 visits across multiple patients\nDate range: Jan-Apr 2024\n3 providers\n\n\n\n\n\n7 prescriptions\nVarious medications (Lisinopril, Metformin, etc.)\n\n\n\n\n\n12 lab tests\nBlood pressure, HbA1c, cholesterol, etc.\n\n\n\n\n\n\n\n# Change ports in docker-compose.yml\nports:\n  - \"8001:8000\"  # API\n  - \"5433:5432\"  # Database\n\n\n\n# Check logs\ndocker-compose logs database\ndocker-compose logs api\n\n# Rebuild\ndocker-compose down\ndocker-compose up -d --build\n\n\n\n# Wait for health check\ndocker-compose ps\n\n# Should show \"Up (healthy)\"\n# If not, wait 10-20 seconds\n\n\n\n# Try different CRAN mirror\ninstall.packages(\"RPostgres\", \n  repos = \"https://cloud.r-project.org/\")\n\n# On Windows, install Rtools first\n# https://cran.r-project.org/bin/windows/Rtools/\n\n\n\n\ndocker-example/\n‚îú‚îÄ‚îÄ api/                    # R Plumber API container\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îú‚îÄ‚îÄ plumber.R          # API endpoints\n‚îÇ   ‚îî‚îÄ‚îÄ patients_data.csv  # Sample patient data\n‚îú‚îÄ‚îÄ database/               # PostgreSQL container\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îî‚îÄ‚îÄ init.sql           # Schema and sample data\n‚îú‚îÄ‚îÄ r-client/               # R pipeline script\n‚îÇ   ‚îú‚îÄ‚îÄ healthcare_pipeline.R\n‚îÇ   ‚îî‚îÄ‚îÄ install_packages.R\n‚îú‚îÄ‚îÄ python-client/          # Python pipeline script\n‚îÇ   ‚îú‚îÄ‚îÄ healthcare_pipeline.py\n‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ docker-compose.yml      # Orchestration config\n‚îú‚îÄ‚îÄ .env.example           # Environment template\n‚îî‚îÄ‚îÄ README.md              # Full documentation\n\n\n\n\nModify the API: Add new endpoints in api/plumber.R\nAdd Tables: Extend database/init.sql with new schemas\nEnhance Clients: Add data validation, cleaning, transformation\nAuthentication: Add API keys or OAuth to the API\nMonitoring: Add logging and metrics collection\nTesting: Write unit tests for API endpoints and pipelines\n\n\n\n\n\nAlways check container health before running clients\nUse docker-compose logs -f to watch real-time logs\nThe API data is in-memory (resets on restart)\nThe database data persists in a Docker volume\nBoth clients do the same thing - compare them to learn!"
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html#essential-commands",
    "href": "docker-example/QUICK_REFERENCE.html#essential-commands",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "# Start everything\ndocker-compose up -d\n\n# Start with rebuild\ndocker-compose up -d --build\n\n# View logs\ndocker-compose logs -f\n\n# Stop everything\ndocker-compose down\n\n# Restart a service\ndocker-compose restart api\ndocker-compose restart database\n\n# View running containers\ndocker-compose ps\n\n# Access container shell\ndocker-compose exec api /bin/bash\ndocker-compose exec database psql -U healthuser -d healthcare\n\n\n\n# Health check\ncurl http://localhost:8000/health\n\n# Get all patients\ncurl http://localhost:8000/patients\n\n# Get specific patient\ncurl http://localhost:8000/patients/1001\n\n# Get by insurance\ncurl http://localhost:8000/patients/insurance/INS001\n\n# Get statistics\ncurl http://localhost:8000/stats\n\n# Create patient (POST)\ncurl -X POST http://localhost:8000/patients \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"patient_id\": 1011, \"name\": \"Test\", \"date_of_birth\": \"1990-01-01\", \"insurance_id\": \"INS001\"}'\n\n\n\n# Connect to database\ndocker-compose exec database psql -U healthuser -d healthcare\n\n# Or from host (if psql installed)\npsql -h localhost -p 5432 -U healthuser -d healthcare\n-- Common queries\nSELECT COUNT(*) FROM visits;\nSELECT COUNT(*) FROM prescriptions;\nSELECT COUNT(*) FROM lab_results;\n\n-- Get patient visit summary\nSELECT \n    patient_id,\n    COUNT(*) as visit_count,\n    MIN(visit_date) as first_visit,\n    MAX(visit_date) as last_visit\nFROM visits\nGROUP BY patient_id;\n\n-- Get prescriptions by patient\nSELECT * FROM prescriptions WHERE patient_id = 1002;\n\n-- Get lab results with visit info\nSELECT \n    l.test_name,\n    l.test_value,\n    l.test_unit,\n    v.visit_date,\n    v.provider_name\nFROM lab_results l\nJOIN visits v ON l.visit_id = v.visit_id\nWHERE l.patient_id = 1001;"
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html#configuration",
    "href": "docker-example/QUICK_REFERENCE.html#configuration",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "Create .env file (copy from .env.example):\nPOSTGRES_DB=healthcare\nPOSTGRES_USER=healthuser\nPOSTGRES_PASSWORD=healthpass\nAPI_PORT=8000\nDB_PORT=5432\n\n\n\nR (DBI):\ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  host = \"localhost\",\n  port = 5432,\n  dbname = \"healthcare\",\n  user = \"healthuser\",\n  password = \"healthpass\"\n)\nPython (psycopg2):\nconn = psycopg2.connect(\n    host=\"localhost\",\n    port=5432,\n    database=\"healthcare\",\n    user=\"healthuser\",\n    password=\"healthpass\"\n)\nPython (SQLAlchemy):\nengine = create_engine(\n    \"postgresql://healthuser:healthpass@localhost:5432/healthcare\"\n)"
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html#sample-data-overview",
    "href": "docker-example/QUICK_REFERENCE.html#sample-data-overview",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "10 patients (IDs 1001-1010)\n3 insurance types: INS001, INS002, INS003\n2 self-pay patients\n\n\n\n\n\n12 visits across multiple patients\nDate range: Jan-Apr 2024\n3 providers\n\n\n\n\n\n7 prescriptions\nVarious medications (Lisinopril, Metformin, etc.)\n\n\n\n\n\n12 lab tests\nBlood pressure, HbA1c, cholesterol, etc."
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html#common-issues",
    "href": "docker-example/QUICK_REFERENCE.html#common-issues",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "# Change ports in docker-compose.yml\nports:\n  - \"8001:8000\"  # API\n  - \"5433:5432\"  # Database\n\n\n\n# Check logs\ndocker-compose logs database\ndocker-compose logs api\n\n# Rebuild\ndocker-compose down\ndocker-compose up -d --build\n\n\n\n# Wait for health check\ndocker-compose ps\n\n# Should show \"Up (healthy)\"\n# If not, wait 10-20 seconds\n\n\n\n# Try different CRAN mirror\ninstall.packages(\"RPostgres\", \n  repos = \"https://cloud.r-project.org/\")\n\n# On Windows, install Rtools first\n# https://cran.r-project.org/bin/windows/Rtools/"
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html#project-structure",
    "href": "docker-example/QUICK_REFERENCE.html#project-structure",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "docker-example/\n‚îú‚îÄ‚îÄ api/                    # R Plumber API container\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îú‚îÄ‚îÄ plumber.R          # API endpoints\n‚îÇ   ‚îî‚îÄ‚îÄ patients_data.csv  # Sample patient data\n‚îú‚îÄ‚îÄ database/               # PostgreSQL container\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îî‚îÄ‚îÄ init.sql           # Schema and sample data\n‚îú‚îÄ‚îÄ r-client/               # R pipeline script\n‚îÇ   ‚îú‚îÄ‚îÄ healthcare_pipeline.R\n‚îÇ   ‚îî‚îÄ‚îÄ install_packages.R\n‚îú‚îÄ‚îÄ python-client/          # Python pipeline script\n‚îÇ   ‚îú‚îÄ‚îÄ healthcare_pipeline.py\n‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ docker-compose.yml      # Orchestration config\n‚îú‚îÄ‚îÄ .env.example           # Environment template\n‚îî‚îÄ‚îÄ README.md              # Full documentation"
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html#next-steps",
    "href": "docker-example/QUICK_REFERENCE.html#next-steps",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "Modify the API: Add new endpoints in api/plumber.R\nAdd Tables: Extend database/init.sql with new schemas\nEnhance Clients: Add data validation, cleaning, transformation\nAuthentication: Add API keys or OAuth to the API\nMonitoring: Add logging and metrics collection\nTesting: Write unit tests for API endpoints and pipelines"
  },
  {
    "objectID": "docker-example/QUICK_REFERENCE.html#tips",
    "href": "docker-example/QUICK_REFERENCE.html#tips",
    "title": "Quick Reference Guide",
    "section": "",
    "text": "Always check container health before running clients\nUse docker-compose logs -f to watch real-time logs\nThe API data is in-memory (resets on restart)\nThe database data persists in a Docker volume\nBoth clients do the same thing - compare them to learn!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CFA Data Pipelines Workshop",
    "section": "",
    "text": "Welcome to the CFA Data Pipelines Workshop! This course will teach you essential skills for building robust data pipelines, acquiring data from various sources, and cleaning data for analysis."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "CFA Data Pipelines Workshop",
    "section": "",
    "text": "Welcome to the CFA Data Pipelines Workshop! This course will teach you essential skills for building robust data pipelines, acquiring data from various sources, and cleaning data for analysis."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "CFA Data Pipelines Workshop",
    "section": "Course Overview",
    "text": "Course Overview\nIn this workshop, you will learn:\n\nData Pipeline Design: Best practices for designing and implementing data pipelines\nDatabase Operations: Querying and managing data in databases\nAPI Integration: How to acquire data from RESTful APIs\nData Cleaning: Techniques for preparing and validating data for analysis"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "CFA Data Pipelines Workshop",
    "section": "Getting Started",
    "text": "Getting Started\n\nReview the Syllabus for course objectives and requirements\nCheck the Schedule for class dates and topics\nAccess Slides for lecture materials"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "CFA Data Pipelines Workshop",
    "section": "Prerequisites",
    "text": "Prerequisites\nStudents should have:\n\nBasic programming knowledge (Python or R recommended)\nFamiliarity with data structures (lists, dictionaries, data frames)\nUnderstanding of basic database concepts"
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "CFA Data Pipelines Workshop",
    "section": "Course Materials",
    "text": "Course Materials\nAll course materials, including slides and example code, are available on the GitHub repository."
  },
  {
    "objectID": "pre-survey.html",
    "href": "pre-survey.html",
    "title": "Pre-Workshop Survey",
    "section": "",
    "text": "Replace this page with an embedded form (Google Forms / Microsoft Forms / Qualtrics) or connect the HTML form below to a handler (e.g.¬†Formspree) before collecting responses."
  },
  {
    "objectID": "pre-survey.html#instructions",
    "href": "pre-survey.html#instructions",
    "title": "Pre-Workshop Survey",
    "section": "Instructions",
    "text": "Instructions\nPlease complete this survey before the workshop begins. Your responses help tailor the instruction and measure learning gains. Estimated time: 5‚Äì7 minutes."
  },
  {
    "objectID": "pre-survey.html#section-1-background-demographics",
    "href": "pre-survey.html#section-1-background-demographics",
    "title": "Pre-Workshop Survey",
    "section": "Section 1: Background & Demographics",
    "text": "Section 1: Background & Demographics\n\nRole / Title\nYears of experience with data, analytics, and programming (dropdown: None, &lt;1, 1‚Äì2, 3‚Äì5, &gt;5)\nComfort with command line tools (Likert 1‚Äì5)"
  },
  {
    "objectID": "pre-survey.html#section-2-prior-experience-likert-1never-5expert",
    "href": "pre-survey.html#section-2-prior-experience-likert-1never-5expert",
    "title": "Pre-Workshop Survey",
    "section": "Section 2: Prior Experience (Likert 1=Never ‚Ä¶ 5=Expert)",
    "text": "Section 2: Prior Experience (Likert 1=Never ‚Ä¶ 5=Expert)\nRate your experience/comfort with the following languages 1. R 2. Python 3. SQL\nRate your experience with the following tasks: 1. I have downloaded data from the internet. 2. I can query databases confidently. 3. I can retrieve data from REST APIs. 4. I understand principles of data cleaning and validation. 5. I can design a reproducible data structure."
  },
  {
    "objectID": "pre-survey.html#section-3-self-efficacy-likert-15",
    "href": "pre-survey.html#section-3-self-efficacy-likert-15",
    "title": "Pre-Workshop Survey",
    "section": "Section 3: Self-Efficacy (Likert 1‚Äì5)",
    "text": "Section 3: Self-Efficacy (Likert 1‚Äì5)\n\nI feel confident troubleshooting data pipeline failures.\nI can choose appropriate tools for data acquisition.\nI can assess data quality issues systematically.\nI can document a pipeline for other users."
  },
  {
    "objectID": "pre-survey.html#section-4-knowledge-check-multiple-choice",
    "href": "pre-survey.html#section-4-knowledge-check-multiple-choice",
    "title": "Pre-Workshop Survey",
    "section": "Section 4: Knowledge Check (Multiple Choice)",
    "text": "Section 4: Knowledge Check (Multiple Choice)\nSelect the best answer. 1. A data pipeline is best described as: (a) A single script; (b) A series of connected processes moving and transforming data; (c) A database schema; (d) A visualization tool. 2. The HTTP status code 429 indicates: (a) Success; (b) Not Found; (c) Too Many Requests; (d) Unauthorized. 3. In SQL, selecting unique values uses: (a) DISTINCT; (b) UNIQUE; (c) GROUP; (d) ONLY. 4. Best first step when cleaning a dataset: (a) Delete all NAs; (b) Profile structure and summary statistics; (c) Normalize everything; (d) Load into production. 5. An API rate limit is: (a) Maximum query complexity; (b) Restriction on number of requests per time window; (c) Disk quota; (d) Memory threshold."
  },
  {
    "objectID": "pre-survey.html#section-5-expectations",
    "href": "pre-survey.html#section-5-expectations",
    "title": "Pre-Workshop Survey",
    "section": "Section 5: Expectations",
    "text": "Section 5: Expectations\n\nWhat is your primary goal for attending? (long text)\nWhich topic are you MOST interested in? (Single choice: APIs, Databases, Cleaning, Pipeline Design, Other)\nAny specific questions you hope get answered? (long text)"
  },
  {
    "objectID": "SETUP.html",
    "href": "SETUP.html",
    "title": "CFA Data Pipelines Workshop - Setup",
    "section": "",
    "text": "This project uses renv to manage both R and Python dependencies.\n\nTell renv where Python is:\n# In R console\nrenv::use_python(\"C:/Users/u0092104/AppData/Local/Python/pythoncore-3.14-64/python.exe\")\nInstall Python packages:\n# renv will create a virtual environment and install from requirements.txt\nrenv::restore()\n\n\n\n\nIf you prefer traditional Python workflow:\n# Create virtual environment\npython -m venv .venv\n\n# Activate\n.venv\\Scripts\\Activate.ps1\n\n# Install packages\npip install -r requirements.txt\nThen set the Python path in your R session:\nSys.setenv(RETICULATE_PYTHON = \".venv/Scripts/python.exe\")\n\n\n\n\n# Install required R packages\ninstall.packages(c(\"httr\", \"jsonlite\", \"dplyr\", \"DBI\", \"RSQLite\", \"ggplot2\"))\n\n# Or use renv\nrenv::restore()\n\n\n\nR:\nlibrary(httr)\nlibrary(dplyr)\nlibrary(ggplot2)\nPython (in R via reticulate):\nlibrary(reticulate)\npy_config()  # Should show your Python path\n\n\n\nquarto render slides/01-introduction.qmd\nOr preview:\nquarto preview slides/01-introduction.qmd"
  },
  {
    "objectID": "SETUP.html#python-environment-setup",
    "href": "SETUP.html#python-environment-setup",
    "title": "CFA Data Pipelines Workshop - Setup",
    "section": "",
    "text": "This project uses renv to manage both R and Python dependencies.\n\nTell renv where Python is:\n# In R console\nrenv::use_python(\"C:/Users/u0092104/AppData/Local/Python/pythoncore-3.14-64/python.exe\")\nInstall Python packages:\n# renv will create a virtual environment and install from requirements.txt\nrenv::restore()\n\n\n\n\nIf you prefer traditional Python workflow:\n# Create virtual environment\npython -m venv .venv\n\n# Activate\n.venv\\Scripts\\Activate.ps1\n\n# Install packages\npip install -r requirements.txt\nThen set the Python path in your R session:\nSys.setenv(RETICULATE_PYTHON = \".venv/Scripts/python.exe\")"
  },
  {
    "objectID": "SETUP.html#r-package-setup",
    "href": "SETUP.html#r-package-setup",
    "title": "CFA Data Pipelines Workshop - Setup",
    "section": "",
    "text": "# Install required R packages\ninstall.packages(c(\"httr\", \"jsonlite\", \"dplyr\", \"DBI\", \"RSQLite\", \"ggplot2\"))\n\n# Or use renv\nrenv::restore()"
  },
  {
    "objectID": "SETUP.html#verifying-setup",
    "href": "SETUP.html#verifying-setup",
    "title": "CFA Data Pipelines Workshop - Setup",
    "section": "",
    "text": "R:\nlibrary(httr)\nlibrary(dplyr)\nlibrary(ggplot2)\nPython (in R via reticulate):\nlibrary(reticulate)\npy_config()  # Should show your Python path"
  },
  {
    "objectID": "SETUP.html#rendering-slides",
    "href": "SETUP.html#rendering-slides",
    "title": "CFA Data Pipelines Workshop - Setup",
    "section": "",
    "text": "quarto render slides/01-introduction.qmd\nOr preview:\nquarto preview slides/01-introduction.qmd"
  },
  {
    "objectID": "slides/02-apis.html",
    "href": "slides/02-apis.html",
    "title": "Data Acquisition from APIs",
    "section": "",
    "text": "This sessions‚Äôs topics:\n\nRESTful API fundamentals\nHTTP methods and status codes\nAuthentication\nPagination\nRate limiting\nError handling"
  },
  {
    "objectID": "slides/02-apis.html#overview",
    "href": "slides/02-apis.html#overview",
    "title": "Data Acquisition from APIs",
    "section": "",
    "text": "This sessions‚Äôs topics:\n\nRESTful API fundamentals\nHTTP methods and status codes\nAuthentication\nPagination\nRate limiting\nError handling"
  },
  {
    "objectID": "slides/02-apis.html#what-is-an-api",
    "href": "slides/02-apis.html#what-is-an-api",
    "title": "Data Acquisition from APIs",
    "section": "What is an API?",
    "text": "What is an API?\nApplication Programming Interface (API)\n\nA way for applications to communicate\nDefines methods and data structures\nREST is most common web API style\nGraphQL is gaining popularity\nOther API styles are irrelevant to this presentation"
  },
  {
    "objectID": "slides/02-apis.html#api-process",
    "href": "slides/02-apis.html#api-process",
    "title": "Data Acquisition from APIs",
    "section": "API Process",
    "text": "API Process\n\n\n\n\n\nsequenceDiagram\n    actor U as User\n    participant C as Client\n    participant A as Authentication Server\n    participant H as API Server\n    box pink Restricted\n    participant S as SQL Server\n    end\n    U-&gt;&gt;C: I need data\n    C-&gt;&gt;A: Let me in?\n    A-&gt;&gt;C: What's the password? \n    C-&gt;&gt;U: Password?\n    U-&gt;&gt;C: ******\n    C-&gt;&gt;A: credentials\n    A-&gt;&gt;C: token\n    C-&gt;&gt;H: I can haz data? + token\n    H-&gt;&gt;A: token good?\n    A-&gt;&gt;H: token is good.\n    H-&gt;&gt;S: Data?\n    S-&gt;&gt;H: Data!\n    H-&gt;&gt;C: Data\n    C-&gt;&gt;U: Show data"
  },
  {
    "objectID": "slides/02-apis.html#restful-apis",
    "href": "slides/02-apis.html#restful-apis",
    "title": "Data Acquisition from APIs",
    "section": "RESTful APIs",
    "text": "RESTful APIs\nREST = Representational State Transfer\nKey concepts:\n\nResources (nouns)\nHTTP methods (verbs)\nStateless communication\nStandard data formats (JSON, XML)"
  },
  {
    "objectID": "slides/02-apis.html#graphql-apis",
    "href": "slides/02-apis.html#graphql-apis",
    "title": "Data Acquisition from APIs",
    "section": "GraphQL APIs",
    "text": "GraphQL APIs\nGraphQL = Query Language for APIs\nKey concepts:\n\nSingle endpoint\nClient specifies exact data needed\nStrongly typed schema\nNo over-fetching or under-fetching\nIntrospective (self-documenting)"
  },
  {
    "objectID": "slides/02-apis.html#rest-vs-graphql",
    "href": "slides/02-apis.html#rest-vs-graphql",
    "title": "Data Acquisition from APIs",
    "section": "REST vs GraphQL",
    "text": "REST vs GraphQL\n\n\nREST\n\nMultiple endpoints\nFixed data structure\nOver/under-fetching common\nEasier to cache\nSimpler to learn\nWidely adopted\n\n\nGraphQL\n\nSingle endpoint\nFlexible queries\nPrecise data fetching\nMore complex caching\nSteeper learning curve\nGrowing adoption"
  },
  {
    "objectID": "slides/02-apis.html#rest-api",
    "href": "slides/02-apis.html#rest-api",
    "title": "Data Acquisition from APIs",
    "section": "REST API",
    "text": "REST API\nCRUD = Create, Read, Update, and Delete\nREST API Components\n\nEndpoint: URL where resource can be accessed\nMethod: HTTP verb (GET, POST, PUT, DELETE)\nHeaders: Metadata (auth, content-type)\nQuery/Body: Data sent with request (POST, PUT)\nResponse: Data returned by server\n\nStatus Code: Result of request (200, 404, 500)\nHeaders: Metadata (content-type)\nBody: The data."
  },
  {
    "objectID": "slides/02-apis.html#http-methods",
    "href": "slides/02-apis.html#http-methods",
    "title": "Data Acquisition from APIs",
    "section": "HTTP Methods",
    "text": "HTTP Methods\n\n\n\nMethod\nPurpose\nIdempotent\nReturns Data\n\n\n\n\nGET\nRetrieve data\nYes\nYes\n\n\nPOST\nCreate resource\nNo\nDepends\n\n\nPUT\nUpdate/replace\nYes\nDepends\n\n\nPATCH\nPartial update\nNo\nDepends\n\n\nDELETE\nRemove resource\nYes\nNo"
  },
  {
    "objectID": "slides/02-apis.html#common-request-headers",
    "href": "slides/02-apis.html#common-request-headers",
    "title": "Data Acquisition from APIs",
    "section": "Common Request Headers",
    "text": "Common Request Headers\n\n\n\nHeader\nPurpose\nExample\n\n\n\n\nAuthorization\nAuthentication credentials\nBearer token123\n\n\nContent-Type\nFormat of request body\napplication/json\n\n\nAccept\nDesired response format\napplication/json\n\n\nUser-Agent\nClient identification\nMyApp/1.0"
  },
  {
    "objectID": "slides/02-apis.html#query-parameters",
    "href": "slides/02-apis.html#query-parameters",
    "title": "Data Acquisition from APIs",
    "section": "Query Parameters",
    "text": "Query Parameters\nURL parameters that modify API requests:\nFormat: &lt;&lt;URL&gt;&gt;?param1=value1&param2=value2\nCommon Uses:\n\nFiltering: ?status=active&type=user\nSorting: ?sort=name&order=asc\nPagination: ?page=2&limit=50\nField selection: ?fields=id,name,email\n\nExample:\nhttps://www.google.com/search?q=url+parameters+example"
  },
  {
    "objectID": "slides/02-apis.html#request-body",
    "href": "slides/02-apis.html#request-body",
    "title": "Data Acquisition from APIs",
    "section": "Request Body",
    "text": "Request Body\nUsed with POST, PUT, and PATCH methods to send data to the server.\n\nJSON Body Example\n{\n    \"title\": \"CFA-Data-Pipelines-Workshop\",\n    \"start_date\": \"2025-12-02\"\n}"
  },
  {
    "objectID": "slides/02-apis.html#revisit-example",
    "href": "slides/02-apis.html#revisit-example",
    "title": "Data Acquisition from APIs",
    "section": "Revisit Example",
    "text": "Revisit Example\n\n\nWarning: package 'httr2' was built under R version 4.5.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nWarning: package 'RSQLite' was built under R version 4.5.2\n\n\n\n\nCode:\n\nend_date &lt;- Sys.Date()\nstart_date &lt;- end_date - 365\n\nurl &lt;- \"https://archive-api.open-meteo.com/v1/archive\"\nresponse &lt;- request(url) |&gt;\n  req_url_query(\n    latitude = 40.5981,\n    longitude = -111.5831,\n    start_date = format(start_date, \"%Y-%m-%d\"),\n    end_date = format(end_date, \"%Y-%m-%d\"),\n    daily = \"precipitation_sum,snowfall_sum\",\n    timezone = \"America/Denver\"\n  ) |&gt;\n  httr2::req_perform()\n\n\nComponents:\n\nEndpoint: https://archive-api.open-meteo.com/v1/archive\nMethod: GET\nHeaders: None\nQuery:\n\nlatitude=40.5981&\nlongitude=-111.5831&\nstart_date=2024-11-26&\nend_date=2025-11-26&\ndaily=precipitation_sum%2Csnowfall_sum&\ntimezone=America%2FDenver"
  },
  {
    "objectID": "slides/02-apis.html#api-response",
    "href": "slides/02-apis.html#api-response",
    "title": "Data Acquisition from APIs",
    "section": "API Response",
    "text": "API Response\nThe data returned by the server after processing a request.\nResponse Components\n\nStatus Code: Indicates success or failure (200, 404, etc.)\nHeaders: Metadata about the response (content-type, rate limits)\nBody: The actual data (usually JSON or XML)\n\n\nnames(response)\n\n[1] \"method\"      \"url\"         \"status_code\" \"headers\"     \"body\"       \n[6] \"timing\"      \"request\"     \"cache\""
  },
  {
    "objectID": "slides/02-apis.html#http-status-codes",
    "href": "slides/02-apis.html#http-status-codes",
    "title": "Data Acquisition from APIs",
    "section": "HTTP Status Codes",
    "text": "HTTP Status Codes\n\nresponse$status_code\n\n[1] 200\n\n\n\n\nSuccess (2xx)\n\n200 OK\n201 Created\n204 No Content\n\n\nClient Errors (4xx)\n\n400 Bad Request\n401 Unauthorized\n404 Not Found\n429 Too Many Requests\n\n\nServer Errors (5xx)\n\n500 Internal Server Error\n502 Bad Gateway\n503 Service Unavailable"
  },
  {
    "objectID": "slides/02-apis.html#response-headers",
    "href": "slides/02-apis.html#response-headers",
    "title": "Data Acquisition from APIs",
    "section": "Response Headers",
    "text": "Response Headers\n\nglimpse(response$headers)\n\n &lt;httr2_headers&gt;\n $ Date             : chr \"Mon, 01 Dec 2025 20:52:55 GMT\"\n $ Content-Type     : chr \"application/json; charset=utf-8\"\n $ Transfer-Encoding: chr \"chunked\"\n $ Connection       : chr \"keep-alive\"\n $ Content-Encoding : chr \"deflate\"\n\n\n\n\n\nHeader\nPurpose\nExample\n\n\n\n\nContent-Type\nFormat of response\napplication/json\n\n\nX-RateLimit-Limit\nMax requests allowed\n1000\n\n\nX-RateLimit-Remaining\nRequests left\n742\n\n\nX-RateLimit-Reset\nWhen limit resets\n1640995200"
  },
  {
    "objectID": "slides/02-apis.html#response-body",
    "href": "slides/02-apis.html#response-body",
    "title": "Data Acquisition from APIs",
    "section": "Response Body",
    "text": "Response Body\n\nUsually JSON or XML\nDefined by the header Content-Type\n\nPOST example\n{\n    \"id\": 1,\n    \"title\": \"CFA-Data-Pipelines-Workshop\",\n    \"start_date\": \"2025-12-02\"\n}\nWeather example\n{\n    \"latitude\":40.597538, \"longitude\":-111.64073,\n    \"generationtime_ms\":24.38044548034668,\n    \"utc_offset_seconds\":-25200, \"timezone\":\"America/Denver\", \"timezone_abbreviation\":\"GMT-7\",\n    \"elevation\":2680.0,\n    \"daily_units\":{\"time\":\"iso8601\", \"precipitation_sum\":\"mm\", \"snowfall_sum\":\"cm\"},\n    \"daily\":{\n        \"time\":[\"2024-11-26\",\"2024-11-27\",\"2024-11-28\",...],\n        \"precipitation_sum\":[19.20,0.50,0.00,0.00,0.00,...],\n        \"snowfall_sum\":[13.44,0.35,0.00,0.00,0.00,0.00,...]\n    }\n}\nShwon with manual formatting and truncation."
  },
  {
    "objectID": "slides/02-apis.html#authentication-methods",
    "href": "slides/02-apis.html#authentication-methods",
    "title": "Data Acquisition from APIs",
    "section": "Authentication Methods",
    "text": "Authentication Methods\nDifferent ways APIs verify client identity and authorize access:\nCommon Authentication Methods:\n\nAPI Keys: Simple token in header or query parameter\nBearer Tokens: Token-based authentication (often JWT - JSON Web Token)\nBasic Auth: Username and password (base64 encoded)\nOAuth 2.0: Delegated authorization protocol\nOAuth 1.0: Legacy delegated authorization (less common)\nDigest Authentication: More secure than Basic Auth\nClient Certificates: Mutual TLS authentication\n\nSecurity Best Practices:\n\nNever commit credentials to version control\nUse environment variables or secure credential stores\nRotate keys/tokens regularly\nUse HTTPS for all API requests"
  },
  {
    "objectID": "slides/02-apis.html#authentication-personal-access-token",
    "href": "slides/02-apis.html#authentication-personal-access-token",
    "title": "Data Acquisition from APIs",
    "section": "Authentication: Personal Access Token",
    "text": "Authentication: Personal Access Token\nGitHub API Example (students will follow along)\nGitHub Rest API Documentation\n\nStep 1: Generate Token\n\nGo to GitHub ‚Üí Settings ‚Üí Developer settings ‚Üí Personal access tokens ‚Üí Tokens (classic)\nClick ‚ÄúGenerate new token (classic)‚Äù\nGive it a note (e.g., ‚ÄúWorkshop API Demo‚Äù)\nSelect scopes: repo, read:user\nGenerate and copy token (starts with ghp_)\n\n‚ö†Ô∏è Save immediately ‚Äî you won‚Äôt see it again!"
  },
  {
    "objectID": "slides/02-apis.html#storing-tokens-securely",
    "href": "slides/02-apis.html#storing-tokens-securely",
    "title": "Data Acquisition from APIs",
    "section": "Storing Tokens Securely",
    "text": "Storing Tokens Securely\nOption 1: .Renviron file (Recommended)\nCreate/edit .Renviron in your home directory:\nGITHUB_TOKEN=ghp_xaIy********************************\nObfuscated because it is supposed to be secret."
  },
  {
    "objectID": "slides/02-apis.html#authentication-using-your-token-r",
    "href": "slides/02-apis.html#authentication-using-your-token-r",
    "title": "Data Acquisition from APIs",
    "section": "Authentication: Using Your Token (R)",
    "text": "Authentication: Using Your Token (R)\n\nlibrary(httr2)\n\n# Get token from environment\ntoken &lt;- Sys.getenv(\"GITHUB_TOKEN\")\n\nresponse &lt;- request(\"https://api.github.com/user/repos\") |&gt;\n  req_auth_bearer_token(token) |&gt;\n  req_perform()\n\nresp_check_status(response)\nif (resp_status(response) == 200) {\n  repos &lt;- resp_body_json(response)\n  cat(sprintf(\"%d projects total.\\n\", length(repos)))\n  for (repo in repos[1:5]) {\n    cat(sprintf(\"%s: %d stars\\n\", \n                repo$name, \n                repo$stargazers_count))\n  }\n}\n\nMy output\n30 projects total.\n.github: 0 stars\n.github-private: 0 stars\nabm-benchmark: 1 stars\naccessibility-check: 1 stars\naccessible-wastewater-dashboard: 1 stars"
  },
  {
    "objectID": "slides/02-apis.html#authentication-using-your-token-python",
    "href": "slides/02-apis.html#authentication-using-your-token-python",
    "title": "Data Acquisition from APIs",
    "section": "Authentication: Using Your Token (Python)",
    "text": "Authentication: Using Your Token (Python)\nimport requests\nimport os\n\n# Store token in environment variable (never hardcode!)\ntoken = os.getenv('GITHUB_TOKEN')\nheaders = {'Authorization': f'token {token}'}\n\n# Get your repos\nresponse = requests.get(\n    'https://api.github.com/user/repos',\n    headers=headers\n)\n\nif response.status_code == 200:\n    repos = response.json()\n    for repo in repos[:5]:\n        print(f\"{repo['name']}: {repo['stargazers_count']} stars\")"
  },
  {
    "objectID": "slides/02-apis.html#setting-environment-variables",
    "href": "slides/02-apis.html#setting-environment-variables",
    "title": "Data Acquisition from APIs",
    "section": "Setting Environment Variables",
    "text": "Setting Environment Variables\nWindows PowerShell:\n$env:GITHUB_TOKEN = \"ghp_your_token_here\"\nWindows CMD:\nset GITHUB_TOKEN=ghp_your_token_here\nmacOS/Linux:\nexport GITHUB_TOKEN=\"ghp_your_token_here\"\nPersistent (add to .env file):\nGITHUB_TOKEN=ghp_your_token_here\nThen load with python-dotenv or similar."
  },
  {
    "objectID": "slides/02-apis.html#authentication-using-your-token-r-with-data",
    "href": "slides/02-apis.html#authentication-using-your-token-r-with-data",
    "title": "Data Acquisition from APIs",
    "section": "Authentication: Using Your Token (R), with data",
    "text": "Authentication: Using Your Token (R), with data\n\nlibrary(dplyr)\n# Token not teechnically needed as the data is public.\n# token &lt;- Sys.getenv(\"GITHUB_TOKEN\")\n\nresponse &lt;- request(\n  \"https://api.github.com/repos/EpiForeSITE/multigroup-vaccine/contents/data-raw/cc-est2024-agesex-49.csv\"\n) |&gt;\n  # req_auth_bearer_token(token) # if authentication needed\n  req_perform()\n\nresp_check_status(response)\n# Decode base64 content and load into R\ncontent_data &lt;- resp_body_json(response)\nraw_data &lt;- base64enc::base64decode(content_data$content)\ntemp_file &lt;- tempfile(fileext = \".csv\")\nwriteBin(raw_data, temp_file)\n# Cleaning replicates \n# https://github.com/EpiForeSITE/multigroup-vaccine/blob/main/data-raw/UtahAgeCountyPop.R\nUtahAgeCountyPop &lt;- read.csv(temp_file) |&gt; \n        filter( YEAR == max(YEAR)) |&gt;  # Most recent year only\n        select(CTYNAME, POPESTIMATE, ends_with(\"_TOT\")) |&gt;\n        rename(\n            county = CTYNAME, \n            total = POPESTIMATE,\n            age0to4 = AGE04_TOT,\n            age5to9 = AGE59_TOT,\n            age85plus = AGE85PLUS_TOT\n        ) |&gt;\n        mutate(  # Select, rename, and create variables\n            # New Variables\n            age10to13 = AGE513_TOT - age5to9,\n            age14 = AGE1014_TOT - age10to13,\n            age15 = total - AGE16PLUS_TOT - age0to4 - age5to9 - age10to13 - age14, \n            age16to17 = AGE16PLUS_TOT - AGE18PLUS_TOT,\n            age18to19 = AGE1519_TOT - age15 - age16to17,\n        ) |&gt;\n        rename_with(\n            \\(name)gsub('AGE(\\\\d\\\\d)(\\\\d\\\\d)_TOT$', 'age\\\\1to\\\\2', name),\n            .cols = matches(\"AGE\\\\d{4}_TOT\")\n        )\nunlink(temp_file)"
  },
  {
    "objectID": "slides/02-apis.html#authentication-using-your-token-r-with-data-continued",
    "href": "slides/02-apis.html#authentication-using-your-token-r-with-data-continued",
    "title": "Data Acquisition from APIs",
    "section": "Authentication: Using Your Token (R), with data, continued",
    "text": "Authentication: Using Your Token (R), with data, continued\n\nglimpse(UtahAgeCountyPop)\n\nRows: 29\nColumns: 36\n$ county         &lt;chr&gt; \"Beaver County\", \"Box Elder County\", \"Cache County\", \"C‚Ä¶\n$ total          &lt;int&gt; 7424, 64120, 145487, 20613, 956, 378470, 20803, 10161, ‚Ä¶\n$ UNDER5_TOT     &lt;int&gt; 545, 4445, 10295, 1164, 39, 24440, 1505, 650, 265, 482,‚Ä¶\n$ AGE513_TOT     &lt;int&gt; 1046, 9599, 20627, 2561, 104, 55781, 3318, 1369, 554, 1‚Ä¶\n$ age14to17      &lt;int&gt; 542, 4737, 9982, 1333, 58, 28131, 1635, 728, 295, 474, ‚Ä¶\n$ age18to24      &lt;int&gt; 744, 5917, 27874, 1880, 57, 37273, 1847, 889, 428, 782,‚Ä¶\n$ AGE16PLUS_TOT  &lt;int&gt; 5558, 47787, 109660, 16248, 786, 284401, 15169, 7792, 4‚Ä¶\n$ AGE18PLUS_TOT  &lt;int&gt; 5291, 45339, 104583, 15555, 755, 270118, 14345, 7414, 4‚Ä¶\n$ age15to44      &lt;int&gt; 2980, 27044, 73101, 7942, 270, 167186, 8297, 3817, 1893‚Ä¶\n$ age25to44      &lt;int&gt; 1832, 17488, 37667, 5039, 167, 108580, 5229, 2366, 1242‚Ä¶\n$ age45to64      &lt;int&gt; 1489, 13394, 24065, 4584, 261, 80083, 4359, 2282, 1209,‚Ä¶\n$ AGE65PLUS_TOT  &lt;int&gt; 1226, 8540, 14977, 4052, 270, 44182, 2910, 1877, 1297, ‚Ä¶\n$ age0to4        &lt;int&gt; 545, 4445, 10295, 1164, 39, 24440, 1505, 650, 265, 482,‚Ä¶\n$ age5to9        &lt;int&gt; 552, 5248, 11138, 1315, 57, 29636, 1779, 732, 310, 556,‚Ä¶\n$ age10to14      &lt;int&gt; 632, 5449, 11911, 1556, 59, 32943, 1953, 803, 316, 591,‚Ä¶\n$ age15to19      &lt;int&gt; 612, 5329, 14366, 1565, 66, 31827, 1782, 837, 371, 536,‚Ä¶\n$ age20to24      &lt;int&gt; 536, 4227, 21068, 1338, 37, 26779, 1286, 614, 280, 596,‚Ä¶\n$ age25to29      &lt;int&gt; 508, 4484, 11702, 1173, 30, 26664, 1077, 608, 286, 582,‚Ä¶\n$ age30to34      &lt;int&gt; 447, 4306, 9430, 1215, 38, 26815, 1268, 533, 285, 715, ‚Ä¶\n$ age35to39      &lt;int&gt; 411, 4127, 7964, 1274, 44, 26160, 1388, 547, 306, 647, ‚Ä¶\n$ age40to44      &lt;int&gt; 466, 4571, 8571, 1377, 55, 28941, 1496, 678, 365, 750, ‚Ä¶\n$ age45to49      &lt;int&gt; 447, 4125, 7732, 1326, 55, 26001, 1328, 655, 301, 628, ‚Ä¶\n$ age50to54      &lt;int&gt; 361, 3129, 6169, 1103, 65, 20379, 1035, 548, 269, 546, ‚Ä¶\n$ age55to59      &lt;int&gt; 289, 2825, 5079, 951, 60, 16610, 940, 483, 254, 472, 27‚Ä¶\n$ age60to64      &lt;int&gt; 392, 3315, 5085, 1204, 81, 17093, 1056, 596, 385, 631, ‚Ä¶\n$ age65to69      &lt;int&gt; 425, 2961, 4721, 1388, 71, 15143, 1042, 637, 397, 644, ‚Ä¶\n$ age70to74      &lt;int&gt; 349, 2334, 4081, 1145, 82, 11658, 764, 529, 372, 583, 2‚Ä¶\n$ age75to79      &lt;int&gt; 222, 1508, 2949, 770, 60, 8376, 516, 346, 286, 392, 189‚Ä¶\n$ age80to84      &lt;int&gt; 147, 954, 1794, 419, 38, 5052, 338, 203, 114, 245, 1056‚Ä¶\n$ age85plus      &lt;int&gt; 83, 783, 1432, 330, 19, 3953, 250, 162, 128, 192, 681, ‚Ä¶\n$ MEDIAN_AGE_TOT &lt;dbl&gt; 33.7, 33.4, 26.4, 38.9, 49.8, 33.2, 34.1, 37.9, 43.1, 4‚Ä¶\n$ age10to13      &lt;int&gt; 494, 4351, 9489, 1246, 47, 26145, 1539, 637, 244, 467, ‚Ä¶\n$ age14          &lt;int&gt; 138, 1098, 2422, 310, 12, 6798, 414, 166, 72, 124, 1077‚Ä¶\n$ age15          &lt;int&gt; 137, 1191, 2483, 330, 15, 7050, 397, 184, 68, 120, 1045‚Ä¶\n$ age16to17      &lt;int&gt; 267, 2448, 5077, 693, 31, 14283, 824, 378, 155, 230, 22‚Ä¶\n$ age18to19      &lt;int&gt; 208, 1690, 6806, 542, 20, 10494, 561, 275, 148, 186, 29‚Ä¶"
  },
  {
    "objectID": "slides/02-apis.html#oauth-2.0-pros-and-cons",
    "href": "slides/02-apis.html#oauth-2.0-pros-and-cons",
    "title": "Data Acquisition from APIs",
    "section": "OAuth 2.0: Pros and Cons",
    "text": "OAuth 2.0: Pros and Cons\n\n\nPros\n\nNo password sharing with third parties\nFine-grained permission scopes\nEasy to revoke access per application\nToken expiration and refresh\nIndustry standard (widely supported)\nBetter audit trail\nSecure delegation of access\n\n\nCons\n\nComplex implementation\nRequires web server/callback URL\nMultiple steps in auth flow\nToken management overhead\nPotential security misconfiguration\nOverkill for simple use cases\nRequires user interaction"
  },
  {
    "objectID": "slides/02-apis.html#oauth-2.0-full-flow-demo-only",
    "href": "slides/02-apis.html#oauth-2.0-full-flow-demo-only",
    "title": "Data Acquisition from APIs",
    "section": "OAuth 2.0 Full Flow (Demo Only)",
    "text": "OAuth 2.0 Full Flow (Demo Only)\nThree-Legged OAuth (GitHub OAuth Apps)\n\n\n\n\n\nsequenceDiagram\n    participant User\n    participant YourApp\n    participant GitHub\n    User-&gt;&gt;YourApp: Click \"Login with GitHub\"\n    YourApp-&gt;&gt;GitHub: Redirect to authorization URL\n    GitHub-&gt;&gt;User: Show consent screen\n    User-&gt;&gt;GitHub: Approve access\n    GitHub-&gt;&gt;YourApp: Redirect with authorization code\n    YourApp-&gt;&gt;GitHub: Exchange code for access token\n    GitHub-&gt;&gt;YourApp: Return access token\n    YourApp-&gt;&gt;GitHub: API requests with token"
  },
  {
    "objectID": "slides/02-apis.html#oauth-2.0-implementation-demo-only",
    "href": "slides/02-apis.html#oauth-2.0-implementation-demo-only",
    "title": "Data Acquisition from APIs",
    "section": "OAuth 2.0 Implementation (Demo Only)",
    "text": "OAuth 2.0 Implementation (Demo Only)\nStep 1: Register OAuth App - GitHub ‚Üí Settings ‚Üí Developer settings ‚Üí OAuth Apps - Set callback URL (e.g., http://localhost:8000/callback) - Get Client ID and Client Secret\nStep 2: Create OAuth Client and Get Token\n\nclient &lt;- oauth_client(\n  id = \"Ov23li3mTc0U7AAsfVen\",\n  secret = obfuscated(\"ynoMhQm-aBPCSASy5ep3CniXaCXId54KiYc8b8vRlzshbD7U69M_2y-iSP9R_ilQqWVqfZbFNY8\"),\n  token_url = \"https://github.com/login/oauth/access_token\",\n  name = \"API Workshop Demo\"\n)\n# Retrieve authorization token\ntoken &lt;- oauth_flow_auth_code(\n  client = client,\n  auth_url = \"https://github.com/login/oauth/authorize\"\n)"
  },
  {
    "objectID": "slides/02-apis.html#oauth-2.0-implementation-continued",
    "href": "slides/02-apis.html#oauth-2.0-implementation-continued",
    "title": "Data Acquisition from APIs",
    "section": "OAuth 2.0 Implementation (continued)",
    "text": "OAuth 2.0 Implementation (continued)\nStep 3: Use Token\n\n# Use the access token to make authenticated requests\nuser_info &lt;- request('https://api.github.com/user') |&gt;\n  req_auth_bearer_token(token$access_token) |&gt;\n  req_perform() |&gt;\n  resp_body_json()\n\nprint(user_info$login)"
  },
  {
    "objectID": "slides/02-apis.html#personal-access-token-vs-oauth",
    "href": "slides/02-apis.html#personal-access-token-vs-oauth",
    "title": "Data Acquisition from APIs",
    "section": "Personal Access Token vs OAuth",
    "text": "Personal Access Token vs OAuth\n\n\nPersonal Access Token\n\n‚úÖ Simple for scripts/tools\n‚úÖ Quick setup\n‚úÖ Good for personal use\n‚ùå User must manually generate\n‚ùå Harder to revoke selectively\nUse for: CLI tools, personal scripts\n\n\nOAuth 2.0\n\n‚úÖ User doesn‚Äôt see/manage token\n‚úÖ Granular permissions\n‚úÖ Easy to revoke per-app\n‚ùå Complex implementation\n‚ùå Requires web server\nUse for: Web apps, third-party integrations"
  },
  {
    "objectID": "slides/02-apis.html#hands-on-github-api-authentication",
    "href": "slides/02-apis.html#hands-on-github-api-authentication",
    "title": "Data Acquisition from APIs",
    "section": "Hands-on: GitHub API Authentication",
    "text": "Hands-on: GitHub API Authentication\nExercise:\n\nGenerate a GitHub personal access token\nSet it as an environment variable\nFetch your repositories using the API\nPrint repo names and star counts\nBonus: Filter for public repos only\nBonus: Get your latest commit for each repo\n\nStarter code and solutions in exercises/ folder:\n\nR: 02-github-api-auth-starter.R and 02-github-api-auth-solution.R\nPython: 02-github-api-auth-starter.py and 02-github-api-auth-solution.py"
  },
  {
    "objectID": "slides/02-apis.html#pagination",
    "href": "slides/02-apis.html#pagination",
    "title": "Data Acquisition from APIs",
    "section": "Pagination",
    "text": "Pagination\nAPIs limit response size. Common patterns:\n\nOffset-based\npage = 0\nper_page = 100\n\nwhile True:\n    response = requests.get(url, params={\n        'offset': page * per_page,\n        'limit': per_page\n    })\n    data = response.json()\n    if not data:\n        break\n    # Process data\n    page += 1"
  },
  {
    "objectID": "slides/02-apis.html#pagination-continued",
    "href": "slides/02-apis.html#pagination-continued",
    "title": "Data Acquisition from APIs",
    "section": "Pagination (continued)",
    "text": "Pagination (continued)\n\nCursor-based\ncursor = None\n\nwhile True:\n    params = {'cursor': cursor} if cursor else {}\n    response = requests.get(url, params=params)\n    data = response.json()\n    \n    # Process data['results']\n    \n    cursor = data.get('next_cursor')\n    if not cursor:\n        break"
  },
  {
    "objectID": "slides/02-apis.html#rate-limiting",
    "href": "slides/02-apis.html#rate-limiting",
    "title": "Data Acquisition from APIs",
    "section": "Rate Limiting",
    "text": "Rate Limiting\nAPIs limit request frequency:\n\nRequests per second/minute/hour\nOften returned in response headers\n\nimport time\n\nrate_limit = 60  # requests per minute\ndelay = 60 / rate_limit\n\nfor item in items:\n    response = requests.get(url)\n    # Process response\n    time.sleep(delay)"
  },
  {
    "objectID": "slides/02-apis.html#error-handling",
    "href": "slides/02-apis.html#error-handling",
    "title": "Data Acquisition from APIs",
    "section": "Error Handling",
    "text": "Error Handling\nimport requests\nfrom requests.exceptions import RequestException\n\ndef fetch_data(url, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            return response.json()\n        except RequestException as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(2 ** attempt)  # Exponential backoff"
  },
  {
    "objectID": "slides/02-apis.html#best-practices",
    "href": "slides/02-apis.html#best-practices",
    "title": "Data Acquisition from APIs",
    "section": "Best Practices",
    "text": "Best Practices\n\nRespect rate limits: Don‚Äôt overwhelm the API\nHandle errors gracefully: Retry with backoff\nCache responses: Reduce unnecessary requests\nUse timeouts: Don‚Äôt wait forever\nLog requests: Track API usage\nValidate responses: Check data structure"
  },
  {
    "objectID": "slides/02-apis.html#working-with-json",
    "href": "slides/02-apis.html#working-with-json",
    "title": "Data Acquisition from APIs",
    "section": "Working with JSON",
    "text": "Working with JSON\nimport json\n\n# Parse JSON response\ndata = response.json()\n\n# Access nested data\nuser_name = data['user']['name']\n\n# Save to file\nwith open('data.json', 'w') as f:\n    json.dump(data, f, indent=2)"
  },
  {
    "objectID": "slides/02-apis.html#real-world-example-weather-data",
    "href": "slides/02-apis.html#real-world-example-weather-data",
    "title": "Data Acquisition from APIs",
    "section": "Real-world Example: Weather Data",
    "text": "Real-world Example: Weather Data\n\nOpen-Meteo Historical API (Brighton Ski Resort)\nBrighton Mountain Resort coordinates: 40.5981¬∞ N, 111.5831¬∞ W\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Calculate date range (last 365 days)\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365)\n\n# API endpoint\nurl = \"https://archive-api.open-meteo.com/v1/archive\"\n\nparams = {\n    'latitude': 40.5981,\n    'longitude': -111.5831,\n    'start_date': start_date.strftime('%Y-%m-%d'),\n    'end_date': end_date.strftime('%Y-%m-%d'),\n    'daily': 'precipitation_sum,snowfall_sum',\n    'timezone': 'America/Denver'\n}\n\nresponse = requests.get(url, params=params)\ndata = response.json()\n\n# Convert to DataFrame\ndf = pd.DataFrame({\n    'date': data['daily']['time'],\n    'precipitation_mm': data['daily']['precipitation_sum'],\n    'snowfall_cm': data['daily']['snowfall_sum']\n})\n\nprint(f\"Total precipitation: {df['precipitation_mm'].sum():.1f} mm\")\nprint(f\"Total snowfall: {df['snowfall_cm'].sum():.1f} cm\")"
  },
  {
    "objectID": "slides/02-apis.html#hands-on-exercise",
    "href": "slides/02-apis.html#hands-on-exercise",
    "title": "Data Acquisition from APIs",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\nBuild a script that fetches historical weather data:\n\nUse Open-Meteo Archive API for a location of your choice\nFetch daily precipitation/snowfall for the past year\nHandle errors with retries and timeouts\nConvert to a pandas DataFrame\nCalculate summary statistics (total, mean, max)\nSave results to CSV\n\nBonus: Compare multiple ski resorts or visualize seasonal patterns\nCoordinates for Utah resorts: - Brighton: 40.5981, -111.5831 - Alta: 40.5885, -111.6381 - Snowbird: 40.5803, -111.6573"
  },
  {
    "objectID": "slides/02-apis.html#common-pitfalls",
    "href": "slides/02-apis.html#common-pitfalls",
    "title": "Data Acquisition from APIs",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nNot handling rate limits\nIgnoring HTTP status codes\nNot using timeouts\nHardcoding credentials\nNot validating responses\nInfinite loops in pagination"
  },
  {
    "objectID": "slides/02-apis.html#questions",
    "href": "slides/02-apis.html#questions",
    "title": "Data Acquisition from APIs",
    "section": "Questions?",
    "text": "Questions?\nNext session: Database Queries"
  },
  {
    "objectID": "slides/02-apis.html#resources",
    "href": "slides/02-apis.html#resources",
    "title": "Data Acquisition from APIs",
    "section": "Resources",
    "text": "Resources\n\nREST API Tutorial\nHTTP Status Codes\nRequests Documentation\nhttr2 Documentation"
  },
  {
    "objectID": "slides/04-data-cleaning.html",
    "href": "slides/04-data-cleaning.html",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "",
    "text": "This sessions‚Äôs topics:\n\nRecap: Building blocks of data pipelines\nData quality and cleaning strategies\nIntegrating API ‚Üí Database ‚Üí Clean Data\nBuilding reusable, idempotent pipelines\nError handling and monitoring\nDocumentation and deployment"
  },
  {
    "objectID": "slides/04-data-cleaning.html#overview",
    "href": "slides/04-data-cleaning.html#overview",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "",
    "text": "This sessions‚Äôs topics:\n\nRecap: Building blocks of data pipelines\nData quality and cleaning strategies\nIntegrating API ‚Üí Database ‚Üí Clean Data\nBuilding reusable, idempotent pipelines\nError handling and monitoring\nDocumentation and deployment"
  },
  {
    "objectID": "slides/04-data-cleaning.html#workshop-recap",
    "href": "slides/04-data-cleaning.html#workshop-recap",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Workshop Recap",
    "text": "Workshop Recap\nWhat we‚Äôve covered:\n\nSession 1: Python/R fundamentals, environment setup\nSession 2: API data acquisition (REST, GraphQL, authentication)\nSession 3: Database operations (SQL, connections, upserts)\nSession 4: Bringing it all together"
  },
  {
    "objectID": "slides/04-data-cleaning.html#the-complete-pipeline",
    "href": "slides/04-data-cleaning.html#the-complete-pipeline",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "The Complete Pipeline",
    "text": "The Complete Pipeline\n\n\n\n\n\nflowchart LR\n    A[API Source] --&gt;|Fetch| B[Validate]\n    B --&gt;|Clean| C[Transform]\n    C --&gt; CB[Combine]\n\n    D[(Database)] --&gt;|Fetch| V2[Validate]\n    V2 --&gt;|Clean| C2[Transform]\n    C2 --&gt; CB[Combine]\n    \n    CB --&gt;|Upsert| W[(Warehouse)]\n    W --&gt;|Export| P[Python Analysis]\n    W --&gt;|Export| R[R Analysis]\n    W --&gt;|Export| PBI[Power BI]\n    \n    B  -.-&gt;|Errors| F[Log]\n    V2 -.-&gt;|Errors| F[Log]\n    C  -.-&gt;|Errors| F\n    C2 -.-&gt;|Errors| F\n    F  --&gt;|Alert| G[Monitor]\n    \n    style A fill:#e1f5ff\n    style D fill:#fff4e1\n    style CB fill:#f3e5f5\n    style P fill:#e8f5e9\n    style R fill:#e8f5e9\n    style PBI fill:#e8f5e9\n    style F fill:#ffebee"
  },
  {
    "objectID": "slides/04-data-cleaning.html#why-data-cleaning",
    "href": "slides/04-data-cleaning.html#why-data-cleaning",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Why Data Cleaning?",
    "text": "Why Data Cleaning?\n\n‚ÄúData scientists spend 80% of their time cleaning data‚Äù\n\nReal data is messy:\n\nMissing values (NULL, empty strings, placeholders)\nInconsistent formats (dates, phone numbers, addresses)\nDuplicates (same record multiple times)\nOutliers (extreme or impossible values)\nType mismatches (numbers as strings)\nEncoding issues (character sets)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#data-quality-in-context",
    "href": "slides/04-data-cleaning.html#data-quality-in-context",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Data Quality in Context",
    "text": "Data Quality in Context\nPipeline perspective:\n\n\nR:\n# What we've learned:\n# 1. Get data from API\nresponse &lt;- GET(api_url, \n  add_headers(Authorization = token))\ndata &lt;- content(response)\n\n# 2. Store in database\ndbWriteTable(con, \"raw_table\", df, \n  append = TRUE)\n\n# 3. NOW: Clean before/after storage?\n# Answer: Both! Validate early, \n# clean for analysis\n\nPython:\n# What we've learned:\n# 1. Get data from API\nresponse = requests.get(api_url, \n  headers=headers)\ndata = response.json()\n\n# 2. Store in database\ndf.to_sql('raw_table', engine, \n  if_exists='append')\n\n# 3. NOW: Clean before/after storage?\n# Answer: Both! Validate early, \n# clean for analysis"
  },
  {
    "objectID": "slides/04-data-cleaning.html#common-data-quality-issues",
    "href": "slides/04-data-cleaning.html#common-data-quality-issues",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Common Data Quality Issues",
    "text": "Common Data Quality Issues\n\nMissing data: NULL insurance_id, empty provider names\nDuplicates: Same patient_id appearing twice\nInconsistency: Date formats (‚Äú2024-01-15‚Äù vs ‚Äú01/15/2024‚Äù)\nOutliers: visit_date in year 2099, age = -5\nType mismatches: patient_id stored as string ‚Äú001‚Äù\nInvalid data: ICD codes that don‚Äôt exist\n\nRemember: These issues affect both API data AND database queries!"
  },
  {
    "objectID": "slides/04-data-cleaning.html#logging-tools",
    "href": "slides/04-data-cleaning.html#logging-tools",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Logging Tools",
    "text": "Logging Tools\nEssential packages for pipeline logging:\n\n\nR: logger\n\nMultiple log levels (DEBUG, INFO, WARN, ERROR)\nFlexible log formatting (glue syntax)\nMultiple appenders (console, file, syslog)\nEasy to configure\n\n\nPython: logging\n\nBuilt-in standard library\nMultiple log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)\nHandlers for console, file, HTTP, etc.\nIndustry-standard logging framework"
  },
  {
    "objectID": "slides/04-data-cleaning.html#logging-tools-installation-setup",
    "href": "slides/04-data-cleaning.html#logging-tools-installation-setup",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Logging Tools: Installation & Setup",
    "text": "Logging Tools: Installation & Setup\n\n\nR:\n# Install\ninstall.packages(\"logger\")\n\n# Load\nlibrary(logger)\n\n# Basic usage\nlog_info(\"Processing {n} records\")\nlog_error(\"Failed: {error_msg}\")\n\nPython:\n# Import (built-in, no install needed)\nimport logging\n\n# Configure\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Basic usage\nlogger.info(f\"Processing {n} records\")\nlogger.error(f\"Failed: {error_msg}\")"
  },
  {
    "objectID": "slides/04-data-cleaning.html#why-logging-is-critical",
    "href": "slides/04-data-cleaning.html#why-logging-is-critical",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Why Logging is Critical",
    "text": "Why Logging is Critical\nDocumentation and accountability:\n\nTrack transformations: Record what data was cleaned and why\nDebug failures: Understand exactly where and why pipelines break\nMonitor quality: Track data quality metrics over time\nAudit trails: Create compliance documentation for regulatory requirements\nTeam communication: Document decisions for future maintainers\n\nBest practice: Log before and after each transformation step"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-first-thinking",
    "href": "slides/04-data-cleaning.html#pipeline-first-thinking",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline-First Thinking",
    "text": "Pipeline-First Thinking\nWhere to clean data?\n\n\nOption 1: Clean at API\n# Validate immediately\ndata &lt;- fetch_api()\nif (validate(data)) {\n  store_in_db(data)\n} else {\n  log_error(data)\n}\nPros: Bad data never enters system\nCons: May lose raw data for debugging\n\nOption 2: Store Raw, Clean Later\n# Store everything\nstore_raw(api_data)\n\n# Clean for analysis\nclean_data &lt;- query_raw_data() %&gt;%\n  transform()\nPros: Keep raw data, flexible cleaning\nCons: Storage costs, duplicate effort\n\n\nBest practice: Both! Validate early, keep raw, clean for use"
  },
  {
    "objectID": "slides/04-data-cleaning.html#validation-at-api-layer",
    "href": "slides/04-data-cleaning.html#validation-at-api-layer",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Validation at API Layer",
    "text": "Validation at API Layer\nRemember Session 2: Error handling at source\n\n\nR:\nfetch_and_validate_patients &lt;- function(\n  api_url, token) {\n  \n  tryCatch({\n    response &lt;- GET(\n      api_url,\n      add_headers(\n        Authorization = paste(\"Bearer\", token)\n      ),\n      timeout(30)\n    )\n    stop_for_status(response)\n    \n    data &lt;- content(response)\n    \n    # Immediate validation\n    validated &lt;- validate_patient_schema(data)\n    log_info(\"Fetched {nrow(validated)} records\")\n    \n    validated\n  }, error = function(e) {\n    log_error(\"API fetch failed: {e$message}\")\n    NULL\n  })\n}\n\nPython:\ndef fetch_and_validate_patients(\n  api_url, token):\n  \"\"\"Fetch patient data with validation\"\"\"\n  try:\n    response = requests.get(\n      api_url,\n      headers={'Authorization': f'Bearer {token}'},\n      timeout=30\n    )\n    response.raise_for_status()\n    \n    data = response.json()\n    \n    # Immediate validation\n    validated = validate_patient_schema(data)\n    logger.info(f\"Fetched {len(validated)} records\")\n    \n    return validated\n    \n  except requests.exceptions.RequestException as e:\n    logger.error(f\"API fetch failed: {e}\")\n    return None"
  },
  {
    "objectID": "slides/04-data-cleaning.html#data-validation-tools",
    "href": "slides/04-data-cleaning.html#data-validation-tools",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Data Validation Tools",
    "text": "Data Validation Tools\nEssential packages for schema validation:\n\n\nR: validate(book)\n\nDefine data validation rules\nConfront data with rules\nGet detailed validation reports\nExport results for documentation\n\n# Install\ninstall.packages(\"validate\")\n\n# Load\nlibrary(validate)\n\nPython: pandera(docs)\n\nType checking and validation\nStatistical hypothesis testing\nSchema inference from data\nIntegration with pandas\n\n# Install\npip install pandera\n\n# Import\nimport pandera as pa"
  },
  {
    "objectID": "slides/04-data-cleaning.html#data-validation-with-schema",
    "href": "slides/04-data-cleaning.html#data-validation-with-schema",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Data Validation with Schema",
    "text": "Data Validation with Schema\n\n\nR: validate package\nlibrary(validate)\nlibrary(dplyr)\n\n# Define validation rules\npatient_rules &lt;- validator(\n  patient_id &gt; 0,\n  !is.na(patient_id),\n  !is.na(name),\n  nchar(name) &gt; 0,\n  date_of_birth &lt;= Sys.Date(),\n  grepl(\"^[^@]+@[^@]+\\\\.[^@]+\", email) | \n    is.na(email)\n)\n\n# Validate and handle errors\nresults &lt;- confront(df, patient_rules)\n\nif (all(results)) {\n  dbWriteTable(con, \"patients\", df, \n    append = TRUE)\n} else {\n  errors &lt;- summary(results) %&gt;%\n    filter(!passes)\n  log_error(\"Validation failed: {nrow(errors)} rules\")\n  dbWriteTable(con, \"validation_errors\", \n    errors, append = TRUE)\n}\n\nPython: pandera\nimport pandera as pa\nfrom datetime import datetime\n\n# Define expected schema\npatient_schema = pa.DataFrameSchema({\n  \"patient_id\": pa.Column(\n    int, pa.Check.greater_than(0), \n    nullable=False),\n  \"name\": pa.Column(\n    str, pa.Check.str_length(min_value=1), \n    nullable=False),\n  \"date_of_birth\": pa.Column(\n    pa.DateTime, \n    pa.Check.less_than_or_equal_to(\n      datetime.now())),\n  \"insurance_id\": pa.Column(\n    str, nullable=True),\n  \"email\": pa.Column(\n    str, pa.Check.str_matches(\n      r'^[^@]+@[^@]+\\.[^@]+'), \n    nullable=True)\n})\n\n# Validate data from API\ntry:\n  validated_df = patient_schema.validate(\n    df, lazy=True)\n  validated_df.to_sql('patients', \n    engine, if_exists='append')\nexcept pa.errors.SchemaErrors as e:\n  logger.error(f\"Validation failed\")\n  e.failure_cases.to_sql(\n    'validation_errors', engine, \n    if_exists='append')"
  },
  {
    "objectID": "slides/04-data-cleaning.html#missing-data-strategies",
    "href": "slides/04-data-cleaning.html#missing-data-strategies",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Missing Data Strategies",
    "text": "Missing Data Strategies\nTypes of missingness:\n\nMCAR: Missing Completely At Random (safe to drop)\nMAR: Missing At Random (can impute)\nMNAR: Missing Not At Random (domain knowledge needed)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#mcar-missing-completely-at-random",
    "href": "slides/04-data-cleaning.html#mcar-missing-completely-at-random",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "MCAR: Missing Completely At Random",
    "text": "MCAR: Missing Completely At Random\nProperties:\n\nMissingness is independent of both observed and unobserved data\nNo systematic pattern to the missing values\nProbability of being missing is the same for all observations\n\nHow to identify:\n\nStatistical tests (Little‚Äôs MCAR test)\nVisual inspection shows no patterns\nMissing values distributed randomly across all groups\n\nSafe handling: Can drop rows or use any imputation method"
  },
  {
    "objectID": "slides/04-data-cleaning.html#mar-missing-at-random",
    "href": "slides/04-data-cleaning.html#mar-missing-at-random",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "MAR: Missing At Random",
    "text": "MAR: Missing At Random\nProperties:\n\nMissingness depends on observed data, not the missing value itself\nSystematic pattern related to other variables\nCan be predicted from other available information\n\nHow to identify:\n\nCompare missingness rates across groups\nCorrelation between missingness and other variables\nDifferent missing rates in different subgroups\n\nSafe handling: Imputation using observed data (mean, median, regression)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#mnar-missing-not-at-random",
    "href": "slides/04-data-cleaning.html#mnar-missing-not-at-random",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "MNAR: Missing Not At Random",
    "text": "MNAR: Missing Not At Random\nProperties:\n\nMissingness depends on the unobserved missing value itself\nSystematic relationship between missing value and its missingness\nCannot be predicted from observed data alone\n\nHow to identify:\n\nDomain knowledge and subject matter expertise\nMissing pattern makes logical sense given the nature of data\nCannot be explained by other observed variables\n\nSafe handling: Requires domain expertise, sensitivity analysis, or specialized models"
  },
  {
    "objectID": "slides/04-data-cleaning.html#healthcare-examples-of-missing-data-types",
    "href": "slides/04-data-cleaning.html#healthcare-examples-of-missing-data-types",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Healthcare Examples of Missing Data Types",
    "text": "Healthcare Examples of Missing Data Types\n\n\n\n\n\n\n\n\nMCAR\nMAR\nMNAR\n\n\n\n\nRandom equipment failures during data collection\nOlder patients more likely to skip income questions\nPatients with severe illness don‚Äôt report weight (too sick)\n\n\nLab results missing due to random computer glitches\nMen less likely to report weight than women\nVery high income earners refuse to disclose income\n\n\nSurvey responses lost due to random technical errors\nInsurance_id missing more often for certain visit types\nPatients with no insurance leave insurance_id blank\n\n\n\n\nAbnormal lab results not recorded (measurement failed)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#missing-data-strategies-1",
    "href": "slides/04-data-cleaning.html#missing-data-strategies-1",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Missing Data Strategies",
    "text": "Missing Data Strategies\n\n\nR:\nhandle_missing_patients &lt;- function(df) {\n  # Critical fields: Drop if missing\n  df &lt;- df %&gt;%\n    filter(!is.na(patient_id), \n           !is.na(name))\n  log_info(\"Kept {nrow(df)} rows\")\n  \n  # Optional fields: Keep but flag\n  df &lt;- df %&gt;%\n    mutate(\n      has_insurance = !is.na(insurance_id)\n    )\n  \n  # Numeric fields: Impute with care\n  median_age &lt;- median(df$age, \n    na.rm = TRUE)\n  df &lt;- df %&gt;%\n    mutate(\n      age_imputed = is.na(age),\n      age = if_else(is.na(age), \n        median_age, age)\n    )\n  \n  df\n}\n\nPython:\ndef handle_missing_patients(df):\n  \"\"\"Handle missing values\"\"\"\n  \n  # Critical fields: Drop if missing\n  df = df.dropna(\n    subset=['patient_id', 'name'])\n  logger.info(\n    f\"Kept {len(df)} rows\")\n  \n  # Optional fields: Keep but flag\n  df['has_insurance'] = \\\n    df['insurance_id'].notna()\n  \n  # Numeric fields: Impute with care\n  median_age = df['age'].median()\n  df['age_imputed'] = df['age'].isna()\n  df['age'] = df['age'].fillna(\n    median_age)\n  \n  return df"
  },
  {
    "objectID": "slides/04-data-cleaning.html#handling-duplicates-in-pipelines",
    "href": "slides/04-data-cleaning.html#handling-duplicates-in-pipelines",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Handling Duplicates in Pipelines",
    "text": "Handling Duplicates in Pipelines\nIdempotent pipelines prevent duplicates!\n\n\n\nR\n\n\nPython\n\n\n\n\nBAD: Creates duplicates\n\n\n\n\nbad_pipeline &lt;- function() {\n  data &lt;- fetch_api()\n  dbWriteTable(con, \"patients\", data, \n    append = TRUE)  # Duplicates!\n}\n\n\ndef bad_pipeline():\n  data = fetch_api()\n  df.to_sql('patients', engine, \n    if_exists='append')  # Duplicates!\n\n\n\n\nGOOD: Upsert prevents duplicates\n\n\n\n\ngood_pipeline &lt;- function() {\n  data &lt;- fetch_api()\n  \n  # Using rows_upsert from Session 3\n  patients_tbl &lt;- tbl(con, \"patients\")\n  rows_upsert(\n    patients_tbl, \n    data, \n    by = \"patient_id\", \n    in_place = TRUE\n  )\n}\n\n\ndef good_pipeline():\n  data = fetch_api()\n  \n  # Using ON CONFLICT\n  for row in data:\n    cursor.execute(\"\"\"\n      INSERT INTO patients \n        (patient_id, name, \n         date_of_birth, insurance_id)\n      VALUES (?, ?, ?, ?)\n      ON CONFLICT(patient_id) \n      DO UPDATE SET \n        name = excluded.name,\n        date_of_birth = excluded.date_of_birth,\n        insurance_id = excluded.insurance_id\n    \"\"\", (row['patient_id'], \n          row['name'], \n          row['dob'], \n          row['insurance']))\n  \n  conn.commit()"
  },
  {
    "objectID": "slides/04-data-cleaning.html#complete-pipeline-example",
    "href": "slides/04-data-cleaning.html#complete-pipeline-example",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Complete Pipeline Example",
    "text": "Complete Pipeline Example\nIntegrating Sessions 2 + 3 + 4:\n\n\nR:\nlibrary(httr)\nlibrary(dplyr)\nlibrary(DBI)\nlibrary(logger)\n\nhealthcare_pipeline &lt;- function(\n  api_url, api_token, db_conn) {\n  \n  log_info(\"Starting pipeline...\")\n  \n  tryCatch({\n    # 1. FETCH (Session 2: API)\n    response &lt;- GET(\n      api_url,\n      add_headers(\n        Authorization = paste(\"Bearer\", api_token)\n      ),\n      timeout(30)\n    )\n    stop_for_status(response)\n    data &lt;- content(response) %&gt;% \n      bind_rows()\n    log_info(\"Fetched {nrow(data)} records\")\n    \n    # 2. VALIDATE (Session 4)\n    data &lt;- validate_schema(data)\n    \n    # 3. CLEAN (Session 4)\n    data &lt;- clean_patient_data(data)\n    \n    # 4. STORE (Session 3: Upsert)\n    con &lt;- dbConnect(\n      odbc::odbc(), \n      .connection_string = db_conn\n    )\n    patients_tbl &lt;- tbl(con, \"patients\")\n    rows_upsert(patients_tbl, data, \n      by = \"patient_id\", \n      in_place = TRUE)\n    dbDisconnect(con)\n    \n    log_info(\"Pipeline completed\")\n    TRUE\n  }, error = function(e) {\n    log_error(\"Pipeline failed: {e$message}\")\n    FALSE\n  })\n}\n\nPython:\nimport requests\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport logging\n\ndef healthcare_pipeline(\n  api_url, api_token, db_conn):\n  \"\"\"Complete pipeline: \n  API ‚Üí Validate ‚Üí Clean ‚Üí DB\"\"\"\n  \n  logger = logging.getLogger(__name__)\n  engine = create_engine(db_conn)\n  \n  try:\n    # 1. FETCH (Session 2: API)\n    logger.info(\"Fetching from API...\")\n    response = requests.get(\n      api_url,\n      headers={\n        'Authorization': f'Bearer {api_token}'\n      },\n      timeout=30\n    )\n    response.raise_for_status()\n    data = response.json()\n    df = pd.DataFrame(data)\n    logger.info(f\"Fetched {len(df)} records\")\n    \n    # 2. VALIDATE (Session 4)\n    df = patient_schema.validate(df)\n    \n    # 3. CLEAN (Session 4)\n    df = clean_patient_data(df)\n    \n    # 4. STORE (Session 3: Upsert)\n    for _, row in df.iterrows():\n      upsert_patient(engine, row)\n    \n    logger.info(f\"Pipeline completed\")\n    return True\n    \n  except Exception as e:\n    logger.error(f\"Pipeline failed: {e}\")\n    return False"
  },
  {
    "objectID": "slides/04-data-cleaning.html#data-transformation-functions",
    "href": "slides/04-data-cleaning.html#data-transformation-functions",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Data Transformation Functions",
    "text": "Data Transformation Functions\nReusable cleaning functions:\n\n\nR:\nclean_patient_data &lt;- function(df) {\n  original_count &lt;- nrow(df)\n  \n  # Remove duplicates (by patient_id)\n  df &lt;- df %&gt;%\n    distinct(patient_id, .keep_all = TRUE)\n  log_info(\"Removed {original_count - nrow(df)} dupes\")\n  \n  # Standardize text fields\n  df &lt;- df %&gt;%\n    mutate(\n      name = str_to_title(str_trim(name)),\n      insurance_id = str_to_upper(\n        str_trim(insurance_id)\n      )\n    )\n  \n  # Convert dates\n  df &lt;- df %&gt;%\n    mutate(\n      date_of_birth = as.Date(date_of_birth)\n    )\n  \n  # Validate age (0-120)\n  df &lt;- df %&gt;%\n    mutate(\n      age = as.numeric(\n        difftime(Sys.Date(), \n                 date_of_birth, \n                 units = \"days\") / 365.25\n      )\n    ) %&gt;%\n    filter(age &gt;= 0, age &lt;= 120)\n  \n  # Handle missing insurance\n  df &lt;- df %&gt;%\n    mutate(\n      insurance_id = coalesce(\n        insurance_id, \"SELF_PAY\"\n      )\n    )\n  \n  df\n}\n\nPython:\ndef clean_patient_data(df):\n  \"\"\"Transform and standardize\"\"\"\n  df = df.copy()\n  \n  # Remove duplicates\n  original_count = len(df)\n  df = df.drop_duplicates(\n    subset=['patient_id'], \n    keep='last'\n  )\n  logger.info(\n    f\"Removed {original_count - len(df)} dupes\"\n  )\n  \n  # Standardize text\n  df['name'] = df['name'].str.strip()\\\n    .str.title()\n  df['insurance_id'] = df['insurance_id']\\\n    .str.upper().str.strip()\n  \n  # Convert dates\n  df['date_of_birth'] = pd.to_datetime(\n    df['date_of_birth'], \n    errors='coerce'\n  )\n  \n  # Validate age (0-120)\n  df['age'] = (\n    (pd.Timestamp.now() - \n     df['date_of_birth']).dt.days / 365.25\n  )\n  df = df[\n    (df['age'] &gt;= 0) & (df['age'] &lt;= 120)\n  ]\n  \n  # Handle missing insurance\n  df['insurance_id'] = df['insurance_id']\\\n    .fillna('SELF_PAY')\n  \n  return df"
  },
  {
    "objectID": "slides/04-data-cleaning.html#object-oriented-programming-tools",
    "href": "slides/04-data-cleaning.html#object-oriented-programming-tools",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Object-Oriented Programming Tools",
    "text": "Object-Oriented Programming Tools\nBuilding reusable class-based pipelines:\n\n\nR: R6\n\nObject-oriented programming for R\nReference semantics (mutable objects)\nInheritance and polymorphism\nSimilar to Python classes\n\n\nPython: Classes\n\nBuilt-in object-oriented programming\nInheritance and composition\nSpecial methods (dunder methods)\nFoundation for reusable code"
  },
  {
    "objectID": "slides/04-data-cleaning.html#oop-tools-code-examples",
    "href": "slides/04-data-cleaning.html#oop-tools-code-examples",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "OOP Tools: Code Examples",
    "text": "OOP Tools: Code Examples\n\n\nR:\n# Install\ninstall.packages(\"R6\")\n\n# Load\nlibrary(R6)\n\n# Create class\nMyClass &lt;- R6Class(\"MyClass\",\n  public = list(\n    initialize = function(x) {\n      self$x &lt;- x\n    },\n    print = function() {\n      print(self$x)\n    }\n  )\n)\n\n# Use class\nobj &lt;- MyClass$new(\"hello\")\nobj$print()\n\nPython:\n# Built-in, no install needed\n\n# Create class\nclass MyClass:\n    def __init__(self, x):\n        self.x = x\n    \n    def print(self):\n        print(self.x)\n\n# Use class\nobj = MyClass(\"hello\")\nobj.print()"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-class-architecture",
    "href": "slides/04-data-cleaning.html#pipeline-class-architecture",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline Class Architecture",
    "text": "Pipeline Class Architecture\nObject-oriented design pattern:\n\n\n\n\n\nclassDiagram\ndirection LR\n    class DataPipeline {\n        +config\n        +logger\n        +engine/con\n        +initialize(config)\n        +extract()\n        +validate(df)\n        +transform(df)\n        +load(df)\n        +run()\n    }\n    \n    class HealthcarePipeline {\n        +extract()\n        +validate(df)\n        +transform(df)\n        +load(df)\n    }\n    \n    DataPipeline &lt;|-- HealthcarePipeline : inherits\n    \n    note for DataPipeline \"Base class with framework logic\"\n    note for HealthcarePipeline \"Concrete implementation\""
  },
  {
    "objectID": "slides/04-data-cleaning.html#building-reusable-pipelines",
    "href": "slides/04-data-cleaning.html#building-reusable-pipelines",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Building Reusable Pipelines",
    "text": "Building Reusable Pipelines\nModular design for flexibility:\n\n\nR (R6 Class):\nlibrary(R6)\n\n#' DataPipeline Class\n#' \n#' @description Reusable pipeline framework\n#' @field config Configuration list\n#' @field logger Logger instance\n#' @field con Database connection\nDataPipeline &lt;- R6Class(\"DataPipeline\",\n  public = list(\n    config = NULL,\n    logger = NULL,\n    con = NULL,\n    \n    #' @description Initialize pipeline\n    #' @param config Configuration list\n    initialize = function(config) {\n      self$config &lt;- config\n      self$logger &lt;- log_appender(appender_file(\n        paste0(\"pipeline_\", Sys.Date(), \".log\")\n      ))\n      self$con &lt;- dbConnect(\n        odbc::odbc(),\n        .connection_string = config$db_conn\n      )\n    },\n    \n    #' @description Override: Fetch from source\n    extract = function() {\n      stop(\"Override: Fetch from source\")\n    },\n    \n    #' @description Override: Validate schema\n    #' @param df Data frame to validate\n    validate = function(df) {\n      stop(\"Override: Validate schema\")\n    },\n    \n    #' @description Override: Clean data\n    #' @param df Data frame to transform\n    transform = function(df) {\n      stop(\"Override: Clean data\")\n    },\n    \n    #' @description Override: Load to dest\n    #' @param df Data frame to load\n    load = function(df) {\n      stop(\"Override: Load to dest\")\n    },\n    \n    #' @description Execute full ETL\n    #' @return TRUE on success, FALSE on failure\n    run = function() {\n      tryCatch({\n        log_info(\"Starting pipeline...\")\n        \n        df &lt;- self$extract()\n        log_info(\"Extracted {nrow(df)} records\")\n        \n        df &lt;- self$validate(df)\n        log_info(\"Validated {nrow(df)} records\")\n        \n        df &lt;- self$transform(df)\n        log_info(\"Transformed {nrow(df)} records\")\n        \n        self$load(df)\n        log_info(\"Pipeline completed\")\n        \n        TRUE\n      }, error = function(e) {\n        log_error(\"Pipeline failed: {e$message}\")\n        FALSE\n      })\n    }\n  )\n)\n\nPython (Class):\nclass DataPipeline:\n  \"\"\"Reusable pipeline framework\"\"\"\n  \n  def __init__(self, config):\n    self.config = config\n    self.logger = logging.getLogger(\n      self.__class__.__name__\n    )\n    self.engine = create_engine(\n      config['db_connection']\n    )\n  \n  def extract(self):\n    \"\"\"Override: Fetch data\"\"\"\n    raise NotImplementedError\n  \n  def validate(self, df):\n    \"\"\"Override: Validate schema\"\"\"\n    raise NotImplementedError\n  \n  def transform(self, df):\n    \"\"\"Override: Clean data\"\"\"\n    raise NotImplementedError\n  \n  def load(self, df):\n    \"\"\"Override: Load to dest\"\"\"\n    raise NotImplementedError\n  \n  def run(self):\n    \"\"\"Execute full ETL\"\"\"\n    try:\n      self.logger.info(\"Starting...\")\n      \n      df = self.extract()\n      self.logger.info(\n        f\"Extracted {len(df)} records\"\n      )\n      \n      df = self.validate(df)\n      self.logger.info(\n        f\"Validated {len(df)} records\"\n      )\n      \n      df = self.transform(df)\n      self.logger.info(\n        f\"Transformed {len(df)} records\"\n      )\n      \n      self.load(df)\n      self.logger.info(\"Completed\")\n      \n      return True\n    except Exception as e:\n      self.logger.error(\n        f\"Pipeline failed: {e}\"\n      )\n      return False"
  },
  {
    "objectID": "slides/04-data-cleaning.html#implementing-healthcare-pipeline",
    "href": "slides/04-data-cleaning.html#implementing-healthcare-pipeline",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Implementing Healthcare Pipeline",
    "text": "Implementing Healthcare Pipeline\nConcrete implementation:\n\n\nR:\nHealthcarePipeline &lt;- R6Class(\n  \"HealthcarePipeline\",\n  inherit = DataPipeline,\n  \n  public = list(\n    extract = function() {\n      # Fetch from API (Session 2)\n      response &lt;- GET(\n        self$config$api_url,\n        add_headers(\n          Authorization = paste(\n            \"Bearer\", \n            self$config$api_token\n          )\n        ),\n        timeout(30)\n      )\n      stop_for_status(response)\n      content(response) %&gt;% bind_rows()\n    },\n    \n    validate = function(df) {\n      # Validate schema (Session 4)\n      validate_patient_schema(df)\n    },\n    \n    transform = function(df) {\n      # Clean data (Session 4)\n      clean_patient_data(df)\n    },\n    \n    load = function(df) {\n      # Upsert to DB (Session 3)\n      patients_tbl &lt;- tbl(\n        self$con, \"patients\"\n      )\n      rows_upsert(\n        patients_tbl, \n        df, \n        by = \"patient_id\", \n        in_place = TRUE\n      )\n    }\n  )\n)\n\n# Use the pipeline\nconfig &lt;- list(\n  api_url = \"https://api.example.com/patients\",\n  api_token = Sys.getenv(\"API_TOKEN\"),\n  db_conn = Sys.getenv(\"DB_CONNECTION\")\n)\n\npipeline &lt;- HealthcarePipeline$new(config)\nsuccess &lt;- pipeline$run()\n\nPython:\nclass HealthcarePipeline(DataPipeline):\n  \"\"\"Specific pipeline for healthcare\"\"\"\n  \n  def extract(self):\n    \"\"\"Fetch from API (Session 2)\"\"\"\n    response = requests.get(\n      self.config['api_url'],\n      headers={\n        'Authorization': \n          f\"Bearer {self.config['api_token']}\"\n      },\n      timeout=30\n    )\n    response.raise_for_status()\n    return pd.DataFrame(response.json())\n  \n  def validate(self, df):\n    \"\"\"Validate schema (Session 4)\"\"\"\n    return patient_schema.validate(df)\n  \n  def transform(self, df):\n    \"\"\"Clean data (Session 4)\"\"\"\n    return clean_patient_data(df)\n  \n  def load(self, df):\n    \"\"\"Upsert to DB (Session 3)\"\"\"\n    for _, row in df.iterrows():\n      self.engine.execute(\"\"\"\n        INSERT INTO patients \n          (patient_id, name, \n           date_of_birth, insurance_id)\n        VALUES (?, ?, ?, ?)\n        ON CONFLICT(patient_id) \n        DO UPDATE SET \n          name=excluded.name,\n          date_of_birth=excluded.date_of_birth,\n          insurance_id=excluded.insurance_id\n      \"\"\", (row['patient_id'], \n            row['name'], \n            row['date_of_birth'], \n            row['insurance_id']))\n\n# Use the pipeline\nconfig = {\n  'api_url': 'https://api.example.com/patients',\n  'api_token': os.getenv('API_TOKEN'),\n  'db_connection': os.getenv('DB_CONNECTION')\n}\n\npipeline = HealthcarePipeline(config)\nsuccess = pipeline.run()"
  },
  {
    "objectID": "slides/04-data-cleaning.html#error-handling-retry-logic",
    "href": "slides/04-data-cleaning.html#error-handling-retry-logic",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Error Handling & Retry Logic",
    "text": "Error Handling & Retry Logic\nRobust pipelines handle failures gracefully:\n\n\nR:\n# Retry with exponential backoff\nretry_with_backoff &lt;- function(\n  func, max_retries = 3, delay = 5) {\n  \n  for (attempt in 1:max_retries) {\n    result &lt;- tryCatch({\n      func()\n    }, error = function(e) {\n      if (attempt == max_retries) {\n        stop(e)\n      }\n      log_warn(\n        \"Attempt {attempt} failed: {e$message}. \n         Retrying in {delay}s...\"\n      )\n      Sys.sleep(delay)\n      NULL\n    })\n    \n    if (!is.null(result)) {\n      return(result)\n    }\n  }\n}\n\nRobustHealthcarePipeline &lt;- R6Class(\n  \"RobustHealthcarePipeline\",\n  inherit = HealthcarePipeline,\n  \n  public = list(\n    extract = function() {\n      # Fetch with retry\n      retry_with_backoff(\n        function() super$extract(),\n        max_retries = 3,\n        delay = 10\n      )\n    },\n    \n    run = function() {\n      tryCatch({\n        super$run()\n      }, validation_error = function(e) {\n        log_error(\"Validation failed: {e}\")\n        self$save_errors(e)\n        FALSE\n      }, error = function(e) {\n        log_critical(\n          \"Unexpected error: {e$message}\"\n        )\n        self$send_alert(e$message)\n        FALSE\n      })\n    }\n  )\n)\n\nPython:\nimport time\nfrom functools import wraps\n\ndef retry_on_failure(\n  max_retries=3, delay=5):\n  \"\"\"Decorator for retry logic\"\"\"\n  def decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n      for attempt in range(max_retries):\n        try:\n          return func(*args, **kwargs)\n        except requests.exceptions\\\n          .RequestException as e:\n          if attempt == max_retries - 1:\n            raise\n          logger.warning(\n            f\"Attempt {attempt + 1} failed: \n             {e}. Retrying in {delay}s...\"\n          )\n          time.sleep(delay)\n    return wrapper\n  return decorator\n\nclass RobustHealthcarePipeline(\n  HealthcarePipeline):\n  \"\"\"Pipeline with error handling\"\"\"\n  \n  @retry_on_failure(\n    max_retries=3, delay=10)\n  def extract(self):\n    \"\"\"Fetch with automatic retry\"\"\"\n    return super().extract()\n  \n  def run(self):\n    \"\"\"Run with error handling\"\"\"\n    try:\n      return super().run()\n    except pa.errors.SchemaErrors as e:\n      # Validation errors\n      logger.error(\n        f\"Data validation failed: {e}\"\n      )\n      self.save_errors(e.failure_cases)\n      return False\n    except Exception as e:\n      # Unexpected errors\n      logger.critical(\n        f\"Unexpected error: {e}\", \n        exc_info=True\n      )\n      self.send_alert(str(e))\n      return False"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-class-architecture-updated",
    "href": "slides/04-data-cleaning.html#pipeline-class-architecture-updated",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline Class Architecture (Updated)",
    "text": "Pipeline Class Architecture (Updated)\nObject-oriented design pattern:\n\n\n\n\n\nclassDiagram\ndirection LR\n    class DataPipeline {\n        +config\n        +logger\n        +engine/con\n        +initialize(config)\n        +extract()\n        +validate(df)\n        +transform(df)\n        +load(df)\n        +run()\n    }\n    \n    class HealthcarePipeline {\n        +extract()\n        +validate(df)\n        +transform(df)\n        +load(df)\n    }\n\n    class RobustHealthcarePipeline {\n        +extract()\n        +run()\n    }\n    \n    DataPipeline &lt;|-- HealthcarePipeline : inherits\n    HealthcarePipeline &lt;|-- RobustHealthcarePipeline : inherits\n    \n    note for DataPipeline \"Base class with framework logic\"\n    note for HealthcarePipeline \"Concrete implementation\"\n    note for RobustHealthcarePipeline \"Added retry & error handling\""
  },
  {
    "objectID": "slides/04-data-cleaning.html#monitoring-logging",
    "href": "slides/04-data-cleaning.html#monitoring-logging",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Monitoring & Logging",
    "text": "Monitoring & Logging\nTrack pipeline performance:\n\n\nR:\nlibrary(logger)\n\n# Configure structured logging\nlog_threshold(INFO)\nlog_appender(appender_tee(\n  appender_file(\n    paste0(\"pipeline_\", Sys.Date(), \".log\")\n  )\n))\nlog_formatter(formatter_glue)\n\nMonitoredPipeline &lt;- R6Class(\n  \"MonitoredPipeline\",\n  inherit = RobustHealthcarePipeline,\n  \n  public = list(\n    metrics = NULL,\n    \n    initialize = function(config) {\n      super$initialize(config)\n      self$metrics &lt;- list(\n        start_time = NULL,\n        end_time = NULL,\n        records_extracted = 0,\n        records_validated = 0,\n        records_loaded = 0,\n        errors = list()\n      )\n    },\n    \n    run = function() {\n      self$metrics$start_time &lt;- Sys.time()\n      \n      success &lt;- super$run()\n      \n      self$metrics$end_time &lt;- Sys.time()\n      self$metrics$duration &lt;- \n        as.numeric(\n          difftime(\n            self$metrics$end_time,\n            self$metrics$start_time,\n            units = \"secs\"\n          )\n        )\n      \n      # Log metrics\n      log_info(\"Pipeline metrics: \n        {self$metrics}\")\n      \n      # Store metrics in DB\n      self$save_metrics()\n      \n      success\n    }\n  )\n)\n\nPython:\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogging.basicConfig(\n  level=logging.INFO,\n  format=\n    '%(asctime)s - %(name)s - \n     %(levelname)s - %(message)s',\n  handlers=[\n    logging.FileHandler(\n      f'pipeline_{datetime.now():%Y%m%d}.log'\n    ),\n    logging.StreamHandler()\n  ]\n)\n\nclass MonitoredPipeline(\n  RobustHealthcarePipeline):\n  \"\"\"Pipeline with monitoring metrics\"\"\"\n  \n  def __init__(self, config):\n    super().__init__(config)\n    self.metrics = {\n      'start_time': None,\n      'end_time': None,\n      'records_extracted': 0,\n      'records_validated': 0,\n      'records_loaded': 0,\n      'errors': []\n    }\n  \n  def run(self):\n    \"\"\"Run with metrics tracking\"\"\"\n    self.metrics['start_time'] = \\\n      datetime.now()\n    \n    success = super().run()\n    \n    self.metrics['end_time'] = \\\n      datetime.now()\n    self.metrics['duration'] = (\n      self.metrics['end_time'] - \n      self.metrics['start_time']\n    ).total_seconds()\n    \n    # Log metrics\n    self.logger.info(\n      f\"Pipeline metrics: {self.metrics}\"\n    )\n    \n    # Store metrics in database\n    self.save_metrics()\n    \n    return success"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-class-architecture-final",
    "href": "slides/04-data-cleaning.html#pipeline-class-architecture-final",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline Class Architecture (Final)",
    "text": "Pipeline Class Architecture (Final)\nComplete class hierarchy with monitoring:\n\n\n\n\n\nclassDiagram\ndirection LR\n    class DataPipeline {\n        +config\n        +logger\n        +engine/con\n        +initialize(config)\n        +extract()\n        +validate(df)\n        +transform(df)\n        +load(df)\n        +run()\n    }\n    \n    class HealthcarePipeline {\n        +extract()\n        +validate(df)\n        +transform(df)\n        +load(df)\n    }\n\n    class RobustHealthcarePipeline {\n        +extract()\n        +run()\n    }\n    \n    class MonitoredPipeline {\n        +metrics\n        +initialize(config)\n        +run()\n    }\n    \n    DataPipeline &lt;|-- HealthcarePipeline : inherits\n    HealthcarePipeline &lt;|-- RobustHealthcarePipeline : inherits\n    RobustHealthcarePipeline &lt;|-- MonitoredPipeline : inherits\n    \n    note for DataPipeline \"Base framework\"\n    note for HealthcarePipeline \"Domain logic\"\n    note for RobustHealthcarePipeline \"Error handling\"\n    note for MonitoredPipeline \"Metrics tracking\""
  },
  {
    "objectID": "slides/04-data-cleaning.html#configuration-management",
    "href": "slides/04-data-cleaning.html#configuration-management",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Configuration Management",
    "text": "Configuration Management\nEnvironment-based configuration with unified config file:\nconfig.yaml (works for both R and Python):\ndevelopment:\n  api_url: \"https://api-dev.example.com\"\n  db_connection: \"sqlite:///dev.db\"\n  log_level: \"DEBUG\"\n  retry_attempts: 3\n  timeout_seconds: 30\n  \nproduction:\n  api_url: \"https://api.example.com\"\n  db_connection: \"postgresql://prod/db\"  \n  log_level: \"INFO\"\n  retry_attempts: 5\n  timeout_seconds: 60\n\nstaging:\n  api_url: \"https://api-staging.example.com\"\n  db_connection: \"postgresql://staging/db\"\n  log_level: \"INFO\"\n  retry_attempts: 3\n  timeout_seconds: 45\nKey principles: - Environment variables override config file values - Sensitive data (tokens, passwords) stored in environment variables only - Same config structure works for both languages"
  },
  {
    "objectID": "slides/04-data-cleaning.html#configuration-management-implementation",
    "href": "slides/04-data-cleaning.html#configuration-management-implementation",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Configuration Management: Implementation",
    "text": "Configuration Management: Implementation\nLanguage-specific implementations:\n\n\nR:\nlibrary(config)\nlibrary(yaml)\n\n# Option 1: Using config package\nload_config &lt;- function(env = \"development\") {\n  Sys.setenv(R_CONFIG_ACTIVE = env)\n  cfg &lt;- config::get(file = \"config.yaml\")\n  \n  # Override with environment variables\n  cfg$api_token &lt;- Sys.getenv(\"API_TOKEN\")\n  cfg$db_password &lt;- Sys.getenv(\"DB_PASSWORD\")\n  \n  cfg\n}\n\n# Option 2: Using yaml package\nload_config_yaml &lt;- function(env = \"development\") {\n  cfg &lt;- yaml::read_yaml(\"config.yaml\")[[env]]\n  \n  # Override with environment variables  \n  cfg$api_token &lt;- Sys.getenv(\"API_TOKEN\")\n  cfg$db_password &lt;- Sys.getenv(\"DB_PASSWORD\")\n  \n  cfg\n}\n\n# Use in pipeline\nenv &lt;- Sys.getenv(\"ENVIRONMENT\", \"development\")\nconfig &lt;- load_config(env)\npipeline &lt;- MonitoredPipeline$new(config)\n\nPython:\nimport yaml\nimport os\n\ndef load_config(env='development'):\n  \"\"\"Load environment-specific config\"\"\"\n  with open('config.yaml') as f:\n    all_configs = yaml.safe_load(f)\n  \n  if env not in all_configs:\n    raise ValueError(f\"Environment '{env}' not found\")\n  \n  config = all_configs[env].copy()\n  \n  # Override with environment variables\n  config['api_token'] = os.getenv('API_TOKEN')\n  config['db_password'] = os.getenv('DB_PASSWORD')\n  \n  # Validate required settings\n  required = ['api_url', 'db_connection']\n  missing = [k for k in required if not config.get(k)]\n  if missing:\n    raise ValueError(f\"Missing config: {missing}\")\n  \n  return config\n\n# Use in pipeline\nenv = os.getenv('ENVIRONMENT', 'development')\nconfig = load_config(env)\npipeline = MonitoredPipeline(config)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-execution-patterns",
    "href": "slides/04-data-cleaning.html#pipeline-execution-patterns",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline Execution Patterns",
    "text": "Pipeline Execution Patterns\nChoose the right execution model for your use case:\n\nüîÑ On-Demand (Manual)\n\nTrigger when needed (button click, API call)\nBest for: Ad-hoc analysis, debugging, one-time migrations\nPros: Full control, immediate feedback\nCons: Requires manual intervention"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-execution-patterns-1",
    "href": "slides/04-data-cleaning.html#pipeline-execution-patterns-1",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline Execution Patterns",
    "text": "Pipeline Execution Patterns\nChoose the right execution model for your use case:\n\nüîÑ On-Demand (Manual)\n\n\nüì¶ Batch Processing\n\nScheduled runs at regular intervals\n\nBest for: Daily reports, periodic ETL, data warehousing\nPros: Predictable, efficient for large datasets\nCons: Delayed processing, fixed schedules"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-execution-patterns-2",
    "href": "slides/04-data-cleaning.html#pipeline-execution-patterns-2",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline Execution Patterns",
    "text": "Pipeline Execution Patterns\nChoose the right execution model for your use case:\n\nüîÑ On-Demand (Manual)\n\n\nüì¶ Batch Processing\n\n\n‚ö° Event-Triggered (Real-time)\n\nReact to external events (file upload, webhook, queue message)\nBest for: Real-time alerts, streaming data, API integrations\nPros: Immediate response, efficient resource usage\nCons: Complex error handling, potential cascading failures"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-considerations-warnings",
    "href": "slides/04-data-cleaning.html#scheduling-considerations-warnings",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Considerations & Warnings",
    "text": "Scheduling Considerations & Warnings\n‚ö†Ô∏è Critical considerations before automating:\n\nResource Management\n\nCPU/Memory: Pipelines can consume significant resources\nDatabase Load: Batch operations can impact production systems\nNetwork: API calls may hit rate limits during peak hours"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-considerations-warnings-1",
    "href": "slides/04-data-cleaning.html#scheduling-considerations-warnings-1",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Considerations & Warnings",
    "text": "Scheduling Considerations & Warnings\n‚ö†Ô∏è Critical considerations before automating:\n\nResource Management\n\n\nFailure Scenarios\n\nSilent Failures: Scheduled jobs may fail without notification\nInfrastructure: Network outages, server downtime, disk space\nTiming: Computer hibernation, power failures, user logged out\nData Corruption: Bad data can propagate through systems\nCascade Effects: Failed pipeline can break downstream processes"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-considerations-warnings-2",
    "href": "slides/04-data-cleaning.html#scheduling-considerations-warnings-2",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Considerations & Warnings",
    "text": "Scheduling Considerations & Warnings\n‚ö†Ô∏è Critical considerations before automating:\n\nResource Management\n\n\nFailure Scenarios\n\n\nOperational Complexity\n\nMonitoring: Need alerts, logs, and health checks\nDebugging: Harder to troubleshoot scheduled failures\nMaintenance: Updates require coordination with schedules"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-options-tools",
    "href": "slides/04-data-cleaning.html#scheduling-options-tools",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Options & Tools",
    "text": "Scheduling Options & Tools\nPlatform-specific scheduling tools:\n\nWindows\n\nTask Scheduler: Built-in, GUI-based, reliable\ntaskscheduleR: R package for Windows scheduling\nPowerShell: Script-based automation"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-options-tools-1",
    "href": "slides/04-data-cleaning.html#scheduling-options-tools-1",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Options & Tools",
    "text": "Scheduling Options & Tools\nPlatform-specific scheduling tools:\n\nWindows\n\n\nLinux/Mac\n\ncron: Standard Unix scheduler, text-based configuration\ncronR: R package wrapper for cron\nsystemd timers: Modern alternative to cron"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-options-tools-2",
    "href": "slides/04-data-cleaning.html#scheduling-options-tools-2",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Options & Tools",
    "text": "Scheduling Options & Tools\nPlatform-specific scheduling tools:\n\nWindows\n\n\nLinux/Mac\n\n\nCloud/Modern\n\nGitHub Actions: CI/CD with scheduled workflows\nAzure Logic Apps / AWS Lambda: Serverless scheduling\nApache Airflow: Enterprise workflow orchestration\nAPScheduler (Python): In-process scheduling"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-best-practices",
    "href": "slides/04-data-cleaning.html#scheduling-best-practices",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Best Practices",
    "text": "Scheduling Best Practices\nDesign principles for reliable automation:\n\nStart Simple\n\nBegin with manual execution, then automate\nTest thoroughly before scheduling\nUse development environment first"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-best-practices-1",
    "href": "slides/04-data-cleaning.html#scheduling-best-practices-1",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Best Practices",
    "text": "Scheduling Best Practices\nDesign principles for reliable automation:\n\nStart Simple\n\n\nIdempotency is Critical\n\nPipeline should produce same result if run multiple times\nUse upserts instead of inserts\nHandle partial failures gracefully"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-best-practices-2",
    "href": "slides/04-data-cleaning.html#scheduling-best-practices-2",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Best Practices",
    "text": "Scheduling Best Practices\nDesign principles for reliable automation:\n\nStart Simple\n\n\nIdempotency is Critical\n\n\nMonitoring & Alerting\n\nLog all operations with timestamps\nSet up failure notifications (email, Slack, Teams)\nTrack execution metrics and performance"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-best-practices-3",
    "href": "slides/04-data-cleaning.html#scheduling-best-practices-3",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling Best Practices",
    "text": "Scheduling Best Practices\nDesign principles for reliable automation:\n\nStart Simple\n\n\nIdempotency is Critical\n\n\nMonitoring & Alerting\n\n\nResource Isolation\n\nRun during off-peak hours when possible\nUse separate environments for scheduled jobs\nImplement connection pooling and timeouts"
  },
  {
    "objectID": "slides/04-data-cleaning.html#scheduling-automation-overview",
    "href": "slides/04-data-cleaning.html#scheduling-automation-overview",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Scheduling & Automation Overview",
    "text": "Scheduling & Automation Overview\nThree main approaches for running pipelines:\n\nPlatform-specific tools (Windows Task Scheduler, cron)\nLanguage-specific libraries (APScheduler, taskscheduleR, cronR)\nCloud/Enterprise solutions (GitHub Actions, Airflow)\n\nKey considerations:\n\nTiming: When should the pipeline run?\nDependencies: What needs to be available?\nMonitoring: How do you know if it worked?\nRecovery: What happens when it fails?"
  },
  {
    "objectID": "slides/04-data-cleaning.html#r-scheduling-solutions",
    "href": "slides/04-data-cleaning.html#r-scheduling-solutions",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "R Scheduling Solutions",
    "text": "R Scheduling Solutions\nMultiple options for different environments:\n\nWindows (taskscheduleR)Unix/Linux (cronR)pipeline.R\n\n\nlibrary(taskscheduleR)\n\n# Schedule daily at 2 AM\ntaskscheduler_create(\n  taskname = \"healthcare_pipeline\",\n  rscript = \"pipeline.R\",\n  schedule = \"DAILY\",\n  starttime = \"02:00\",\n  startdate = format(Sys.Date(), \"%d/%m/%Y\")\n)\n\n\nlibrary(cronR)\n\ncron_add(\n  command = cron_rscript(\n    \"pipeline.R\",\n    rscript_args = c(Sys.getenv(\"ENVIRONMENT\"))\n  ),\n  frequency = \"daily\",\n  at = \"2AM\",\n  id = \"healthcare_pipeline\",\n  description = \"Daily healthcare data sync\"\n)\n\n\n#!/usr/bin/env Rscript\nsource(\"healthcare_pipeline.R\")\n\nrun_daily_pipeline &lt;- function() {\n  config &lt;- load_config(\n    Sys.getenv(\"ENVIRONMENT\", \"production\")\n  )\n  pipeline &lt;- MonitoredPipeline$new(config)\n  \n  success &lt;- pipeline$run()\n  \n  if (!success) {\n    send_alert(\"Pipeline failed!\")\n  }\n}\n\nrun_daily_pipeline()"
  },
  {
    "objectID": "slides/04-data-cleaning.html#python-scheduling-solutions",
    "href": "slides/04-data-cleaning.html#python-scheduling-solutions",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Python Scheduling Solutions",
    "text": "Python Scheduling Solutions\nMultiple options for different environments:\n\nAPScheduler (In-process)System cronpipeline.py\n\n\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.triggers.cron import CronTrigger\n\ndef run_daily_pipeline():\n  \"\"\"Daily pipeline execution\"\"\"\n  config = load_config(\n    os.getenv('ENVIRONMENT', 'production')\n  )\n  pipeline = MonitoredPipeline(config)\n  \n  success = pipeline.run()\n  \n  if not success:\n    send_alert(\"Pipeline failed!\")\n\n# Schedule daily at 2 AM\nscheduler = BlockingScheduler()\nscheduler.add_job(\n  run_daily_pipeline,\n  trigger=CronTrigger(hour=2, minute=0),\n  id='healthcare_pipeline',\n  name='Daily healthcare data sync'\n)\n\nscheduler.start()\n\n\n# crontab -e\n0 2 * * * /path/to/venv/bin/python /path/to/pipeline.py\n\n\n#!/usr/bin/env python3\nimport os\nfrom healthcare_pipeline import MonitoredPipeline, load_config\n\ndef main():\n  config = load_config(\n    os.getenv('ENVIRONMENT', 'production')\n  )\n  pipeline = MonitoredPipeline(config)\n  \n  success = pipeline.run()\n  \n  if not success:\n    send_alert(\"Pipeline failed!\")\n    exit(1)\n\nif __name__ == '__main__':\n  main()"
  },
  {
    "objectID": "slides/04-data-cleaning.html#testing-pipelines",
    "href": "slides/04-data-cleaning.html#testing-pipelines",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Testing Pipelines",
    "text": "Testing Pipelines\nUnit tests for pipeline components:\n\n\nR (testthat):\nlibrary(testthat)\n\ntest_that(\n  \"clean_patient_data removes duplicates\", {\n  df &lt;- tibble(\n    patient_id = c(1, 1, 2),\n    name = c(\"John\", \"John\", \"Jane\"),\n    date_of_birth = as.Date(\n      c(\"1990-01-01\", \"1990-01-01\", \n        \"1990-01-01\")\n    )\n  )\n  \n  cleaned &lt;- clean_patient_data(df)\n  \n  expect_equal(nrow(cleaned), 2)\n  expect_true(\n    !any(duplicated(cleaned$patient_id))\n  )\n})\n\ntest_that(\n  \"clean_patient_data standardizes names\", {\n  df &lt;- tibble(\n    patient_id = 1,\n    name = \"  john doe  \",\n    date_of_birth = as.Date(\"1990-01-01\")\n  )\n  \n  cleaned &lt;- clean_patient_data(df)\n  \n  expect_equal(cleaned$name[1], \"John Doe\")\n})\n\ntest_that(\"pipeline handles API errors\", {\n  config &lt;- list(\n    api_url = \"http://invalid-url\"\n  )\n  pipeline &lt;- HealthcarePipeline$new(config)\n  \n  success &lt;- pipeline$run()\n  \n  expect_false(success)\n})\n\n# Run tests\ntest_dir(\"tests/testthat\")\n\nPython (pytest):\nimport pytest\nimport pandas as pd\n\nclass TestHealthcarePipeline:\n  \n  def test_clean_patient_data_removes_dupes(\n    self):\n    \"\"\"Test duplicate removal\"\"\"\n    df = pd.DataFrame({\n      'patient_id': [1, 1, 2],\n      'name': ['John', 'John', 'Jane'],\n      'date_of_birth': ['1990-01-01'] * 3\n    })\n    \n    cleaned = clean_patient_data(df)\n    \n    assert len(cleaned) == 2\n    assert cleaned['patient_id'].is_unique\n  \n  def test_clean_patient_data_standardizes(\n    self):\n    \"\"\"Test name standardization\"\"\"\n    df = pd.DataFrame({\n      'patient_id': [1],\n      'name': ['  john doe  '],\n      'date_of_birth': ['1990-01-01']\n    })\n    \n    cleaned = clean_patient_data(df)\n    \n    assert cleaned['name'].iloc[0] == \\\n      'John Doe'\n  \n  def test_pipeline_handles_api_errors(\n    self):\n    \"\"\"Test error handling\"\"\"\n    config = {\n      'api_url': 'http://invalid-url'\n    }\n    pipeline = HealthcarePipeline(config)\n    \n    success = pipeline.run()\n    \n    assert success == False\n\n# Run tests\npytest.main([__file__, '-v'])"
  },
  {
    "objectID": "slides/04-data-cleaning.html#r-pipeline-example",
    "href": "slides/04-data-cleaning.html#r-pipeline-example",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "R Pipeline Example",
    "text": "R Pipeline Example\nSame concepts in R:\nlibrary(httr)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\nlibrary(logger)\n\nhealthcare_pipeline &lt;- function(config) {\n  log_info(\"Starting pipeline...\")\n  \n  tryCatch({\n    # 1. Extract (Session 2: API)\n    response &lt;- GET(\n      config$api_url,\n      add_headers(Authorization = paste(\"Bearer\", config$api_token))\n    )\n    stop_for_status(response)\n    data &lt;- content(response) %&gt;% bind_rows()\n    log_info(\"Extracted {nrow(data)} records\")\n    \n    # 2. Transform (Session 4: Clean)\n    data &lt;- data %&gt;%\n      distinct(patient_id, .keep_all = TRUE) %&gt;%\n      mutate(\n        name = str_to_title(str_trim(name)),\n        insurance_id = coalesce(insurance_id, \"SELF_PAY\")\n      )\n    log_info(\"Cleaned {nrow(data)} records\")\n    \n    # 3. Load (Session 3: Database upsert)\n    con &lt;- dbConnect(odbc::odbc(), .connection_string = config$db_connection)\n    patients_tbl &lt;- tbl(con, \"patients\")\n    rows_upsert(patients_tbl, data, by = \"patient_id\", in_place = TRUE)\n    dbDisconnect(con)\n    \n    log_info(\"Pipeline completed successfully\")\n    TRUE\n  }, error = function(e) {\n    log_error(\"Pipeline failed: {e$message}\")\n    FALSE\n  })\n}"
  },
  {
    "objectID": "slides/04-data-cleaning.html#documentation-strategy-overview",
    "href": "slides/04-data-cleaning.html#documentation-strategy-overview",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Documentation Strategy Overview",
    "text": "Documentation Strategy Overview\nWhy documentation matters in data pipelines:\n\nFuture you: Remember decisions made months ago\nTeam collaboration: Enable others to understand and maintain\nTroubleshooting: Debug issues faster with clear context\nCompliance: Meet regulatory requirements (HIPAA, SOX, etc.)\nKnowledge transfer: Onboard new team members effectively\n\nDocumentation layers:\n\nRepository documentation (README, architecture)\nCode documentation (functions, classes, inline comments)\nPipeline documentation (data flow, business rules)\nVersion control (commit messages, change history)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#repository-documentation-readme",
    "href": "slides/04-data-cleaning.html#repository-documentation-readme",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Repository Documentation: README",
    "text": "Repository Documentation: README\nThe first impression - make it count:\n# Healthcare Data Pipeline\n\n![Pipeline Status](https://img.shields.io/badge/pipeline-passing-green)\n\n## Overview\nAutomated ETL pipeline that syncs patient data from external healthcare APIs to our internal database. Runs daily at 2 AM EST.\n\n## Quick Start\n```bash\n# Clone repository\ngit clone https://github.com/yourorg/healthcare-pipeline.git\ncd healthcare-pipeline\n\n# Setup environment (Python)\npip install -r requirements.txt\ncp .env.example .env  # Edit with your credentials\n\n# Setup environment (R)\nrenv::restore()\n# Edit config.yaml with your settings\n\n# Run pipeline\npython healthcare_pipeline.py\n# OR\nRscript pipeline.R\n```\n\n## Architecture\n```\nAPI Source ‚Üí Validation ‚Üí Transformation ‚Üí Database\n     ‚Üì           ‚Üì            ‚Üì            ‚Üì\n  Raw Data ‚Üí Schema Check ‚Üí Clean Data ‚Üí Upserted Records\n```\n\n## Configuration\nSee [CONFIGURATION.md](docs/CONFIGURATION.md) for detailed setup.\n\n## Contributing\nSee [CONTRIBUTING.md](docs/CONTRIBUTING.md) for development guidelines.\n\n## License\nMIT License - see [LICENSE](LICENSE) for details."
  },
  {
    "objectID": "slides/04-data-cleaning.html#repository-documentation-structure",
    "href": "slides/04-data-cleaning.html#repository-documentation-structure",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Repository Documentation: Structure",
    "text": "Repository Documentation: Structure\nOrganize supporting documentation:\nproject/\n‚îú‚îÄ‚îÄ README.md                 # Main entry point\n‚îú‚îÄ‚îÄ docs/\n‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md       # System design\n‚îÇ   ‚îú‚îÄ‚îÄ CONFIGURATION.md      # Setup and config\n‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md         # How to deploy\n‚îÇ   ‚îú‚îÄ‚îÄ TROUBLESHOOTING.md    # Common issues\n‚îÇ   ‚îî‚îÄ‚îÄ API_REFERENCE.md      # Function docs\n‚îú‚îÄ‚îÄ examples/\n‚îÇ   ‚îú‚îÄ‚îÄ sample_config.yaml    # Example configurations\n‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py      # Usage examples\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_extraction.py    # Unit tests\n‚îÇ   ‚îî‚îÄ‚îÄ fixtures/             # Test data\n‚îú‚îÄ‚îÄ .env.example              # Template env vars\n‚îú‚îÄ‚îÄ requirements.txt          # Python deps\n‚îú‚îÄ‚îÄ renv.lock                 # R dependencies\n‚îî‚îÄ‚îÄ CHANGELOG.md              # Version history\nEach document serves a specific purpose"
  },
  {
    "objectID": "slides/04-data-cleaning.html#function-method-documentation",
    "href": "slides/04-data-cleaning.html#function-method-documentation",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Function & Method Documentation",
    "text": "Function & Method Documentation\nSelf-documenting code with clear docstrings:\n\nPython (docstring)R (roxygen2)\n\n\ndef clean_patient_data(df: pd.DataFrame, \n                      config: dict = None) -&gt; pd.DataFrame:\n    \"\"\"Clean and standardize patient data.\n    \n    Performs the following transformations:\n    - Removes duplicate patient_id records (keeps most recent)\n    - Standardizes name formatting (Title Case, trimmed)\n    - Handles missing insurance_id (defaults to 'SELF_PAY')\n    - Validates age range (0-120 years)\n    \n    Args:\n        df: Raw patient DataFrame with columns: \n            patient_id, name, date_of_birth, insurance_id\n        config: Optional configuration dict for custom rules\n            \n    Returns:\n        Cleaned DataFrame with same schema but standardized values\n        \n    Raises:\n        ValueError: If required columns are missing\n        pandas.errors.DataError: If date parsing fails\n        \n    Example:\n        &gt;&gt;&gt; raw_data = pd.DataFrame({\n        ...     'patient_id': [1, 1, 2],\n        ...     'name': ['  john doe  ', 'john doe', 'Jane Smith'],\n        ...     'date_of_birth': ['1990-01-01', '1990-01-01', '1985-05-15'],\n        ...     'insurance_id': [None, 'BC123', 'AETNA456']\n        ... })\n        &gt;&gt;&gt; cleaned = clean_patient_data(raw_data)\n        &gt;&gt;&gt; len(cleaned)  # Duplicates removed\n        2\n        &gt;&gt;&gt; cleaned.loc[0, 'name']  # Standardized\n        'John Doe'\n    \"\"\"\n    df = df.copy()\n    logger.info(f\"Cleaning {len(df)} patient records\")\n    \n    # Validate required columns\n    required_cols = ['patient_id', 'name', 'date_of_birth']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing required columns: {missing_cols}\")\n    \n    # Remove duplicates (keep latest)\n    original_count = len(df)\n    df = df.sort_values('date_of_birth').drop_duplicates(\n        subset=['patient_id'], keep='last'\n    )\n    if len(df) &lt; original_count:\n        logger.info(f\"Removed {original_count - len(df)} duplicate records\")\n    \n    # Standardize names\n    df['name'] = df['name'].str.strip().str.title()\n    \n    # Handle missing insurance\n    df['insurance_id'] = df['insurance_id'].fillna('SELF_PAY')\n    \n    # Validate age range\n    df['date_of_birth'] = pd.to_datetime(df['date_of_birth'])\n    df['age'] = ((pd.Timestamp.now() - df['date_of_birth']).dt.days / 365.25)\n    invalid_ages = (df['age'] &lt; 0) | (df['age'] &gt; 120)\n    if invalid_ages.any():\n        logger.warning(f\"Removing {invalid_ages.sum()} records with invalid ages\")\n        df = df[~invalid_ages]\n    \n    logger.info(f\"Cleaning complete: {len(df)} valid records\")\n    return df[['patient_id', 'name', 'date_of_birth', 'insurance_id']]\n\n\n#' Clean and Standardize Patient Data\n#'\n#' Performs comprehensive cleaning of raw patient data including\n#' deduplication, standardization, and validation.\n#'\n#' @description\n#' This function applies a series of data quality rules to ensure\n#' patient data meets our database standards. All transformations\n#' are logged for audit purposes.\n#'\n#' @param df Data frame containing raw patient data\n#' @param config List of optional configuration parameters\n#'   \\describe{\n#'     \\item{max_age}{Maximum valid age (default: 120)}\n#'     \\item{min_age}{Minimum valid age (default: 0)}\n#'     \\item{default_insurance}{Default insurance code (default: \"SELF_PAY\")}\n#'   }\n#'\n#' @return Data frame with cleaned patient records containing columns:\n#'   \\describe{\n#'     \\item{patient_id}{Unique patient identifier (integer)}\n#'     \\item{name}{Standardized patient name (Title Case)}\n#'     \\item{date_of_birth}{Patient birth date (Date)}\n#'     \\item{insurance_id}{Insurance identifier or \"SELF_PAY\"}\n#'   }\n#'\n#' @details\n#' The cleaning process follows these steps:\n#' \\enumerate{\n#'   \\item Validate required columns exist\n#'   \\item Remove duplicate patient_id records (keeps most recent)\n#'   \\item Standardize name formatting (Title Case, trimmed)\n#'   \\item Convert date_of_birth to Date type\n#'   \\item Calculate and validate age (0-120 years)\n#'   \\item Fill missing insurance_id with default value\n#' }\n#'\n#' @examples\n#' \\dontrun{\n#' # Basic usage\n#' raw_data &lt;- data.frame(\n#'   patient_id = c(1, 1, 2),\n#'   name = c(\"  john doe  \", \"john doe\", \"Jane Smith\"),\n#'   date_of_birth = as.Date(c(\"1990-01-01\", \"1990-01-01\", \"1985-05-15\")),\n#'   insurance_id = c(NA, \"BC123\", \"AETNA456\")\n#' )\n#' cleaned &lt;- clean_patient_data(raw_data)\n#' \n#' # With custom config\n#' config &lt;- list(max_age = 100, default_insurance = \"UNINSURED\")\n#' cleaned &lt;- clean_patient_data(raw_data, config)\n#' }\n#'\n#' @importFrom dplyr filter mutate distinct arrange\n#' @importFrom stringr str_trim str_to_title\n#' @importFrom logger log_info log_warning\n#'\n#' @export\nclean_patient_data &lt;- function(df, config = NULL) {\n  # Set default configuration\n  default_config &lt;- list(\n    max_age = 120,\n    min_age = 0,\n    default_insurance = \"SELF_PAY\"\n  )\n  config &lt;- modifyList(default_config, config %||% list())\n  \n  log_info(\"Cleaning {nrow(df)} patient records\")\n  \n  # Validate required columns\n  required_cols &lt;- c(\"patient_id\", \"name\", \"date_of_birth\")\n  missing_cols &lt;- setdiff(required_cols, names(df))\n  if (length(missing_cols) &gt; 0) {\n    stop(\"Missing required columns: \", paste(missing_cols, collapse = \", \"))\n  }\n  \n  original_count &lt;- nrow(df)\n  \n  # Remove duplicates (keep latest by date_of_birth)\n  df &lt;- df %&gt;%\n    arrange(date_of_birth) %&gt;%\n    distinct(patient_id, .keep_all = TRUE)\n  \n  if (nrow(df) &lt; original_count) {\n    log_info(\"Removed {original_count - nrow(df)} duplicate records\")\n  }\n  \n  # Standardize and validate\n  df &lt;- df %&gt;%\n    mutate(\n      name = str_to_title(str_trim(name)),\n      date_of_birth = as.Date(date_of_birth),\n      age = as.numeric(difftime(Sys.Date(), date_of_birth, units = \"days\")) / 365.25,\n      insurance_id = coalesce(insurance_id, config$default_insurance)\n    ) %&gt;%\n    filter(\n      age &gt;= config$min_age,\n      age &lt;= config$max_age\n    ) %&gt;%\n    select(patient_id, name, date_of_birth, insurance_id)\n  \n  log_info(\"Cleaning complete: {nrow(df)} valid records\")\n  df\n}"
  },
  {
    "objectID": "slides/04-data-cleaning.html#inline-code-documentation",
    "href": "slides/04-data-cleaning.html#inline-code-documentation",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Inline Code Documentation",
    "text": "Inline Code Documentation\nStrategic commenting for complex logic:\n\nGood (Why, not What)Bad (Obvious statements)\n\n\ndef upsert_patients(engine, df):\n    \"\"\"Upsert patient records using PostgreSQL syntax\"\"\"\n    \n    # Use ON CONFLICT for idempotency - critical for scheduled runs\n    # This prevents duplicate records when pipeline runs multiple times\n    # with overlapping data (common in healthcare APIs)\n    for _, row in df.iterrows():\n        engine.execute(\"\"\"\n            INSERT INTO patients (patient_id, name, date_of_birth, insurance_id)\n            VALUES (%(patient_id)s, %(name)s, %(dob)s, %(insurance)s)\n            ON CONFLICT (patient_id) \n            DO UPDATE SET \n                name = EXCLUDED.name,\n                date_of_birth = EXCLUDED.date_of_birth,\n                insurance_id = EXCLUDED.insurance_id,\n                updated_at = CURRENT_TIMESTAMP\n        \"\"\", {\n            'patient_id': row['patient_id'],\n            'name': row['name'],\n            'dob': row['date_of_birth'],\n            'insurance': row['insurance_id']\n        })\n    \n    # Commit after all records to maintain transaction consistency\n    # If any record fails, entire batch is rolled back\n    engine.commit()\n\n\ndef upsert_patients(engine, df):\n    # Loop through dataframe\n    for _, row in df.iterrows():\n        # Execute SQL statement\n        engine.execute(\"\"\"\n            INSERT INTO patients (patient_id, name, date_of_birth, insurance_id)\n            VALUES (%(patient_id)s, %(name)s, %(dob)s, %(insurance)s)\n            ON CONFLICT (patient_id) DO UPDATE SET name = EXCLUDED.name\n        \"\"\", {\n            'patient_id': row['patient_id'],  # Set patient ID\n            'name': row['name'],              # Set name\n            'dob': row['date_of_birth'],      # Set date of birth\n            'insurance': row['insurance_id']  # Set insurance\n        })\n    \n    engine.commit()  # Commit transaction"
  },
  {
    "objectID": "slides/04-data-cleaning.html#version-control-documentation-git",
    "href": "slides/04-data-cleaning.html#version-control-documentation-git",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Version Control Documentation (Git)",
    "text": "Version Control Documentation (Git)\n\nCommitsBranchs.gitignore\n\n\nUse Commit Conventions.\n# Good commit messages (follow conventional commits)\nfeat: add patient data validation with pandera schema\nfix: handle missing insurance_id in clean_patient_data()  \ndocs: add API authentication setup guide\nrefactor: extract database connection into config class\nchore: update requirements.txt with latest pandas version\n\n# Include context for data changes\nfix: correct age calculation to use fractional years\n\nPreviously using integer division which caused patients \nborn in December to appear 1 year younger. Now using \n365.25 for more accurate age calculation.\n\nFixes #123\nTested with patients born 1990-12-31, 1991-01-01\n\n\n# Feature branches for new functionality\ngit checkout -b feature/add-graphql-api-support\ngit checkout -b feature/implement-data-validation\ngit checkout -b fix/database-connection-timeout\n\n# Use descriptive branch names\ngit checkout -b refactor/split-pipeline-into-classes\ngit checkout -b docs/add-deployment-guide\ngit checkout -b test/add-integration-tests\n\n\n# Environment and secrets\n.env\n.env.*\nconfig/production.yaml\n*.key\n*.pem\n\n# Data files (never commit actual data)\ndata/\n*.csv\n*.json\n*.xlsx\n!data/sample/  # Except sample/test data\n!*_example.csv\n\n# Logs and outputs\nlogs/\n*.log\npipeline_metrics_*.json\n\n# Language-specific\n# Python\n__pycache__/\n*.pyc\n.venv/\n.pytest_cache/\n\n# R\n.RData\n.Rhistory\nrenv/library/\npackrat/lib*/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n*.Rproj\n.Rproj.user\n\n# OS\n.DS_Store\nThumbs.db"
  },
  {
    "objectID": "slides/04-data-cleaning.html#pipeline-specific-documentation",
    "href": "slides/04-data-cleaning.html#pipeline-specific-documentation",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Pipeline-Specific Documentation",
    "text": "Pipeline-Specific Documentation\nDocument the data flow and business rules:\n# Data Pipeline Documentation\n\n## Data Sources\n\n### Healthcare API (Primary)\n- **Endpoint**: `https://api.healthcare-provider.com/v2/patients`\n- **Authentication**: Bearer token (expires every 24 hours)\n- **Rate Limits**: 1000 requests/hour, 10 requests/second\n- **Data Freshness**: Updated every 4 hours\n- **Expected Volume**: ~500 new patients/day, ~2000 updates/day\n\n### Legacy Database (Backup)\n- **Connection**: PostgreSQL on `legacy-db.internal:5432`\n- **Tables**: `patients`, `insurance_providers`, `visit_history`\n- **Update Frequency**: Real-time for active patients\n- **Data Quality**: Known issues with pre-2020 records\n\n## Business Rules\n\n### Patient Data Validation\n1. **patient_id**: Must be positive integer, unique across all sources\n2. **name**: Required, must be 2-100 characters after trimming\n3. **date_of_birth**: Required, cannot be future date, must be after 1900-01-01\n4. **insurance_id**: Optional, but if present must match active provider list\n\n### Data Transformation Rules\n- **Name standardization**: Convert to Title Case, remove extra whitespace\n- **Insurance handling**: Missing values become \"SELF_PAY\"\n- **Duplicate resolution**: For same patient_id, keep record with latest update timestamp\n\n### Error Handling\n- **Validation failures**: Log error, store in `validation_errors` table, continue processing\n- **API timeouts**: Retry up to 3 times with exponential backoff\n- **Database errors**: Rollback transaction, alert #data-alerts channel\n\n## Monitoring & Alerts\n\n### Success Metrics\n- **Daily volume**: Expected 400-600 new/updated records\n- **Processing time**: Should complete within 15 minutes\n- **Error rate**: Should be &lt;1% of total records\n\n### Alert Conditions\n- No data received in 6+ hours ‚Üí **CRITICAL**\n- Error rate &gt;5% ‚Üí **WARNING**\n- Processing time &gt;30 minutes ‚Üí **WARNING**\n- Database connection fails ‚Üí **CRITICAL**\n\n### Log Locations\n- **Application logs**: `/var/log/healthcare-pipeline/`\n- **Database logs**: CloudWatch LogGroup `/aws/rds/healthcare-db`\n- **Metrics dashboard**: `https://grafana.internal/dashboard/healthcare-pipeline`"
  },
  {
    "objectID": "slides/04-data-cleaning.html#best-practices-summary",
    "href": "slides/04-data-cleaning.html#best-practices-summary",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\nFrom all 4 sessions:\n\nAPI Layer (Session 2)\n\nUse authentication tokens securely (env variables)\nHandle rate limiting and pagination\nRetry on transient failures"
  },
  {
    "objectID": "slides/04-data-cleaning.html#best-practices-summary-1",
    "href": "slides/04-data-cleaning.html#best-practices-summary-1",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\nFrom all 4 sessions:\n\nAPI Layer (Session 2)\n\n\nDatabase Layer (Session 3)\n\nUse parameterized queries (prevent SQL injection)\nImplement upserts for idempotency\nClose connections properly"
  },
  {
    "objectID": "slides/04-data-cleaning.html#best-practices-summary-2",
    "href": "slides/04-data-cleaning.html#best-practices-summary-2",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\nFrom all 4 sessions:\n\nAPI Layer (Session 2)\n\n\nDatabase Layer (Session 3)\n\n\nData Quality (Session 4)\n\nValidate early and often\nKeep raw data separate from clean data\nDocument all transformations"
  },
  {
    "objectID": "slides/04-data-cleaning.html#best-practices-summary-3",
    "href": "slides/04-data-cleaning.html#best-practices-summary-3",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\nFrom all 4 sessions:\n\nAPI Layer (Session 2)\n\n\nDatabase Layer (Session 3)\n\n\nData Quality (Session 4)\n\n\nPipeline Design\n\nModular, reusable components\nComprehensive error handling\nMonitor and log everything"
  },
  {
    "objectID": "slides/04-data-cleaning.html#common-pitfalls-revisited",
    "href": "slides/04-data-cleaning.html#common-pitfalls-revisited",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Common Pitfalls (Revisited)",
    "text": "Common Pitfalls (Revisited)\nLessons from the workshop:\n\nSession 2: Hardcoding API tokens in code\nSession 2: Not handling pagination (missing data)\nSession 3: Using string concatenation for SQL (injection risk)\nSession 3: Appending without checking for duplicates\nSession 4: Deleting outliers without domain knowledge\nSession 4: Not documenting why data was removed\nAll Sessions: Not logging errors and metrics\nAll Sessions: Not making pipelines idempotent"
  },
  {
    "objectID": "slides/04-data-cleaning.html#deployment-checklist",
    "href": "slides/04-data-cleaning.html#deployment-checklist",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Deployment Checklist",
    "text": "Deployment Checklist\nBefore going to production:\n\nSecurity: No credentials in code, use env variables\nTesting: Unit tests for all pipeline components\nError Handling: Try-catch blocks, retry logic\nLogging: Structured logs with appropriate levels\nMonitoring: Metrics tracked and alerts configured\nIdempotency: Pipeline can be safely re-run\nDocumentation: README, comments, data dictionary\nConfiguration: Environment-based config files\nDependencies: requirements.txt or renv.lock\nScheduling: Cron or scheduler configured\nBackups: Database backups automated\nRollback Plan: Know how to revert changes"
  },
  {
    "objectID": "slides/04-data-cleaning.html#hands-on-exercise",
    "href": "slides/04-data-cleaning.html#hands-on-exercise",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\nBuild a complete healthcare data pipeline:\n\nExtract: Fetch patient data from mock API\nValidate: Check schema with pandera/validate\nTransform: Clean names, dates, handle missing insurance\nLoad: Upsert to SQLite database\nMonitor: Log metrics and errors\nTest: Write unit tests for cleaning functions\nDocument: Create README with data flow\nSchedule: Set up to run every 15 minutes\n\nUse the healthcare.db from Session 3 as your target!"
  },
  {
    "objectID": "slides/04-data-cleaning.html#docker-example-vs-exercise-requirements",
    "href": "slides/04-data-cleaning.html#docker-example-vs-exercise-requirements",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Docker Example vs Exercise Requirements",
    "text": "Docker Example vs Exercise Requirements\nHow does our docker-example/ compare?\n\n‚úÖ Completed Features‚ö†Ô∏è Partially CompletedüîÑ What‚Äôs Different\n\n\n\n\n\n\n\n\n\n\nRequirement\nDocker Example\nStatus\n\n\n\n\nExtract\n‚úÖ R Plumber API + PostgreSQL\nEXCEEDS - Multiple sources\n\n\nLoad\n‚úÖ rows_upsert() in tidyverse version\nMEETS - Uses upsert pattern\n\n\nMonitor\n‚úÖ logger package with structured logging\nMEETS - Production-ready logging\n\n\nDocument\n‚úÖ Comprehensive README + architecture docs\nEXCEEDS - Multiple doc types\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequirement\nDocker Example\nGap\n\n\n\n\nValidate\n‚ö†Ô∏è Basic error handling, no schema validation\nMISSING - No pandera/validate\n\n\nTransform\n‚ö†Ô∏è Basic joins, no standardization\nMISSING - No name cleaning, date parsing\n\n\nTest\n‚ö†Ô∏è Integration tests (validate.ps1/sh), no unit tests\nMISSING - No function-level tests\n\n\nSchedule\n‚ö†Ô∏è Manual execution only\nMISSING - No cron/scheduler\n\n\n\n\n\nDatabase Target: - Exercise: SQLite (healthcare.db)\n- Docker: PostgreSQL (more enterprise-like)\nAPI Source: - Exercise: ‚ÄúMock API‚Äù (undefined) - Docker: Real R Plumber API with sample data\nScope: - Exercise: Simple single-source pipeline - Docker: Multi-source integration (API + DB)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#exercise-enhancement-opportunities",
    "href": "slides/04-data-cleaning.html#exercise-enhancement-opportunities",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Exercise Enhancement Opportunities",
    "text": "Exercise Enhancement Opportunities\nBuilding on the Docker example:\n\nAdd Missing ComponentsAdd Data CleaningAdd Unit TestsAdd Scheduling\n\n\n# 1. Add schema validation to docker example\nlibrary(validate)\n\npatient_rules &lt;- validator(\n  patient_id &gt; 0,\n  !is.na(name),\n  nchar(name) &gt; 0,\n  !is.na(date_of_birth)\n)\n\n# Add to healthcare_pipeline_tidyverse.R:\nvalidated_patients &lt;- patients_api %&gt;%\n  confront(patient_rules) %&gt;%\n  # Only keep valid records\n  filter(passes)\n\n\n# 2. Add transformation functions\nclean_patient_data &lt;- function(df) {\n  df %&gt;%\n    # Remove duplicates\n    distinct(patient_id, .keep_all = TRUE) %&gt;%\n    # Standardize names\n    mutate(\n      name = str_to_title(str_trim(name)),\n      insurance_id = coalesce(insurance_id, \"SELF_PAY\")\n    ) %&gt;%\n    # Validate age\n    filter(\n      date_of_birth &lt;= today(),\n      date_of_birth &gt;= as.Date(\"1900-01-01\")\n    )\n}\n\n\n# 3. Create tests/test-cleaning.R\nlibrary(testthat)\n\ntest_that(\"clean_patient_data removes duplicates\", {\n  df &lt;- tibble(\n    patient_id = c(1, 1, 2),\n    name = c(\"john\", \"john\", \"jane\"),\n    date_of_birth = as.Date(c(\"1990-01-01\", \"1990-01-01\", \"1985-01-01\")),\n    insurance_id = c(NA, \"INS1\", \"INS2\")\n  )\n  \n  result &lt;- clean_patient_data(df)\n  \n  expect_equal(nrow(result), 2)\n  expect_true(!any(duplicated(result$patient_id)))\n  expect_equal(result$name[1], \"John\")  # Title case\n})\n\n\n# 4. Add scheduler.R\nlibrary(cronR)\n\n# Schedule every 15 minutes\ncron_add(\n  command = cron_rscript(\n    \"docker-example/r-client/healthcare_pipeline_tidyverse.R\"\n  ),\n  frequency = \"*/15 * * * *\",  # Every 15 minutes\n  id = \"healthcare_pipeline\",\n  description = \"Healthcare data sync\"\n)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#complete-example-end-to-end",
    "href": "slides/04-data-cleaning.html#complete-example-end-to-end",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Complete Example: End-to-End",
    "text": "Complete Example: End-to-End\nFull working example tying everything together:\n\n\nR (complete_pipeline.R):\n#!/usr/bin/env Rscript\nlibrary(httr)\nlibrary(dplyr)\nlibrary(DBI)\nlibrary(validate)\nlibrary(logger)\n\n# Setup logging\nlog_threshold(INFO)\n\n# Validation rules\npatient_rules &lt;- validator(\n  patient_id &gt; 0,\n  !is.na(patient_id),\n  !is.na(name),\n  nchar(name) &gt; 0,\n  date_of_birth &lt;= Sys.Date()\n)\n\nrun_pipeline &lt;- function() {\n  # Configuration from environment\n  api_url &lt;- Sys.getenv(\"API_URL\")\n  api_token &lt;- Sys.getenv(\"API_TOKEN\")\n  db_conn &lt;- Sys.getenv(\n    \"DB_CONNECTION\", \n    \"sqlite:///healthcare.db\"\n  )\n  \n  tryCatch({\n    # EXTRACT (Session 2: API)\n    log_info(\"Fetching from API...\")\n    response &lt;- GET(\n      api_url,\n      add_headers(\n        Authorization = paste(\"Bearer\", api_token)\n      ),\n      timeout(30)\n    )\n    stop_for_status(response)\n    df &lt;- content(response) %&gt;% bind_rows()\n    log_info(\"Fetched {nrow(df)} records\")\n    \n    # VALIDATE (Session 4: Quality)\n    log_info(\"Validating data...\")\n    results &lt;- confront(df, patient_rules)\n    if (!all(results)) {\n      stop(\"Validation failed\")\n    }\n    \n    # TRANSFORM (Session 4: Clean)\n    log_info(\"Cleaning data...\")\n    df &lt;- df %&gt;%\n      distinct(patient_id, .keep_all = TRUE) %&gt;%\n      mutate(\n        name = str_to_title(str_trim(name)),\n        insurance_id = coalesce(\n          insurance_id, \"SELF_PAY\"\n        )\n      )\n    \n    # LOAD (Session 3: Database upsert)\n    log_info(\"Loading to database...\")\n    con &lt;- dbConnect(\n      RSQLite::SQLite(), \n      \"healthcare.db\"\n    )\n    \n    patients_tbl &lt;- tbl(con, \"patients\")\n    rows_upsert(\n      patients_tbl, \n      df, \n      by = \"patient_id\", \n      in_place = TRUE\n    )\n    \n    dbDisconnect(con)\n    \n    log_info(\"Pipeline completed: {nrow(df)} records\")\n    TRUE\n  }, error = function(e) {\n    log_error(\"Pipeline failed: {e$message}\")\n    FALSE\n  })\n}\n\n# Run pipeline\nsuccess &lt;- run_pipeline()\nquit(status = if (success) 0 else 1)\n\nPython (complete_pipeline.py):\n#!/usr/bin/env python3\nimport os\nimport requests\nimport pandas as pd\nimport pandera as pa\nfrom sqlalchemy import create_engine\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Schema from Session 4\npatient_schema = pa.DataFrameSchema({\n  \"patient_id\": pa.Column(\n    int, pa.Check.greater_than(0)\n  ),\n  \"name\": pa.Column(\n    str, pa.Check.str_length(min_value=1)\n  ),\n  \"date_of_birth\": pa.Column(pa.DateTime),\n  \"insurance_id\": pa.Column(str, nullable=True)\n})\n\ndef run_pipeline():\n  \"\"\"Complete ETL pipeline\"\"\"\n  # Configuration from environment\n  api_url = os.getenv('API_URL')\n  api_token = os.getenv('API_TOKEN')\n  db_conn = os.getenv(\n    'DB_CONNECTION', \n    'sqlite:///healthcare.db'\n  )\n  \n  try:\n    # EXTRACT (Session 2: API)\n    logger.info(\"Fetching from API...\")\n    response = requests.get(\n      api_url,\n      headers={\n        'Authorization': f'Bearer {api_token}'\n      },\n      timeout=30\n    )\n    response.raise_for_status()\n    df = pd.DataFrame(response.json())\n    logger.info(f\"Fetched {len(df)} records\")\n    \n    # VALIDATE (Session 4: Quality)\n    logger.info(\"Validating data...\")\n    df = patient_schema.validate(df)\n    \n    # TRANSFORM (Session 4: Clean)\n    logger.info(\"Cleaning data...\")\n    df = df.drop_duplicates(\n      subset=['patient_id']\n    )\n    df['name'] = df['name'].str.strip()\\\\\\n      .str.title()\n    df['insurance_id'] = df['insurance_id']\\\\\\n      .fillna('SELF_PAY')\n    \n    # LOAD (Session 3: Database upsert)\n    logger.info(\"Loading to database...\")\n    engine = create_engine(db_conn)\n    \n    for _, row in df.iterrows():\n      engine.execute(\\\"\\\"\\\"\n        INSERT INTO patients \n          (patient_id, name, \n           date_of_birth, insurance_id)\n        VALUES (?, ?, ?, ?)\n        ON CONFLICT(patient_id) DO UPDATE SET\n          name=excluded.name,\n          date_of_birth=excluded.date_of_birth,\n          insurance_id=excluded.insurance_id\n      \\\"\\\"\\\", (row['patient_id'], \n            row['name'], \n            row['date_of_birth'], \n            row['insurance_id']))\n    \n    logger.info(\n      f\"Pipeline completed: {len(df)} records\"\n    )\n    return True\n    \n  except Exception as e:\n    logger.error(f\"Pipeline failed: {e}\")\n    return False\n\nif __name__ == '__main__':\n  success = run_pipeline()\n  exit(0 if success else 1)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#workshop-recap-1",
    "href": "slides/04-data-cleaning.html#workshop-recap-1",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Workshop Recap",
    "text": "Workshop Recap\nYou now know how to:\n\nSession 1: Set up Python/R environments and use git\nSession 2: Fetch data from REST and GraphQL APIs\nSession 3: Connect to databases and perform upserts\nSession 4: Clean data and build complete pipelines\n\nKey concepts: - Idempotency (upserts, safe re-runs) - Error handling (retries, logging) - Security (env variables, parameterized queries) - Modularity (reusable components) - Testing (validate assumptions)"
  },
  {
    "objectID": "slides/04-data-cleaning.html#next-steps",
    "href": "slides/04-data-cleaning.html#next-steps",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Next Steps",
    "text": "Next Steps\nContinue learning:\n\nAdvanced scheduling: Apache Airflow, Prefect\nData validation: Great Expectations, dbt tests\nMonitoring: Prometheus, Grafana, Datadog\nContainerization: Docker, Kubernetes\nCloud platforms: AWS Lambda, Azure Functions\nStream processing: Apache Kafka, Apache Spark\nData warehousing: Snowflake, BigQuery, Redshift"
  },
  {
    "objectID": "slides/04-data-cleaning.html#questions",
    "href": "slides/04-data-cleaning.html#questions",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Questions?",
    "text": "Questions?\nThank you for attending!"
  },
  {
    "objectID": "slides/04-data-cleaning.html#resources",
    "href": "slides/04-data-cleaning.html#resources",
    "title": "Data Cleaning & Pipeline Integration",
    "section": "Resources",
    "text": "Resources\nFrom the workshop: - Course Website - GitHub Repository - Healthcare database example: exercises/healthcare.db\nAdditional resources: - pandas Documentation - requests Documentation - SQLAlchemy Documentation - pandera Documentation - Great Expectations - Tidy Data Principles - ConnectionStrings.com"
  }
]